{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db496b92",
   "metadata": {},
   "source": [
    "# Run *SDePER* on sequencing-based simulated data: Scenario 1 + Spatial data as reference + WITHOUT pseudo-spots\n",
    "\n",
    "In this Notebook we run ablation test on SDePER on simulated data. For generating **sequencing-based** simulated data via coarse-graining procedure please refer [generate_simulated_spatial_data.nb.html](https://rawcdn.githack.com/az7jh2/SDePER_Analysis/c963d08f74f4591c2ef6f132177795297793d878/Simulation_seq_based/Generate_simulation_data/generate_simulated_spatial_data.nb.html) in [Simulation_seq_based](https://github.com/az7jh2/SDePER_Analysis/tree/main/Simulation_seq_based) folder.\n",
    "\n",
    "**Scenario 1** means the reference data for deconvolution includes all single cells with the **matched 12 cell types**.\n",
    "\n",
    "**Spatial data as reference** means the reference data is actually the [GSE102827](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE102827) scRNA-seq data which is used to generate the simulated data, therefore it's **free of platform effect**.\n",
    "\n",
    "**WITHOUT pseudo-spots** means we train the Conditional Variational Autoencoder (CVAE) without incorporating pseudo-spots in the training data.\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "So here we use the **4 input files** as shown below:\n",
    "\n",
    "1. raw nUMI counts of simulated spatial transcriptomic data (spots × genes): [sim_seq_based_spatial_spot_nUMI.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation_seq_based/Generate_simulation_data/sim_seq_based_spatial_spot_nUMI.csv)\n",
    "2. raw nUMI counts of reference GSE102827 scRNA-seq data (cells × genes): `GSE102827_scRNA_cell_nUMI.csv`. Since the file size of csv file of raw nUMI matrix of all 65,539 cells and 25,187 genes is up to 3.1 GB, we do not provide this file in our repository. It's just a **matrix transpose** of [GSE102827_merged_all_raw.csv.gz](https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE102827&format=file&file=GSE102827%5Fmerged%5Fall%5Fraw%2Ecsv%2Egz) in [GSE102827](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE102827) to satisty the file format requirement that rows as cells and columns as genes\n",
    "3. cell type annotations for selected 2,002 cells used for simulated data generation in [GSE102827](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE102827) scRNA-seq data (cells × 1): [GSE102827_scRNA_cell_celltype.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation_seq_based/Run_SDePER_on_simulation_data/Scenario_1/ref_spatial/GSE102827_scRNA_cell_celltype.csv)\n",
    "4. adjacency matrix of spots in simulated spatial transcriptomic data (spots × spots): [sim_spatial_spot_adjacency_matrix.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Generate_simulation_data/sim_spatial_spot_adjacency_matrix.csv)\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "SDePER settings are the same as baseline run [S1_ref_spatial_SDePER_WITH_CVAE.ipynb](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation_seq_based/Run_SDePER_on_simulation_data/Scenario_1/ref_spatial/S1_ref_spatial_SDePER_WITH_CVAE.ipynb), and we discarded unneeded command-line options:\n",
    "\n",
    "* number of selected TOP marker genes for each comparison in Differential `n_marker_per_cmp`: 20\n",
    "* number of used CPU cores `n_core`: 64\n",
    "* initial learning rate for training CVAE `cvae_init_lr`: 0.003\n",
    "* number of hidden layers in encoder and decoder of CVAE `num_hidden_layer`: 1\n",
    "* whether to use Batch Normalization `use_batch_norm`: false\n",
    "* CVAE training epochs `cvae_train_epoch`: 1000\n",
    "\n",
    "ALL other options are left as default.\n",
    "\n",
    "**For ablation test, set the number of pseudo-spots during CVAE training `n_pseudo_spot` as 0**.\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "the `bash` command to start cell type deconvolution is\n",
    "\n",
    "`runDeconvolution -q sim_seq_based_spatial_spot_nUMI.csv -r GSE102827_scRNA_cell_nUMI.csv -c GSE102827_scRNA_cell_celltype.csv -a sim_spatial_spot_adjacency_matrix.csv --n_marker_per_cmp 20 -n 64 --cvae_init_lr 0.003 --num_hidden_layer 1 --use_batch_norm false --cvae_train_epoch 1000 --n_pseudo_spot 0`\n",
    "\n",
    "Note this Notebook uses **SDePER v1.2.1**. Cell type deconvolution result is renamed as [S1_ref_spatial_SDePER_Ablation_NO_Pseudo_spots_celltype_proportions.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Ablation/Ablation_simulation_seq_based/S1_ref_spatial_SDePER_Ablation_NO_Pseudo_spots_celltype_proportions.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9e3939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SDePER (Spatial Deconvolution method with Platform Effect Removal) v1.2.1\n",
      "\n",
      "\n",
      "running options:\n",
      "spatial_file: /home/exouser/Spatial/sim_seq_based_spatial_spot_nUMI.csv\n",
      "ref_file: /home/exouser/Spatial/GSE102827_scRNA_cell_nUMI.csv\n",
      "ref_celltype_file: /home/exouser/Spatial/GSE102827_scRNA_cell_celltype.csv\n",
      "marker_file: None\n",
      "loc_file: None\n",
      "A_file: /home/exouser/Spatial/sim_spatial_spot_adjacency_matrix.csv\n",
      "n_cores: 64\n",
      "threshold: 0\n",
      "use_cvae: True\n",
      "use_imputation: False\n",
      "diagnosis: False\n",
      "verbose: True\n",
      "use_fdr: True\n",
      "p_val_cutoff: 0.05\n",
      "fc_cutoff: 1.2\n",
      "pct1_cutoff: 0.3\n",
      "pct2_cutoff: 0.1\n",
      "sortby_fc: True\n",
      "n_marker_per_cmp: 20\n",
      "filter_cell: True\n",
      "filter_gene: True\n",
      "n_hv_gene: 200\n",
      "n_pseudo_spot: 0\n",
      "pseudo_spot_min_cell: 2\n",
      "pseudo_spot_max_cell: 8\n",
      "seq_depth_scaler: 10000\n",
      "cvae_input_scaler: 10\n",
      "cvae_init_lr: 0.003\n",
      "num_hidden_layer: 1\n",
      "use_batch_norm: False\n",
      "cvae_train_epoch: 1000\n",
      "use_spatial_pseudo: False\n",
      "redo_de: True\n",
      "seed: 383\n",
      "lambda_r: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "lambda_g: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "diameter: 200\n",
      "impute_diameter: [160, 114, 80]\n",
      "\n",
      "\n",
      "######### Preprocessing... #########\n",
      "\n",
      "first build CVAE...\n",
      "\n",
      "read spatial data from file /home/exouser/Spatial/sim_seq_based_spatial_spot_nUMI.csv\n",
      "total 581 spots; 25187 genes\n",
      "\n",
      "filtering genes present in <3 spots: 9510 genes removed\n",
      "\n",
      "read scRNA-seq data from file /home/exouser/Spatial/GSE102827_scRNA_cell_nUMI.csv\n",
      "total 65539 cells; 25187 genes\n",
      "read scRNA-seq cell-type annotation from file /home/exouser/Spatial/GSE102827_scRNA_cell_celltype.csv\n",
      "total 12 cell-types\n",
      "subset cells with cell-type annotation, finally keep 2002 cells; 25187 genes\n",
      "\n",
      "filtering cells with <200 genes: No cells removed\n",
      "\n",
      "filtering genes present in <10 cells: 11492 genes removed\n",
      "\n",
      "total 13695 overlapped genes\n",
      "\n",
      "identify 200 highly variable genes from scRNA-seq data...\n",
      "\n",
      "identify cell-type marker genes...\n",
      "no marker gene profile provided. Perform DE to get cell-type marker genes on scRNA-seq data...\n",
      "\n",
      "Differential analysis across cell-types on scRNA-seq data...\n",
      "0%...8%...WARNING: only 0 genes passing filtering (<20) for Endo vs Smc\n",
      "17%...25%...33%...WARNING: only 11 genes passing filtering (<20) for PVALB vs SST\n",
      "42%...WARNING: only 15 genes passing filtering (<20) for SST vs PVALB\n",
      "WARNING: only 14 genes passing filtering (<20) for SST vs VIP\n",
      "50%...58%...WARNING: only 12 genes passing filtering (<20) for VIP vs SST\n",
      "67%...75%...WARNING: only 16 genes passing filtering (<20) for eL4 vs eL2/3\n",
      "WARNING: only 10 genes passing filtering (<20) for eL4 vs eL5\n",
      "83%...WARNING: only 18 genes passing filtering (<20) for eL5 vs eL2/3\n",
      "WARNING: only 17 genes passing filtering (<20) for eL5 vs eL4\n",
      "92%...WARNING: only 10 genes passing filtering (<20) for eL6 vs eL5\n",
      "finally selected 497 cell-type marker genes\n",
      "\n",
      "\n",
      "use union of highly variable gene list and cell-type marker gene list derived from scRNA-seq data, finally get 573 genes for downstream analysis\n",
      "\n",
      "start CVAE building...\n",
      "\n",
      "generate 0 pseudo-spots containing 2 to 8 cells from scRNA-seq cells...\n",
      "generate 0 pseudo-spots containing 2 to 6 spots from spatial spots...\n",
      "\n",
      "WARNING: first apply log transformation on sequencing depth normalized gene expressions, followed by Min-Max scaling\n",
      "\n",
      "                         |  training | validation\n",
      "spatial spots            |       581 |         0\n",
      "spatial pseudo-spots     |         0 |         0\n",
      "scRNA-seq cells          |      2002 |         0\n",
      "scRNA-seq pseudo-spots   |         0 |         0\n",
      "\n",
      "scaling inputs to range 0 to 10\n",
      "\n",
      "CVAE structure:\n",
      "Encoder: 574 - 143 - 36\n",
      "Decoder: 37 - 143 - 573\n",
      "\n",
      "\n",
      "Start training without validation data...\n",
      "\n",
      "Train on 2583 samples\n",
      "Epoch 1/1000\n",
      "2583/2583 [==============================] - 0s 59us/sample - loss: 868.9794 - reconstruction_loss: 793.0261 - KL_loss: 75.9534 - lr: 0.0030\n",
      "Epoch 2/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 780.6018 - reconstruction_loss: 738.4713 - KL_loss: 42.1305 - lr: 0.0030\n",
      "Epoch 3/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 726.1848 - reconstruction_loss: 686.7756 - KL_loss: 39.4091 - lr: 0.0030\n",
      "Epoch 4/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 677.8771 - reconstruction_loss: 625.3817 - KL_loss: 52.4955 - lr: 0.0030\n",
      "Epoch 5/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 630.9994 - reconstruction_loss: 577.6336 - KL_loss: 53.3658 - lr: 0.0030\n",
      "Epoch 6/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 589.3044 - reconstruction_loss: 539.9737 - KL_loss: 49.3307 - lr: 0.0030\n",
      "Epoch 7/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 555.6402 - reconstruction_loss: 505.9495 - KL_loss: 49.6907 - lr: 0.0030\n",
      "Epoch 8/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 527.9164 - reconstruction_loss: 477.7158 - KL_loss: 50.2006 - lr: 0.0030\n",
      "Epoch 9/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 504.8874 - reconstruction_loss: 457.9361 - KL_loss: 46.9512 - lr: 0.0030\n",
      "Epoch 10/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 486.1508 - reconstruction_loss: 441.8174 - KL_loss: 44.3333 - lr: 0.0030\n",
      "Epoch 11/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 468.1455 - reconstruction_loss: 427.0445 - KL_loss: 41.1009 - lr: 0.0030\n",
      "Epoch 12/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 454.2974 - reconstruction_loss: 416.6849 - KL_loss: 37.6125 - lr: 0.0030\n",
      "Epoch 13/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 441.8356 - reconstruction_loss: 404.7179 - KL_loss: 37.1178 - lr: 0.0030\n",
      "Epoch 14/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 431.3598 - reconstruction_loss: 396.4161 - KL_loss: 34.9437 - lr: 0.0030\n",
      "Epoch 15/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 421.9331 - reconstruction_loss: 388.6323 - KL_loss: 33.3009 - lr: 0.0030\n",
      "Epoch 16/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 415.7846 - reconstruction_loss: 381.8353 - KL_loss: 33.9494 - lr: 0.0030\n",
      "Epoch 17/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 407.9460 - reconstruction_loss: 374.7589 - KL_loss: 33.1871 - lr: 0.0030\n",
      "Epoch 18/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 405.3874 - reconstruction_loss: 374.1451 - KL_loss: 31.2423 - lr: 0.0030\n",
      "Epoch 19/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 398.8292 - reconstruction_loss: 367.7714 - KL_loss: 31.0579 - lr: 0.0030\n",
      "Epoch 20/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 395.7973 - reconstruction_loss: 363.4550 - KL_loss: 32.3423 - lr: 0.0030\n",
      "Epoch 21/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 392.5436 - reconstruction_loss: 360.0032 - KL_loss: 32.5404 - lr: 0.0030\n",
      "Epoch 22/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 385.8372 - reconstruction_loss: 354.3478 - KL_loss: 31.4895 - lr: 0.0030\n",
      "Epoch 23/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 382.3459 - reconstruction_loss: 351.7758 - KL_loss: 30.5700 - lr: 0.0030\n",
      "Epoch 24/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 379.2408 - reconstruction_loss: 348.7793 - KL_loss: 30.4615 - lr: 0.0030\n",
      "Epoch 25/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 375.5267 - reconstruction_loss: 345.8806 - KL_loss: 29.6461 - lr: 0.0030\n",
      "Epoch 26/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 373.6092 - reconstruction_loss: 345.3824 - KL_loss: 28.2268 - lr: 0.0030\n",
      "Epoch 27/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 370.3871 - reconstruction_loss: 342.5226 - KL_loss: 27.8645 - lr: 0.0030\n",
      "Epoch 28/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 370.2854 - reconstruction_loss: 341.6274 - KL_loss: 28.6580 - lr: 0.0030\n",
      "Epoch 29/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 368.6001 - reconstruction_loss: 339.6983 - KL_loss: 28.9017 - lr: 0.0030\n",
      "Epoch 30/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 363.0847 - reconstruction_loss: 334.4135 - KL_loss: 28.6712 - lr: 0.0030\n",
      "Epoch 31/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 361.5260 - reconstruction_loss: 332.5788 - KL_loss: 28.9473 - lr: 0.0030\n",
      "Epoch 32/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 359.1412 - reconstruction_loss: 329.7795 - KL_loss: 29.3616 - lr: 0.0030\n",
      "Epoch 33/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 357.0680 - reconstruction_loss: 327.7663 - KL_loss: 29.3017 - lr: 0.0030\n",
      "Epoch 34/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 354.5867 - reconstruction_loss: 325.0615 - KL_loss: 29.5252 - lr: 0.0030\n",
      "Epoch 35/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 353.2169 - reconstruction_loss: 323.1564 - KL_loss: 30.0605 - lr: 0.0030\n",
      "Epoch 36/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 350.6609 - reconstruction_loss: 320.8701 - KL_loss: 29.7907 - lr: 0.0030\n",
      "Epoch 37/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 349.5681 - reconstruction_loss: 320.1787 - KL_loss: 29.3893 - lr: 0.0030\n",
      "Epoch 38/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 347.1683 - reconstruction_loss: 317.4066 - KL_loss: 29.7617 - lr: 0.0030\n",
      "Epoch 39/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 345.7688 - reconstruction_loss: 315.3088 - KL_loss: 30.4600 - lr: 0.0030\n",
      "Epoch 40/1000\n",
      "2583/2583 [==============================] - 0s 14us/sample - loss: 343.4862 - reconstruction_loss: 312.7580 - KL_loss: 30.7281 - lr: 0.0030\n",
      "Epoch 41/1000\n",
      "2583/2583 [==============================] - 0s 16us/sample - loss: 341.8504 - reconstruction_loss: 311.0769 - KL_loss: 30.7735 - lr: 0.0030\n",
      "Epoch 42/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 340.1531 - reconstruction_loss: 308.7564 - KL_loss: 31.3967 - lr: 0.0030\n",
      "Epoch 43/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 338.4553 - reconstruction_loss: 306.7701 - KL_loss: 31.6852 - lr: 0.0030\n",
      "Epoch 44/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 337.3392 - reconstruction_loss: 305.3878 - KL_loss: 31.9514 - lr: 0.0030\n",
      "Epoch 45/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 334.9052 - reconstruction_loss: 302.5238 - KL_loss: 32.3814 - lr: 0.0030\n",
      "Epoch 46/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 333.3249 - reconstruction_loss: 300.7704 - KL_loss: 32.5545 - lr: 0.0030\n",
      "Epoch 47/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 332.8301 - reconstruction_loss: 300.3519 - KL_loss: 32.4782 - lr: 0.0030\n",
      "Epoch 48/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 330.2896 - reconstruction_loss: 297.5692 - KL_loss: 32.7204 - lr: 0.0030\n",
      "Epoch 49/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 329.3705 - reconstruction_loss: 295.9624 - KL_loss: 33.4081 - lr: 0.0030\n",
      "Epoch 50/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 328.2289 - reconstruction_loss: 294.8057 - KL_loss: 33.4233 - lr: 0.0030\n",
      "Epoch 51/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 326.0083 - reconstruction_loss: 292.7313 - KL_loss: 33.2770 - lr: 0.0030\n",
      "Epoch 52/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 325.0807 - reconstruction_loss: 291.1469 - KL_loss: 33.9339 - lr: 0.0030\n",
      "Epoch 53/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 323.5213 - reconstruction_loss: 288.8483 - KL_loss: 34.6730 - lr: 0.0030\n",
      "Epoch 54/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 321.9963 - reconstruction_loss: 287.2690 - KL_loss: 34.7274 - lr: 0.0030\n",
      "Epoch 55/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 320.6558 - reconstruction_loss: 286.1223 - KL_loss: 34.5334 - lr: 0.0030\n",
      "Epoch 56/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 319.3735 - reconstruction_loss: 284.7457 - KL_loss: 34.6278 - lr: 0.0030\n",
      "Epoch 57/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 318.1453 - reconstruction_loss: 283.0735 - KL_loss: 35.0718 - lr: 0.0030\n",
      "Epoch 58/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 316.7088 - reconstruction_loss: 280.8354 - KL_loss: 35.8734 - lr: 0.0030\n",
      "Epoch 59/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 315.0417 - reconstruction_loss: 278.9884 - KL_loss: 36.0533 - lr: 0.0030\n",
      "Epoch 60/1000\n",
      "2583/2583 [==============================] - 0s 19us/sample - loss: 314.3155 - reconstruction_loss: 278.5446 - KL_loss: 35.7708 - lr: 0.0030\n",
      "Epoch 61/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 312.2531 - reconstruction_loss: 276.2866 - KL_loss: 35.9665 - lr: 0.0030\n",
      "Epoch 62/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 311.9289 - reconstruction_loss: 274.8732 - KL_loss: 37.0557 - lr: 0.0030\n",
      "Epoch 63/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 310.9835 - reconstruction_loss: 273.6601 - KL_loss: 37.3234 - lr: 0.0030\n",
      "Epoch 64/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 308.9616 - reconstruction_loss: 272.3860 - KL_loss: 36.5756 - lr: 0.0030\n",
      "Epoch 65/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 308.2525 - reconstruction_loss: 271.6273 - KL_loss: 36.6251 - lr: 0.0030\n",
      "Epoch 66/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 307.0104 - reconstruction_loss: 269.9756 - KL_loss: 37.0349 - lr: 0.0030\n",
      "Epoch 67/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 305.4478 - reconstruction_loss: 267.7626 - KL_loss: 37.6852 - lr: 0.0030\n",
      "Epoch 68/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 304.3077 - reconstruction_loss: 265.6819 - KL_loss: 38.6258 - lr: 0.0030\n",
      "Epoch 69/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 303.6165 - reconstruction_loss: 264.9016 - KL_loss: 38.7149 - lr: 0.0030\n",
      "Epoch 70/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 301.8092 - reconstruction_loss: 263.5341 - KL_loss: 38.2752 - lr: 0.0030\n",
      "Epoch 71/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 301.5111 - reconstruction_loss: 263.1771 - KL_loss: 38.3340 - lr: 0.0030\n",
      "Epoch 72/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 299.5482 - reconstruction_loss: 260.9282 - KL_loss: 38.6200 - lr: 0.0030\n",
      "Epoch 73/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 298.6057 - reconstruction_loss: 259.4460 - KL_loss: 39.1597 - lr: 0.0030\n",
      "Epoch 74/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 298.0858 - reconstruction_loss: 258.5716 - KL_loss: 39.5142 - lr: 0.0030\n",
      "Epoch 75/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 296.4442 - reconstruction_loss: 257.1527 - KL_loss: 39.2914 - lr: 0.0030\n",
      "Epoch 76/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 295.2628 - reconstruction_loss: 256.0350 - KL_loss: 39.2278 - lr: 0.0030\n",
      "Epoch 77/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 294.3865 - reconstruction_loss: 254.7224 - KL_loss: 39.6641 - lr: 0.0030\n",
      "Epoch 78/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 292.9537 - reconstruction_loss: 252.7940 - KL_loss: 40.1597 - lr: 0.0030\n",
      "Epoch 79/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 292.5172 - reconstruction_loss: 251.9408 - KL_loss: 40.5764 - lr: 0.0030\n",
      "Epoch 80/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 291.1376 - reconstruction_loss: 250.4389 - KL_loss: 40.6987 - lr: 0.0030\n",
      "Epoch 81/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 290.1786 - reconstruction_loss: 249.5482 - KL_loss: 40.6304 - lr: 0.0030\n",
      "Epoch 82/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 289.3416 - reconstruction_loss: 248.3985 - KL_loss: 40.9431 - lr: 0.0030\n",
      "Epoch 83/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 288.5151 - reconstruction_loss: 247.3580 - KL_loss: 41.1571 - lr: 0.0030\n",
      "Epoch 84/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 286.7179 - reconstruction_loss: 245.3105 - KL_loss: 41.4074 - lr: 0.0030\n",
      "Epoch 85/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 286.1177 - reconstruction_loss: 244.6564 - KL_loss: 41.4613 - lr: 0.0030\n",
      "Epoch 86/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 285.0907 - reconstruction_loss: 243.5350 - KL_loss: 41.5556 - lr: 0.0030\n",
      "Epoch 87/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 284.2331 - reconstruction_loss: 242.2912 - KL_loss: 41.9419 - lr: 0.0030\n",
      "Epoch 88/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 283.2341 - reconstruction_loss: 240.8868 - KL_loss: 42.3474 - lr: 0.0030\n",
      "Epoch 89/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 282.5132 - reconstruction_loss: 239.8002 - KL_loss: 42.7130 - lr: 0.0030\n",
      "Epoch 90/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 281.4646 - reconstruction_loss: 238.7342 - KL_loss: 42.7304 - lr: 0.0030\n",
      "Epoch 91/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 280.2063 - reconstruction_loss: 237.4650 - KL_loss: 42.7413 - lr: 0.0030\n",
      "Epoch 92/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 279.5821 - reconstruction_loss: 236.6875 - KL_loss: 42.8946 - lr: 0.0030\n",
      "Epoch 93/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 278.8964 - reconstruction_loss: 235.8943 - KL_loss: 43.0021 - lr: 0.0030\n",
      "Epoch 94/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 278.2610 - reconstruction_loss: 235.0813 - KL_loss: 43.1797 - lr: 0.0030\n",
      "Epoch 95/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 276.7009 - reconstruction_loss: 233.4676 - KL_loss: 43.2333 - lr: 0.0030\n",
      "Epoch 96/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 275.7839 - reconstruction_loss: 232.3663 - KL_loss: 43.4177 - lr: 0.0030\n",
      "Epoch 97/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 275.4050 - reconstruction_loss: 231.7966 - KL_loss: 43.6084 - lr: 0.0030\n",
      "Epoch 98/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 274.3089 - reconstruction_loss: 230.3849 - KL_loss: 43.9239 - lr: 0.0030\n",
      "Epoch 99/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 273.4344 - reconstruction_loss: 229.1197 - KL_loss: 44.3148 - lr: 0.0030\n",
      "Epoch 100/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 272.6319 - reconstruction_loss: 228.3556 - KL_loss: 44.2763 - lr: 0.0030\n",
      "Epoch 101/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 271.9945 - reconstruction_loss: 227.6437 - KL_loss: 44.3508 - lr: 0.0030\n",
      "Epoch 102/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 270.9928 - reconstruction_loss: 226.5689 - KL_loss: 44.4239 - lr: 0.0030\n",
      "Epoch 103/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 270.2643 - reconstruction_loss: 225.6651 - KL_loss: 44.5992 - lr: 0.0030\n",
      "Epoch 104/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 269.6458 - reconstruction_loss: 224.9239 - KL_loss: 44.7219 - lr: 0.0030\n",
      "Epoch 105/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 268.7486 - reconstruction_loss: 223.8854 - KL_loss: 44.8632 - lr: 0.0030\n",
      "Epoch 106/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 268.2426 - reconstruction_loss: 223.1582 - KL_loss: 45.0844 - lr: 0.0030\n",
      "Epoch 107/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 266.9667 - reconstruction_loss: 221.8904 - KL_loss: 45.0763 - lr: 0.0030\n",
      "Epoch 108/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 267.5455 - reconstruction_loss: 222.0944 - KL_loss: 45.4511 - lr: 0.0030\n",
      "Epoch 109/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 266.2752 - reconstruction_loss: 220.6535 - KL_loss: 45.6217 - lr: 0.0030\n",
      "Epoch 110/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 266.1742 - reconstruction_loss: 220.5625 - KL_loss: 45.6117 - lr: 0.0030\n",
      "Epoch 111/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 265.3191 - reconstruction_loss: 219.6384 - KL_loss: 45.6807 - lr: 0.0030\n",
      "Epoch 112/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 264.4188 - reconstruction_loss: 218.5677 - KL_loss: 45.8510 - lr: 0.0030\n",
      "Epoch 113/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 263.8543 - reconstruction_loss: 218.0329 - KL_loss: 45.8214 - lr: 0.0030\n",
      "Epoch 114/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 262.9950 - reconstruction_loss: 217.2025 - KL_loss: 45.7924 - lr: 0.0030\n",
      "Epoch 115/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 262.3978 - reconstruction_loss: 216.3132 - KL_loss: 46.0847 - lr: 0.0030\n",
      "Epoch 116/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 261.3554 - reconstruction_loss: 215.1535 - KL_loss: 46.2020 - lr: 0.0030\n",
      "Epoch 117/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 261.2842 - reconstruction_loss: 215.0370 - KL_loss: 46.2472 - lr: 0.0030\n",
      "Epoch 118/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 260.1651 - reconstruction_loss: 213.7459 - KL_loss: 46.4191 - lr: 0.0030\n",
      "Epoch 119/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 259.9377 - reconstruction_loss: 213.3528 - KL_loss: 46.5849 - lr: 0.0030\n",
      "Epoch 120/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 258.9501 - reconstruction_loss: 212.2938 - KL_loss: 46.6563 - lr: 0.0030\n",
      "Epoch 121/1000\n",
      "2583/2583 [==============================] - 0s 15us/sample - loss: 258.8503 - reconstruction_loss: 212.1787 - KL_loss: 46.6715 - lr: 0.0030\n",
      "Epoch 122/1000\n",
      "2583/2583 [==============================] - 0s 16us/sample - loss: 257.7845 - reconstruction_loss: 211.1333 - KL_loss: 46.6512 - lr: 0.0030\n",
      "Epoch 123/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 257.8267 - reconstruction_loss: 211.1566 - KL_loss: 46.6701 - lr: 0.0030\n",
      "Epoch 124/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 257.3580 - reconstruction_loss: 210.5586 - KL_loss: 46.7994 - lr: 0.0030\n",
      "Epoch 125/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 256.3898 - reconstruction_loss: 209.4132 - KL_loss: 46.9766 - lr: 0.0030\n",
      "Epoch 126/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 256.1918 - reconstruction_loss: 209.2343 - KL_loss: 46.9575 - lr: 0.0030\n",
      "Epoch 127/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 254.8981 - reconstruction_loss: 207.8887 - KL_loss: 47.0094 - lr: 0.0030\n",
      "Epoch 128/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 254.9690 - reconstruction_loss: 207.5013 - KL_loss: 47.4677 - lr: 0.0030\n",
      "Epoch 129/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 254.2506 - reconstruction_loss: 206.7516 - KL_loss: 47.4990 - lr: 0.0030\n",
      "Epoch 130/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 254.0617 - reconstruction_loss: 206.7254 - KL_loss: 47.3363 - lr: 0.0030\n",
      "Epoch 131/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 253.6648 - reconstruction_loss: 206.3835 - KL_loss: 47.2814 - lr: 0.0030\n",
      "Epoch 132/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 252.6676 - reconstruction_loss: 205.1530 - KL_loss: 47.5146 - lr: 0.0030\n",
      "Epoch 133/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 252.7250 - reconstruction_loss: 205.1264 - KL_loss: 47.5985 - lr: 0.0030\n",
      "Epoch 134/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 251.5899 - reconstruction_loss: 204.1758 - KL_loss: 47.4141 - lr: 0.0030\n",
      "Epoch 135/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 251.5872 - reconstruction_loss: 204.1365 - KL_loss: 47.4506 - lr: 0.0030\n",
      "Epoch 136/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 250.5410 - reconstruction_loss: 202.7156 - KL_loss: 47.8254 - lr: 0.0030\n",
      "Epoch 137/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 250.2638 - reconstruction_loss: 202.1876 - KL_loss: 48.0762 - lr: 0.0030\n",
      "Epoch 138/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 249.9396 - reconstruction_loss: 201.8772 - KL_loss: 48.0624 - lr: 0.0030\n",
      "Epoch 139/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 248.9674 - reconstruction_loss: 201.0473 - KL_loss: 47.9201 - lr: 0.0030\n",
      "Epoch 140/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 248.7729 - reconstruction_loss: 200.9348 - KL_loss: 47.8381 - lr: 0.0030\n",
      "Epoch 141/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 248.1121 - reconstruction_loss: 200.0858 - KL_loss: 48.0263 - lr: 0.0030\n",
      "Epoch 142/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 247.5712 - reconstruction_loss: 199.4048 - KL_loss: 48.1664 - lr: 0.0030\n",
      "Epoch 143/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 247.7234 - reconstruction_loss: 199.5139 - KL_loss: 48.2095 - lr: 0.0030\n",
      "Epoch 144/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 246.8623 - reconstruction_loss: 198.5328 - KL_loss: 48.3295 - lr: 0.0030\n",
      "Epoch 145/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 246.1416 - reconstruction_loss: 197.4965 - KL_loss: 48.6451 - lr: 0.0030\n",
      "Epoch 146/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 245.6789 - reconstruction_loss: 197.0027 - KL_loss: 48.6763 - lr: 0.0030\n",
      "Epoch 147/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 245.1081 - reconstruction_loss: 196.6616 - KL_loss: 48.4465 - lr: 0.0030\n",
      "Epoch 148/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 245.1870 - reconstruction_loss: 196.8638 - KL_loss: 48.3232 - lr: 0.0030\n",
      "Epoch 149/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 244.7747 - reconstruction_loss: 196.4898 - KL_loss: 48.2849 - lr: 0.0030\n",
      "Epoch 150/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 243.8023 - reconstruction_loss: 195.4876 - KL_loss: 48.3147 - lr: 0.0030\n",
      "Epoch 151/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 243.7647 - reconstruction_loss: 195.0615 - KL_loss: 48.7032 - lr: 0.0030\n",
      "Epoch 152/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 243.0309 - reconstruction_loss: 194.1257 - KL_loss: 48.9051 - lr: 0.0030\n",
      "Epoch 153/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 242.5034 - reconstruction_loss: 193.5428 - KL_loss: 48.9605 - lr: 0.0030\n",
      "Epoch 154/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 242.4406 - reconstruction_loss: 193.4137 - KL_loss: 49.0268 - lr: 0.0030\n",
      "Epoch 155/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 242.2889 - reconstruction_loss: 193.1521 - KL_loss: 49.1368 - lr: 0.0030\n",
      "Epoch 156/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 241.2782 - reconstruction_loss: 192.1766 - KL_loss: 49.1016 - lr: 0.0030\n",
      "Epoch 157/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 241.6245 - reconstruction_loss: 192.6277 - KL_loss: 48.9968 - lr: 0.0030\n",
      "Epoch 158/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 240.7762 - reconstruction_loss: 191.7962 - KL_loss: 48.9799 - lr: 0.0030\n",
      "Epoch 159/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 241.2988 - reconstruction_loss: 191.8851 - KL_loss: 49.4137 - lr: 0.0030\n",
      "Epoch 160/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 240.6412 - reconstruction_loss: 191.0743 - KL_loss: 49.5668 - lr: 0.0030\n",
      "Epoch 161/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 240.7739 - reconstruction_loss: 191.3681 - KL_loss: 49.4058 - lr: 0.0030\n",
      "Epoch 162/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 240.2391 - reconstruction_loss: 190.8470 - KL_loss: 49.3921 - lr: 0.0030\n",
      "Epoch 163/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 239.4846 - reconstruction_loss: 190.1025 - KL_loss: 49.3821 - lr: 0.0030\n",
      "Epoch 164/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 239.9049 - reconstruction_loss: 190.2362 - KL_loss: 49.6687 - lr: 0.0030\n",
      "Epoch 165/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 238.4511 - reconstruction_loss: 188.8539 - KL_loss: 49.5971 - lr: 0.0030\n",
      "Epoch 166/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 239.9651 - reconstruction_loss: 190.6427 - KL_loss: 49.3224 - lr: 0.0030\n",
      "Epoch 167/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 238.8730 - reconstruction_loss: 189.6337 - KL_loss: 49.2393 - lr: 0.0030\n",
      "Epoch 168/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 238.8663 - reconstruction_loss: 189.4006 - KL_loss: 49.4657 - lr: 0.0030\n",
      "Epoch 169/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 238.0424 - reconstruction_loss: 188.2316 - KL_loss: 49.8107 - lr: 0.0030\n",
      "Epoch 170/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 238.3888 - reconstruction_loss: 188.5542 - KL_loss: 49.8346 - lr: 0.0030\n",
      "Epoch 171/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 237.1884 - reconstruction_loss: 187.4200 - KL_loss: 49.7685 - lr: 0.0030\n",
      "Epoch 172/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 237.8789 - reconstruction_loss: 187.9351 - KL_loss: 49.9437 - lr: 0.0030\n",
      "Epoch 173/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 237.1228 - reconstruction_loss: 187.2290 - KL_loss: 49.8938 - lr: 0.0030\n",
      "Epoch 174/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 236.5271 - reconstruction_loss: 186.8727 - KL_loss: 49.6544 - lr: 0.0030\n",
      "Epoch 175/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 236.3662 - reconstruction_loss: 186.8035 - KL_loss: 49.5627 - lr: 0.0030\n",
      "Epoch 176/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 235.3667 - reconstruction_loss: 185.8643 - KL_loss: 49.5024 - lr: 0.0030\n",
      "Epoch 177/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 235.0945 - reconstruction_loss: 185.4508 - KL_loss: 49.6436 - lr: 0.0030\n",
      "Epoch 178/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 235.5714 - reconstruction_loss: 185.8112 - KL_loss: 49.7603 - lr: 0.0030\n",
      "Epoch 179/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 234.3125 - reconstruction_loss: 184.4247 - KL_loss: 49.8878 - lr: 0.0030\n",
      "Epoch 180/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 234.7724 - reconstruction_loss: 184.5960 - KL_loss: 50.1764 - lr: 0.0030\n",
      "Epoch 181/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 234.3845 - reconstruction_loss: 184.0920 - KL_loss: 50.2925 - lr: 0.0030\n",
      "Epoch 182/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 234.3122 - reconstruction_loss: 183.9205 - KL_loss: 50.3916 - lr: 0.0030\n",
      "Epoch 183/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 234.0056 - reconstruction_loss: 183.7237 - KL_loss: 50.2819 - lr: 0.0030\n",
      "Epoch 184/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 234.0473 - reconstruction_loss: 183.7617 - KL_loss: 50.2856 - lr: 0.0030\n",
      "Epoch 185/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 233.6645 - reconstruction_loss: 183.3571 - KL_loss: 50.3073 - lr: 0.0030\n",
      "Epoch 186/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 233.0899 - reconstruction_loss: 182.9183 - KL_loss: 50.1716 - lr: 0.0030\n",
      "Epoch 187/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 232.1827 - reconstruction_loss: 182.0463 - KL_loss: 50.1365 - lr: 0.0030\n",
      "Epoch 188/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 232.8078 - reconstruction_loss: 182.6137 - KL_loss: 50.1941 - lr: 0.0030\n",
      "Epoch 189/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 232.6240 - reconstruction_loss: 182.3697 - KL_loss: 50.2543 - lr: 0.0030\n",
      "Epoch 190/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 231.6668 - reconstruction_loss: 181.3447 - KL_loss: 50.3220 - lr: 0.0030\n",
      "Epoch 191/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 231.8709 - reconstruction_loss: 181.3110 - KL_loss: 50.5599 - lr: 0.0030\n",
      "Epoch 192/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 230.6604 - reconstruction_loss: 179.9825 - KL_loss: 50.6779 - lr: 0.0030\n",
      "Epoch 193/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 231.4680 - reconstruction_loss: 180.9864 - KL_loss: 50.4816 - lr: 0.0030\n",
      "Epoch 194/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 230.3576 - reconstruction_loss: 180.0667 - KL_loss: 50.2909 - lr: 0.0030\n",
      "Epoch 195/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 230.8294 - reconstruction_loss: 180.4604 - KL_loss: 50.3689 - lr: 0.0030\n",
      "Epoch 196/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 230.0351 - reconstruction_loss: 179.4705 - KL_loss: 50.5646 - lr: 0.0030\n",
      "Epoch 197/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 230.8597 - reconstruction_loss: 180.1170 - KL_loss: 50.7427 - lr: 0.0030\n",
      "Epoch 198/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 230.4791 - reconstruction_loss: 179.6293 - KL_loss: 50.8498 - lr: 0.0030\n",
      "Epoch 199/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 229.9109 - reconstruction_loss: 178.8552 - KL_loss: 51.0556 - lr: 0.0030\n",
      "Epoch 200/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 229.9524 - reconstruction_loss: 178.9178 - KL_loss: 51.0345 - lr: 0.0030\n",
      "Epoch 201/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 229.4421 - reconstruction_loss: 178.5970 - KL_loss: 50.8451 - lr: 0.0030\n",
      "Epoch 202/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 228.6167 - reconstruction_loss: 178.0220 - KL_loss: 50.5946 - lr: 0.0030\n",
      "Epoch 203/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 228.6308 - reconstruction_loss: 178.3998 - KL_loss: 50.2311 - lr: 0.0030\n",
      "Epoch 204/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 228.4487 - reconstruction_loss: 178.3149 - KL_loss: 50.1338 - lr: 0.0030\n",
      "Epoch 205/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 228.2299 - reconstruction_loss: 177.7100 - KL_loss: 50.5200 - lr: 0.0030\n",
      "Epoch 206/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 228.4361 - reconstruction_loss: 177.5844 - KL_loss: 50.8517 - lr: 0.0030\n",
      "Epoch 207/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 227.4848 - reconstruction_loss: 176.5030 - KL_loss: 50.9819 - lr: 0.0030\n",
      "Epoch 208/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 227.9102 - reconstruction_loss: 176.7346 - KL_loss: 51.1756 - lr: 0.0030\n",
      "Epoch 209/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 227.0925 - reconstruction_loss: 175.8526 - KL_loss: 51.2399 - lr: 0.0030\n",
      "Epoch 210/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 227.2962 - reconstruction_loss: 175.9492 - KL_loss: 51.3470 - lr: 0.0030\n",
      "Epoch 211/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 227.3394 - reconstruction_loss: 176.1457 - KL_loss: 51.1937 - lr: 0.0030\n",
      "Epoch 212/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 226.8114 - reconstruction_loss: 176.1627 - KL_loss: 50.6487 - lr: 0.0030\n",
      "Epoch 213/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 226.8989 - reconstruction_loss: 176.3672 - KL_loss: 50.5317 - lr: 0.0030\n",
      "Epoch 214/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 226.1509 - reconstruction_loss: 175.3621 - KL_loss: 50.7888 - lr: 0.0030\n",
      "Epoch 215/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 226.3123 - reconstruction_loss: 175.1163 - KL_loss: 51.1961 - lr: 0.0030\n",
      "Epoch 216/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 226.0162 - reconstruction_loss: 174.4740 - KL_loss: 51.5423 - lr: 0.0030\n",
      "Epoch 217/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 225.1254 - reconstruction_loss: 173.7111 - KL_loss: 51.4143 - lr: 0.0030\n",
      "Epoch 218/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 225.4293 - reconstruction_loss: 174.1645 - KL_loss: 51.2648 - lr: 0.0030\n",
      "Epoch 219/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 225.6810 - reconstruction_loss: 174.5919 - KL_loss: 51.0891 - lr: 0.0030\n",
      "Epoch 220/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 224.9410 - reconstruction_loss: 174.0522 - KL_loss: 50.8888 - lr: 0.0030\n",
      "Epoch 221/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 224.8632 - reconstruction_loss: 173.8343 - KL_loss: 51.0289 - lr: 0.0030\n",
      "Epoch 222/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 224.6483 - reconstruction_loss: 173.5268 - KL_loss: 51.1215 - lr: 0.0030\n",
      "Epoch 223/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 224.6770 - reconstruction_loss: 173.4119 - KL_loss: 51.2651 - lr: 0.0030\n",
      "Epoch 224/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 224.0463 - reconstruction_loss: 172.5259 - KL_loss: 51.5204 - lr: 0.0030\n",
      "Epoch 225/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 223.8391 - reconstruction_loss: 172.1604 - KL_loss: 51.6787 - lr: 0.0030\n",
      "Epoch 226/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 223.6479 - reconstruction_loss: 172.0516 - KL_loss: 51.5964 - lr: 0.0030\n",
      "Epoch 227/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 223.5823 - reconstruction_loss: 172.2318 - KL_loss: 51.3506 - lr: 0.0030\n",
      "Epoch 228/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 222.9835 - reconstruction_loss: 171.7301 - KL_loss: 51.2535 - lr: 0.0030\n",
      "Epoch 229/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 223.3916 - reconstruction_loss: 172.2962 - KL_loss: 51.0954 - lr: 0.0030\n",
      "Epoch 230/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 222.3036 - reconstruction_loss: 171.2493 - KL_loss: 51.0544 - lr: 0.0030\n",
      "Epoch 231/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 222.8484 - reconstruction_loss: 171.6934 - KL_loss: 51.1549 - lr: 0.0030\n",
      "Epoch 232/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 222.0561 - reconstruction_loss: 170.5872 - KL_loss: 51.4689 - lr: 0.0030\n",
      "Epoch 233/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 223.0915 - reconstruction_loss: 171.4053 - KL_loss: 51.6862 - lr: 0.0030\n",
      "Epoch 234/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 222.4115 - reconstruction_loss: 170.6921 - KL_loss: 51.7194 - lr: 0.0030\n",
      "Epoch 235/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 222.0659 - reconstruction_loss: 170.4958 - KL_loss: 51.5700 - lr: 0.0030\n",
      "Epoch 236/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 222.1105 - reconstruction_loss: 170.5587 - KL_loss: 51.5518 - lr: 0.0030\n",
      "Epoch 237/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 221.5771 - reconstruction_loss: 169.8791 - KL_loss: 51.6980 - lr: 0.0030\n",
      "Epoch 238/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 221.6384 - reconstruction_loss: 169.9576 - KL_loss: 51.6808 - lr: 0.0030\n",
      "Epoch 239/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 220.9945 - reconstruction_loss: 169.4379 - KL_loss: 51.5566 - lr: 0.0030\n",
      "Epoch 240/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 221.3316 - reconstruction_loss: 169.6223 - KL_loss: 51.7093 - lr: 0.0030\n",
      "Epoch 241/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 220.6667 - reconstruction_loss: 168.8101 - KL_loss: 51.8565 - lr: 0.0030\n",
      "Epoch 242/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 220.6841 - reconstruction_loss: 168.8360 - KL_loss: 51.8480 - lr: 0.0030\n",
      "Epoch 243/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 220.1272 - reconstruction_loss: 168.4703 - KL_loss: 51.6569 - lr: 0.0030\n",
      "Epoch 244/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 220.5344 - reconstruction_loss: 168.9378 - KL_loss: 51.5965 - lr: 0.0030\n",
      "Epoch 245/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 220.3425 - reconstruction_loss: 168.8171 - KL_loss: 51.5254 - lr: 0.0030\n",
      "Epoch 246/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 219.9668 - reconstruction_loss: 168.2678 - KL_loss: 51.6989 - lr: 0.0030\n",
      "Epoch 247/1000\n",
      "2583/2583 [==============================] - 0s 14us/sample - loss: 219.7016 - reconstruction_loss: 167.7974 - KL_loss: 51.9041 - lr: 0.0030\n",
      "Epoch 248/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 219.4090 - reconstruction_loss: 167.3807 - KL_loss: 52.0284 - lr: 0.0030\n",
      "Epoch 249/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 219.7254 - reconstruction_loss: 167.6995 - KL_loss: 52.0259 - lr: 0.0030\n",
      "Epoch 250/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 218.9803 - reconstruction_loss: 166.9619 - KL_loss: 52.0184 - lr: 0.0030\n",
      "Epoch 251/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 219.5742 - reconstruction_loss: 167.4470 - KL_loss: 52.1272 - lr: 0.0030\n",
      "Epoch 252/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 219.1490 - reconstruction_loss: 167.0652 - KL_loss: 52.0839 - lr: 0.0030\n",
      "Epoch 253/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 218.9705 - reconstruction_loss: 167.0587 - KL_loss: 51.9118 - lr: 0.0030\n",
      "Epoch 254/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 218.4940 - reconstruction_loss: 166.7529 - KL_loss: 51.7412 - lr: 0.0030\n",
      "Epoch 255/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 218.3346 - reconstruction_loss: 166.6776 - KL_loss: 51.6569 - lr: 0.0030\n",
      "Epoch 256/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 217.9276 - reconstruction_loss: 166.1668 - KL_loss: 51.7607 - lr: 0.0030\n",
      "Epoch 257/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 218.6658 - reconstruction_loss: 166.3657 - KL_loss: 52.3001 - lr: 0.0030\n",
      "Epoch 258/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 217.4388 - reconstruction_loss: 165.0400 - KL_loss: 52.3988 - lr: 0.0030\n",
      "Epoch 259/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 218.9744 - reconstruction_loss: 166.7979 - KL_loss: 52.1765 - lr: 0.0030\n",
      "Epoch 260/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 218.6587 - reconstruction_loss: 166.6828 - KL_loss: 51.9759 - lr: 0.0030\n",
      "Epoch 261/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 218.6393 - reconstruction_loss: 166.3841 - KL_loss: 52.2551 - lr: 0.0030\n",
      "Epoch 262/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 218.3324 - reconstruction_loss: 165.8811 - KL_loss: 52.4513 - lr: 0.0030\n",
      "Epoch 263/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 217.7747 - reconstruction_loss: 165.7024 - KL_loss: 52.0722 - lr: 0.0030\n",
      "Epoch 264/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 218.0251 - reconstruction_loss: 166.1974 - KL_loss: 51.8276 - lr: 0.0030\n",
      "Epoch 265/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 217.4870 - reconstruction_loss: 165.6599 - KL_loss: 51.8271 - lr: 0.0030\n",
      "Epoch 266/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 217.9101 - reconstruction_loss: 165.6200 - KL_loss: 52.2901 - lr: 0.0030\n",
      "Epoch 267/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 217.0521 - reconstruction_loss: 164.5200 - KL_loss: 52.5321 - lr: 0.0030\n",
      "Epoch 268/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 218.1159 - reconstruction_loss: 165.7018 - KL_loss: 52.4141 - lr: 0.0030\n",
      "Epoch 269/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 217.7965 - reconstruction_loss: 165.4073 - KL_loss: 52.3892 - lr: 0.0030\n",
      "Epoch 270/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 217.1572 - reconstruction_loss: 164.6364 - KL_loss: 52.5207 - lr: 0.0030\n",
      "Epoch 271/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 216.7773 - reconstruction_loss: 164.2287 - KL_loss: 52.5487 - lr: 0.0030\n",
      "Epoch 272/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 217.3957 - reconstruction_loss: 164.9181 - KL_loss: 52.4776 - lr: 0.0030\n",
      "Epoch 273/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 216.0335 - reconstruction_loss: 163.9539 - KL_loss: 52.0796 - lr: 0.0030\n",
      "Epoch 274/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 218.4747 - reconstruction_loss: 166.4454 - KL_loss: 52.0293 - lr: 0.0030\n",
      "Epoch 275/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 218.1052 - reconstruction_loss: 165.8547 - KL_loss: 52.2505 - lr: 0.0030\n",
      "Epoch 276/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 216.0656 - reconstruction_loss: 163.6236 - KL_loss: 52.4419 - lr: 0.0030\n",
      "Epoch 277/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 217.2024 - reconstruction_loss: 164.7560 - KL_loss: 52.4464 - lr: 0.0030\n",
      "Epoch 278/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 215.8456 - reconstruction_loss: 163.5189 - KL_loss: 52.3267 - lr: 0.0030\n",
      "Epoch 279/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 217.3843 - reconstruction_loss: 164.9128 - KL_loss: 52.4715 - lr: 0.0030\n",
      "Epoch 280/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 216.4320 - reconstruction_loss: 163.7489 - KL_loss: 52.6831 - lr: 0.0030\n",
      "Epoch 281/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 216.7980 - reconstruction_loss: 164.1559 - KL_loss: 52.6421 - lr: 0.0030\n",
      "Epoch 282/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 216.1502 - reconstruction_loss: 163.5853 - KL_loss: 52.5649 - lr: 0.0030\n",
      "Epoch 283/1000\n",
      "2583/2583 [==============================] - 0s 14us/sample - loss: 216.3701 - reconstruction_loss: 163.8849 - KL_loss: 52.4852 - lr: 0.0030\n",
      "Epoch 284/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 216.1263 - reconstruction_loss: 163.7646 - KL_loss: 52.3616 - lr: 0.0030\n",
      "Epoch 285/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 216.1126 - reconstruction_loss: 163.7750 - KL_loss: 52.3376 - lr: 0.0030\n",
      "Epoch 286/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 215.8134 - reconstruction_loss: 163.1593 - KL_loss: 52.6541 - lr: 0.0030\n",
      "Epoch 287/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 215.3591 - reconstruction_loss: 162.4209 - KL_loss: 52.9382 - lr: 0.0030\n",
      "Epoch 288/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 214.8015 - reconstruction_loss: 161.9105 - KL_loss: 52.8910 - lr: 0.0030\n",
      "Epoch 289/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 215.0048 - reconstruction_loss: 162.2598 - KL_loss: 52.7450 - lr: 0.0030\n",
      "Epoch 290/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 214.1725 - reconstruction_loss: 161.4578 - KL_loss: 52.7147 - lr: 0.0030\n",
      "Epoch 291/1000\n",
      "2583/2583 [==============================] - 0s 56us/sample - loss: 214.4253 - reconstruction_loss: 161.8743 - KL_loss: 52.5510 - lr: 0.0030\n",
      "Epoch 292/1000\n",
      "2583/2583 [==============================] - 0s 14us/sample - loss: 214.0885 - reconstruction_loss: 161.7731 - KL_loss: 52.3154 - lr: 0.0030\n",
      "Epoch 293/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 213.6126 - reconstruction_loss: 161.4203 - KL_loss: 52.1924 - lr: 0.0030\n",
      "Epoch 294/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 214.2770 - reconstruction_loss: 161.6923 - KL_loss: 52.5847 - lr: 0.0030\n",
      "Epoch 295/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 213.2243 - reconstruction_loss: 160.4849 - KL_loss: 52.7394 - lr: 0.0030\n",
      "Epoch 296/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 214.9356 - reconstruction_loss: 162.3162 - KL_loss: 52.6194 - lr: 0.0030\n",
      "Epoch 297/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 214.0672 - reconstruction_loss: 161.2524 - KL_loss: 52.8148 - lr: 0.0030\n",
      "Epoch 298/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 214.2350 - reconstruction_loss: 161.1073 - KL_loss: 53.1277 - lr: 0.0030\n",
      "Epoch 299/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 213.8409 - reconstruction_loss: 160.7591 - KL_loss: 53.0818 - lr: 0.0030\n",
      "Epoch 300/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 214.0857 - reconstruction_loss: 161.4639 - KL_loss: 52.6218 - lr: 0.0030\n",
      "Epoch 301/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 212.9855 - reconstruction_loss: 160.5049 - KL_loss: 52.4806 - lr: 0.0030\n",
      "Epoch 302/1000\n",
      "2583/2583 [==============================] - 0s 15us/sample - loss: 214.1987 - reconstruction_loss: 161.4973 - KL_loss: 52.7014 - lr: 0.0030\n",
      "Epoch 303/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 213.4852 - reconstruction_loss: 160.6347 - KL_loss: 52.8505 - lr: 0.0030\n",
      "Epoch 304/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 213.6500 - reconstruction_loss: 160.7347 - KL_loss: 52.9153 - lr: 0.0030\n",
      "Epoch 305/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 212.9665 - reconstruction_loss: 159.8632 - KL_loss: 53.1033 - lr: 0.0030\n",
      "Epoch 306/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 212.7234 - reconstruction_loss: 159.4486 - KL_loss: 53.2748 - lr: 0.0030\n",
      "Epoch 307/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 212.3846 - reconstruction_loss: 159.3823 - KL_loss: 53.0022 - lr: 0.0030\n",
      "Epoch 308/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 212.4618 - reconstruction_loss: 159.7480 - KL_loss: 52.7139 - lr: 0.0030\n",
      "Epoch 309/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 212.2568 - reconstruction_loss: 159.7189 - KL_loss: 52.5379 - lr: 0.0030\n",
      "Epoch 310/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 212.2763 - reconstruction_loss: 159.5191 - KL_loss: 52.7572 - lr: 0.0030\n",
      "Epoch 311/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 211.6827 - reconstruction_loss: 158.7627 - KL_loss: 52.9200 - lr: 0.0030\n",
      "Epoch 312/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 211.6731 - reconstruction_loss: 158.6570 - KL_loss: 53.0161 - lr: 0.0030\n",
      "Epoch 313/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 211.8022 - reconstruction_loss: 158.7497 - KL_loss: 53.0525 - lr: 0.0030\n",
      "Epoch 314/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 211.5438 - reconstruction_loss: 158.3241 - KL_loss: 53.2197 - lr: 0.0030\n",
      "Epoch 315/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 211.5011 - reconstruction_loss: 158.1846 - KL_loss: 53.3165 - lr: 0.0030\n",
      "Epoch 316/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 211.3536 - reconstruction_loss: 158.3294 - KL_loss: 53.0241 - lr: 0.0030\n",
      "Epoch 317/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 211.0821 - reconstruction_loss: 158.1336 - KL_loss: 52.9486 - lr: 0.0030\n",
      "Epoch 318/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 211.3827 - reconstruction_loss: 158.5566 - KL_loss: 52.8260 - lr: 0.0030\n",
      "Epoch 319/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 210.7009 - reconstruction_loss: 157.5256 - KL_loss: 53.1753 - lr: 0.0030\n",
      "Epoch 320/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 210.6938 - reconstruction_loss: 157.4652 - KL_loss: 53.2287 - lr: 0.0030\n",
      "Epoch 321/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 210.4207 - reconstruction_loss: 157.3795 - KL_loss: 53.0412 - lr: 0.0030\n",
      "Epoch 322/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 211.2350 - reconstruction_loss: 158.1529 - KL_loss: 53.0821 - lr: 0.0030\n",
      "Epoch 323/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 210.0934 - reconstruction_loss: 157.1874 - KL_loss: 52.9060 - lr: 0.0030\n",
      "Epoch 324/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 211.8759 - reconstruction_loss: 158.8410 - KL_loss: 53.0349 - lr: 0.0030\n",
      "Epoch 325/1000\n",
      "2583/2583 [==============================] - 0s 14us/sample - loss: 211.9961 - reconstruction_loss: 158.8541 - KL_loss: 53.1419 - lr: 0.0030\n",
      "Epoch 326/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 210.7928 - reconstruction_loss: 157.6147 - KL_loss: 53.1781 - lr: 0.0030\n",
      "Epoch 327/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 211.3169 - reconstruction_loss: 158.1496 - KL_loss: 53.1674 - lr: 0.0030\n",
      "Epoch 328/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 210.5453 - reconstruction_loss: 157.4598 - KL_loss: 53.0856 - lr: 0.0030\n",
      "Epoch 329/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 210.9175 - reconstruction_loss: 157.5742 - KL_loss: 53.3432 - lr: 0.0030\n",
      "Epoch 330/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 210.6378 - reconstruction_loss: 157.0483 - KL_loss: 53.5894 - lr: 0.0030\n",
      "Epoch 331/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 210.4201 - reconstruction_loss: 157.0921 - KL_loss: 53.3280 - lr: 0.0030\n",
      "Epoch 332/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 210.1855 - reconstruction_loss: 157.2710 - KL_loss: 52.9146 - lr: 0.0030\n",
      "Epoch 333/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 209.7560 - reconstruction_loss: 157.1429 - KL_loss: 52.6131 - lr: 0.0030\n",
      "Epoch 334/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 209.4497 - reconstruction_loss: 156.5337 - KL_loss: 52.9160 - lr: 0.0030\n",
      "Epoch 335/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 209.7249 - reconstruction_loss: 156.4885 - KL_loss: 53.2364 - lr: 0.0030\n",
      "Epoch 336/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 208.9025 - reconstruction_loss: 155.6792 - KL_loss: 53.2234 - lr: 0.0030\n",
      "Epoch 337/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 209.7297 - reconstruction_loss: 156.7353 - KL_loss: 52.9944 - lr: 0.0030\n",
      "Epoch 338/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 209.6259 - reconstruction_loss: 156.3679 - KL_loss: 53.2579 - lr: 0.0030\n",
      "Epoch 339/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 208.9890 - reconstruction_loss: 155.2935 - KL_loss: 53.6955 - lr: 0.0030\n",
      "Epoch 340/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 209.3896 - reconstruction_loss: 155.4925 - KL_loss: 53.8971 - lr: 0.0030\n",
      "Epoch 341/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 208.7813 - reconstruction_loss: 155.3560 - KL_loss: 53.4252 - lr: 0.0030\n",
      "Epoch 342/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 209.6017 - reconstruction_loss: 156.5087 - KL_loss: 53.0930 - lr: 0.0030\n",
      "Epoch 343/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 209.3241 - reconstruction_loss: 156.2711 - KL_loss: 53.0529 - lr: 0.0030\n",
      "Epoch 344/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 208.4380 - reconstruction_loss: 155.2301 - KL_loss: 53.2079 - lr: 0.0030\n",
      "Epoch 345/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 208.9946 - reconstruction_loss: 155.4755 - KL_loss: 53.5191 - lr: 0.0030\n",
      "Epoch 346/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 208.1869 - reconstruction_loss: 154.3979 - KL_loss: 53.7890 - lr: 0.0030\n",
      "Epoch 347/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 208.2739 - reconstruction_loss: 154.7797 - KL_loss: 53.4942 - lr: 0.0030\n",
      "Epoch 348/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 208.5553 - reconstruction_loss: 155.1892 - KL_loss: 53.3661 - lr: 0.0030\n",
      "Epoch 349/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 207.9935 - reconstruction_loss: 154.7188 - KL_loss: 53.2747 - lr: 0.0030\n",
      "Epoch 350/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 208.8196 - reconstruction_loss: 155.3736 - KL_loss: 53.4459 - lr: 0.0030\n",
      "Epoch 351/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 207.9910 - reconstruction_loss: 154.6657 - KL_loss: 53.3253 - lr: 0.0030\n",
      "Epoch 352/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 208.1198 - reconstruction_loss: 154.9735 - KL_loss: 53.1463 - lr: 0.0030\n",
      "Epoch 353/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 208.3196 - reconstruction_loss: 155.0892 - KL_loss: 53.2304 - lr: 0.0030\n",
      "Epoch 354/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 208.3664 - reconstruction_loss: 155.0588 - KL_loss: 53.3076 - lr: 0.0030\n",
      "Epoch 355/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 207.9217 - reconstruction_loss: 154.3107 - KL_loss: 53.6110 - lr: 0.0030\n",
      "Epoch 356/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 207.9877 - reconstruction_loss: 154.1233 - KL_loss: 53.8644 - lr: 0.0030\n",
      "Epoch 357/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 207.7660 - reconstruction_loss: 153.8833 - KL_loss: 53.8827 - lr: 0.0030\n",
      "Epoch 358/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 207.4206 - reconstruction_loss: 153.4798 - KL_loss: 53.9409 - lr: 0.0030\n",
      "Epoch 359/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 207.2688 - reconstruction_loss: 153.6934 - KL_loss: 53.5754 - lr: 0.0030\n",
      "Epoch 360/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 207.5328 - reconstruction_loss: 154.1270 - KL_loss: 53.4058 - lr: 0.0030\n",
      "Epoch 361/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 206.9022 - reconstruction_loss: 153.5030 - KL_loss: 53.3992 - lr: 0.0030\n",
      "Epoch 362/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 206.9194 - reconstruction_loss: 153.4636 - KL_loss: 53.4558 - lr: 0.0030\n",
      "Epoch 363/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 206.5139 - reconstruction_loss: 153.0118 - KL_loss: 53.5022 - lr: 0.0030\n",
      "Epoch 364/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 207.4268 - reconstruction_loss: 154.1386 - KL_loss: 53.2881 - lr: 0.0030\n",
      "Epoch 365/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 206.9520 - reconstruction_loss: 153.4521 - KL_loss: 53.4998 - lr: 0.0030\n",
      "Epoch 366/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 207.1229 - reconstruction_loss: 153.1374 - KL_loss: 53.9855 - lr: 0.0030\n",
      "Epoch 367/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 206.8781 - reconstruction_loss: 152.7779 - KL_loss: 54.1002 - lr: 0.0030\n",
      "Epoch 368/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 206.3275 - reconstruction_loss: 152.4201 - KL_loss: 53.9073 - lr: 0.0030\n",
      "Epoch 369/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 206.5635 - reconstruction_loss: 152.9023 - KL_loss: 53.6612 - lr: 0.0030\n",
      "Epoch 370/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 206.0058 - reconstruction_loss: 152.2491 - KL_loss: 53.7567 - lr: 0.0030\n",
      "Epoch 371/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 205.9862 - reconstruction_loss: 152.1780 - KL_loss: 53.8082 - lr: 0.0030\n",
      "Epoch 372/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 206.0240 - reconstruction_loss: 152.4051 - KL_loss: 53.6189 - lr: 0.0030\n",
      "Epoch 373/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 206.2239 - reconstruction_loss: 152.5829 - KL_loss: 53.6411 - lr: 0.0030\n",
      "Epoch 374/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 205.9330 - reconstruction_loss: 152.1069 - KL_loss: 53.8261 - lr: 0.0030\n",
      "Epoch 375/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 205.8539 - reconstruction_loss: 151.6993 - KL_loss: 54.1546 - lr: 0.0030\n",
      "Epoch 376/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 206.2804 - reconstruction_loss: 152.1294 - KL_loss: 54.1510 - lr: 0.0030\n",
      "Epoch 377/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 205.9770 - reconstruction_loss: 151.8950 - KL_loss: 54.0820 - lr: 0.0030\n",
      "Epoch 378/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 205.6046 - reconstruction_loss: 151.6997 - KL_loss: 53.9048 - lr: 0.0030\n",
      "Epoch 379/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 205.5116 - reconstruction_loss: 151.8873 - KL_loss: 53.6243 - lr: 0.0030\n",
      "Epoch 380/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 205.0500 - reconstruction_loss: 151.5109 - KL_loss: 53.5391 - lr: 0.0030\n",
      "Epoch 381/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 205.2171 - reconstruction_loss: 151.6244 - KL_loss: 53.5927 - lr: 0.0030\n",
      "Epoch 382/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 205.1767 - reconstruction_loss: 151.3338 - KL_loss: 53.8429 - lr: 0.0030\n",
      "Epoch 383/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 204.7388 - reconstruction_loss: 150.8855 - KL_loss: 53.8532 - lr: 0.0030\n",
      "Epoch 384/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 205.0773 - reconstruction_loss: 151.0576 - KL_loss: 54.0197 - lr: 0.0030\n",
      "Epoch 385/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 204.7389 - reconstruction_loss: 150.5797 - KL_loss: 54.1592 - lr: 0.0030\n",
      "Epoch 386/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 204.8384 - reconstruction_loss: 150.4409 - KL_loss: 54.3976 - lr: 0.0030\n",
      "Epoch 387/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 204.7095 - reconstruction_loss: 150.4418 - KL_loss: 54.2677 - lr: 0.0030\n",
      "Epoch 388/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 205.5886 - reconstruction_loss: 151.4915 - KL_loss: 54.0971 - lr: 0.0030\n",
      "Epoch 389/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 204.1315 - reconstruction_loss: 150.3377 - KL_loss: 53.7938 - lr: 0.0030\n",
      "Epoch 390/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 206.8966 - reconstruction_loss: 153.1262 - KL_loss: 53.7705 - lr: 0.0030\n",
      "Epoch 391/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 206.6877 - reconstruction_loss: 152.9208 - KL_loss: 53.7669 - lr: 0.0030\n",
      "Epoch 392/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 205.6562 - reconstruction_loss: 152.0315 - KL_loss: 53.6247 - lr: 0.0030\n",
      "Epoch 393/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 206.2388 - reconstruction_loss: 152.4660 - KL_loss: 53.7728 - lr: 0.0030\n",
      "Epoch 394/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 204.9886 - reconstruction_loss: 150.8071 - KL_loss: 54.1815 - lr: 0.0030\n",
      "Epoch 395/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 205.4342 - reconstruction_loss: 150.9662 - KL_loss: 54.4681 - lr: 0.0030\n",
      "Epoch 396/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 205.8430 - reconstruction_loss: 151.4820 - KL_loss: 54.3610 - lr: 0.0030\n",
      "Epoch 397/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 203.9497 - reconstruction_loss: 149.8664 - KL_loss: 54.0833 - lr: 0.0030\n",
      "Epoch 398/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 207.0716 - reconstruction_loss: 152.9332 - KL_loss: 54.1384 - lr: 0.0030\n",
      "Epoch 399/1000\n",
      "2583/2583 [==============================] - 0s 17us/sample - loss: 206.9561 - reconstruction_loss: 152.8839 - KL_loss: 54.0722 - lr: 0.0030\n",
      "Epoch 400/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 205.7534 - reconstruction_loss: 152.0059 - KL_loss: 53.7475 - lr: 0.0030\n",
      "Epoch 401/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 205.3819 - reconstruction_loss: 151.4695 - KL_loss: 53.9124 - lr: 0.0030\n",
      "Epoch 402/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 206.4895 - reconstruction_loss: 152.2190 - KL_loss: 54.2705 - lr: 0.0030\n",
      "Epoch 403/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 204.7575 - reconstruction_loss: 150.3205 - KL_loss: 54.4370 - lr: 0.0030\n",
      "Epoch 404/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 206.2280 - reconstruction_loss: 151.9349 - KL_loss: 54.2931 - lr: 0.0030\n",
      "Epoch 405/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 205.9561 - reconstruction_loss: 151.6164 - KL_loss: 54.3397 - lr: 0.0030\n",
      "Epoch 406/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 205.0854 - reconstruction_loss: 150.8391 - KL_loss: 54.2463 - lr: 0.0030\n",
      "Epoch 407/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 204.7429 - reconstruction_loss: 150.7060 - KL_loss: 54.0368 - lr: 0.0030\n",
      "Epoch 408/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 205.7033 - reconstruction_loss: 151.9999 - KL_loss: 53.7033 - lr: 0.0030\n",
      "Epoch 409/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 204.7663 - reconstruction_loss: 150.9812 - KL_loss: 53.7850 - lr: 0.0030\n",
      "Epoch 410/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 204.6633 - reconstruction_loss: 150.6107 - KL_loss: 54.0526 - lr: 0.0030\n",
      "Epoch 411/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 204.8476 - reconstruction_loss: 150.6244 - KL_loss: 54.2232 - lr: 0.0030\n",
      "Epoch 412/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 203.9264 - reconstruction_loss: 149.8347 - KL_loss: 54.0917 - lr: 0.0030\n",
      "Epoch 413/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 204.2208 - reconstruction_loss: 150.0395 - KL_loss: 54.1814 - lr: 0.0030\n",
      "Epoch 414/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 203.4058 - reconstruction_loss: 149.1696 - KL_loss: 54.2362 - lr: 0.0030\n",
      "Epoch 415/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 203.8552 - reconstruction_loss: 149.8027 - KL_loss: 54.0524 - lr: 0.0030\n",
      "Epoch 416/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 203.2557 - reconstruction_loss: 149.2948 - KL_loss: 53.9609 - lr: 0.0030\n",
      "Epoch 417/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 203.1676 - reconstruction_loss: 149.0271 - KL_loss: 54.1405 - lr: 0.0030\n",
      "Epoch 418/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 203.0609 - reconstruction_loss: 148.8535 - KL_loss: 54.2074 - lr: 0.0030\n",
      "Epoch 419/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 202.6261 - reconstruction_loss: 148.3334 - KL_loss: 54.2926 - lr: 0.0030\n",
      "Epoch 420/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 202.7636 - reconstruction_loss: 148.3596 - KL_loss: 54.4040 - lr: 0.0030\n",
      "Epoch 421/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 202.7056 - reconstruction_loss: 148.3226 - KL_loss: 54.3830 - lr: 0.0030\n",
      "Epoch 422/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 203.4355 - reconstruction_loss: 149.1264 - KL_loss: 54.3091 - lr: 0.0030\n",
      "Epoch 423/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 202.5271 - reconstruction_loss: 148.2848 - KL_loss: 54.2423 - lr: 0.0030\n",
      "Epoch 424/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 204.2581 - reconstruction_loss: 149.8155 - KL_loss: 54.4426 - lr: 0.0030\n",
      "Epoch 425/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 203.2814 - reconstruction_loss: 148.8062 - KL_loss: 54.4753 - lr: 0.0030\n",
      "Epoch 426/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 204.3683 - reconstruction_loss: 150.0663 - KL_loss: 54.3019 - lr: 0.0030\n",
      "Epoch 427/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 204.0228 - reconstruction_loss: 149.8245 - KL_loss: 54.1983 - lr: 0.0030\n",
      "Epoch 428/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 203.6743 - reconstruction_loss: 149.3882 - KL_loss: 54.2860 - lr: 0.0030\n",
      "Epoch 429/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 203.2861 - reconstruction_loss: 148.9542 - KL_loss: 54.3319 - lr: 0.0030\n",
      "Epoch 430/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 203.3155 - reconstruction_loss: 148.9135 - KL_loss: 54.4020 - lr: 0.0030\n",
      "Epoch 431/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 203.2570 - reconstruction_loss: 148.9340 - KL_loss: 54.3230 - lr: 0.0030\n",
      "Epoch 432/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 202.9848 - reconstruction_loss: 148.7908 - KL_loss: 54.1940 - lr: 0.0030\n",
      "Epoch 433/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 202.7581 - reconstruction_loss: 148.5449 - KL_loss: 54.2132 - lr: 0.0030\n",
      "Epoch 434/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 202.2733 - reconstruction_loss: 148.0050 - KL_loss: 54.2683 - lr: 0.0030\n",
      "Epoch 435/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 202.8233 - reconstruction_loss: 148.3326 - KL_loss: 54.4908 - lr: 0.0030\n",
      "Epoch 436/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 201.4506 - reconstruction_loss: 146.8331 - KL_loss: 54.6175 - lr: 0.0030\n",
      "Epoch 437/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 203.0580 - reconstruction_loss: 148.4245 - KL_loss: 54.6335 - lr: 0.0030\n",
      "Epoch 438/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 202.0128 - reconstruction_loss: 147.5225 - KL_loss: 54.4903 - lr: 0.0030\n",
      "Epoch 439/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 203.9833 - reconstruction_loss: 149.4734 - KL_loss: 54.5099 - lr: 0.0030\n",
      "Epoch 440/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 203.4329 - reconstruction_loss: 148.9382 - KL_loss: 54.4947 - lr: 0.0030\n",
      "Epoch 441/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 203.0043 - reconstruction_loss: 148.6233 - KL_loss: 54.3810 - lr: 0.0030\n",
      "Epoch 442/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 202.4450 - reconstruction_loss: 148.0373 - KL_loss: 54.4077 - lr: 0.0030\n",
      "Epoch 443/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 202.4326 - reconstruction_loss: 148.0296 - KL_loss: 54.4030 - lr: 0.0030\n",
      "Epoch 444/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 201.5130 - reconstruction_loss: 147.0648 - KL_loss: 54.4482 - lr: 0.0030\n",
      "Epoch 445/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 202.1520 - reconstruction_loss: 147.6856 - KL_loss: 54.4664 - lr: 0.0030\n",
      "Epoch 446/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 201.8970 - reconstruction_loss: 147.6493 - KL_loss: 54.2477 - lr: 0.0030\n",
      "Epoch 447/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 202.2824 - reconstruction_loss: 148.1747 - KL_loss: 54.1077 - lr: 0.0030\n",
      "Epoch 448/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 202.1237 - reconstruction_loss: 147.9626 - KL_loss: 54.1610 - lr: 0.0030\n",
      "Epoch 449/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 201.7416 - reconstruction_loss: 147.3302 - KL_loss: 54.4114 - lr: 0.0030\n",
      "Epoch 450/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 201.9814 - reconstruction_loss: 147.1743 - KL_loss: 54.8071 - lr: 0.0030\n",
      "Epoch 451/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 201.3847 - reconstruction_loss: 146.4077 - KL_loss: 54.9771 - lr: 0.0030\n",
      "Epoch 452/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 201.7780 - reconstruction_loss: 146.8157 - KL_loss: 54.9623 - lr: 0.0030\n",
      "Epoch 453/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 201.1621 - reconstruction_loss: 146.3733 - KL_loss: 54.7889 - lr: 0.0030\n",
      "Epoch 454/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 201.1201 - reconstruction_loss: 146.4599 - KL_loss: 54.6603 - lr: 0.0030\n",
      "Epoch 455/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 200.9493 - reconstruction_loss: 146.5852 - KL_loss: 54.3641 - lr: 0.0030\n",
      "Epoch 456/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 200.8707 - reconstruction_loss: 146.4459 - KL_loss: 54.4249 - lr: 0.0030\n",
      "Epoch 457/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 200.7731 - reconstruction_loss: 146.3818 - KL_loss: 54.3913 - lr: 0.0030\n",
      "Epoch 458/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 200.8783 - reconstruction_loss: 146.2724 - KL_loss: 54.6059 - lr: 0.0030\n",
      "Epoch 459/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 200.4277 - reconstruction_loss: 145.6550 - KL_loss: 54.7727 - lr: 0.0030\n",
      "Epoch 460/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 200.5125 - reconstruction_loss: 145.4363 - KL_loss: 55.0763 - lr: 0.0030\n",
      "Epoch 461/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 200.2618 - reconstruction_loss: 145.3666 - KL_loss: 54.8953 - lr: 0.0030\n",
      "Epoch 462/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 200.6265 - reconstruction_loss: 146.0331 - KL_loss: 54.5934 - lr: 0.0030\n",
      "Epoch 463/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 200.5920 - reconstruction_loss: 146.2455 - KL_loss: 54.3465 - lr: 0.0030\n",
      "Epoch 464/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 200.5545 - reconstruction_loss: 146.1555 - KL_loss: 54.3991 - lr: 0.0030\n",
      "Epoch 465/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 199.9215 - reconstruction_loss: 145.3386 - KL_loss: 54.5830 - lr: 0.0030\n",
      "Epoch 466/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 200.0249 - reconstruction_loss: 145.3746 - KL_loss: 54.6504 - lr: 0.0030\n",
      "Epoch 467/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 199.7867 - reconstruction_loss: 145.0302 - KL_loss: 54.7566 - lr: 0.0030\n",
      "Epoch 468/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 200.4010 - reconstruction_loss: 145.4736 - KL_loss: 54.9275 - lr: 0.0030\n",
      "Epoch 469/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 200.2131 - reconstruction_loss: 145.5368 - KL_loss: 54.6763 - lr: 0.0030\n",
      "Epoch 470/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 199.8500 - reconstruction_loss: 145.1827 - KL_loss: 54.6673 - lr: 0.0030\n",
      "Epoch 471/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 199.9823 - reconstruction_loss: 145.3933 - KL_loss: 54.5889 - lr: 0.0030\n",
      "Epoch 472/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 200.0470 - reconstruction_loss: 145.4914 - KL_loss: 54.5555 - lr: 0.0030\n",
      "Epoch 473/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 199.8656 - reconstruction_loss: 145.1997 - KL_loss: 54.6659 - lr: 0.0030\n",
      "Epoch 474/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 199.8685 - reconstruction_loss: 145.0714 - KL_loss: 54.7971 - lr: 0.0030\n",
      "Epoch 475/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 199.6686 - reconstruction_loss: 144.5404 - KL_loss: 55.1282 - lr: 0.0030\n",
      "Epoch 476/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 199.3277 - reconstruction_loss: 144.1556 - KL_loss: 55.1721 - lr: 0.0030\n",
      "Epoch 477/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 199.6331 - reconstruction_loss: 144.3624 - KL_loss: 55.2707 - lr: 0.0030\n",
      "Epoch 478/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 199.3652 - reconstruction_loss: 144.5056 - KL_loss: 54.8595 - lr: 0.0030\n",
      "Epoch 479/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 198.9464 - reconstruction_loss: 144.3335 - KL_loss: 54.6129 - lr: 0.0030\n",
      "Epoch 480/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 199.9310 - reconstruction_loss: 145.6386 - KL_loss: 54.2925 - lr: 0.0030\n",
      "Epoch 481/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 198.7861 - reconstruction_loss: 144.4035 - KL_loss: 54.3826 - lr: 0.0030\n",
      "Epoch 482/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 199.7496 - reconstruction_loss: 145.2449 - KL_loss: 54.5048 - lr: 0.0030\n",
      "Epoch 483/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 198.8231 - reconstruction_loss: 144.0543 - KL_loss: 54.7689 - lr: 0.0030\n",
      "Epoch 484/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 199.7124 - reconstruction_loss: 144.9490 - KL_loss: 54.7634 - lr: 0.0030\n",
      "Epoch 485/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 199.4418 - reconstruction_loss: 144.6163 - KL_loss: 54.8255 - lr: 0.0030\n",
      "Epoch 486/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 199.9045 - reconstruction_loss: 144.7821 - KL_loss: 55.1224 - lr: 0.0030\n",
      "Epoch 487/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 199.6798 - reconstruction_loss: 144.6134 - KL_loss: 55.0664 - lr: 0.0030\n",
      "Epoch 488/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 199.1593 - reconstruction_loss: 144.2782 - KL_loss: 54.8811 - lr: 0.0030\n",
      "Epoch 489/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 199.8192 - reconstruction_loss: 145.1602 - KL_loss: 54.6590 - lr: 0.0030\n",
      "Epoch 490/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 198.8529 - reconstruction_loss: 144.2695 - KL_loss: 54.5834 - lr: 0.0030\n",
      "Epoch 491/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 200.2357 - reconstruction_loss: 145.3464 - KL_loss: 54.8892 - lr: 0.0030\n",
      "Epoch 492/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 199.1291 - reconstruction_loss: 144.1104 - KL_loss: 55.0187 - lr: 0.0030\n",
      "Epoch 493/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 199.9953 - reconstruction_loss: 144.8523 - KL_loss: 55.1430 - lr: 0.0030\n",
      "Epoch 494/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 200.4623 - reconstruction_loss: 145.4169 - KL_loss: 55.0454 - lr: 0.0030\n",
      "Epoch 495/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 199.4436 - reconstruction_loss: 144.4812 - KL_loss: 54.9624 - lr: 0.0030\n",
      "Epoch 496/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 199.5507 - reconstruction_loss: 144.6783 - KL_loss: 54.8724 - lr: 0.0030\n",
      "Epoch 497/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 199.4397 - reconstruction_loss: 144.5918 - KL_loss: 54.8480 - lr: 0.0030\n",
      "Epoch 498/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 198.9470 - reconstruction_loss: 144.1837 - KL_loss: 54.7633 - lr: 0.0030\n",
      "Epoch 499/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 198.8578 - reconstruction_loss: 144.0515 - KL_loss: 54.8063 - lr: 0.0030\n",
      "Epoch 500/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 198.5196 - reconstruction_loss: 143.8497 - KL_loss: 54.6699 - lr: 0.0030\n",
      "Epoch 501/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 199.0193 - reconstruction_loss: 144.2539 - KL_loss: 54.7653 - lr: 0.0030\n",
      "Epoch 502/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 198.5823 - reconstruction_loss: 143.5088 - KL_loss: 55.0735 - lr: 0.0030\n",
      "Epoch 503/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 198.7061 - reconstruction_loss: 143.5429 - KL_loss: 55.1632 - lr: 0.0030\n",
      "Epoch 504/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 198.4515 - reconstruction_loss: 143.3039 - KL_loss: 55.1476 - lr: 0.0030\n",
      "Epoch 505/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 198.3158 - reconstruction_loss: 143.3079 - KL_loss: 55.0080 - lr: 0.0030\n",
      "Epoch 506/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 198.4998 - reconstruction_loss: 143.5508 - KL_loss: 54.9491 - lr: 0.0030\n",
      "Epoch 507/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 198.2421 - reconstruction_loss: 143.4815 - KL_loss: 54.7606 - lr: 0.0030\n",
      "Epoch 508/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 198.2326 - reconstruction_loss: 143.3104 - KL_loss: 54.9221 - lr: 0.0030\n",
      "Epoch 509/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 198.1382 - reconstruction_loss: 143.1454 - KL_loss: 54.9928 - lr: 0.0030\n",
      "Epoch 510/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 197.6679 - reconstruction_loss: 142.7354 - KL_loss: 54.9325 - lr: 0.0030\n",
      "Epoch 511/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 198.4252 - reconstruction_loss: 143.5374 - KL_loss: 54.8878 - lr: 0.0030\n",
      "Epoch 512/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 197.7955 - reconstruction_loss: 142.4290 - KL_loss: 55.3665 - lr: 0.0030\n",
      "Epoch 513/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 197.9274 - reconstruction_loss: 142.4829 - KL_loss: 55.4445 - lr: 0.0030\n",
      "Epoch 514/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 197.8815 - reconstruction_loss: 142.5472 - KL_loss: 55.3343 - lr: 0.0030\n",
      "Epoch 515/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 198.3512 - reconstruction_loss: 143.2398 - KL_loss: 55.1115 - lr: 0.0030\n",
      "Epoch 516/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 197.4279 - reconstruction_loss: 142.1087 - KL_loss: 55.3192 - lr: 0.0030\n",
      "Epoch 517/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 197.8921 - reconstruction_loss: 142.7595 - KL_loss: 55.1326 - lr: 0.0030\n",
      "Epoch 518/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 197.6349 - reconstruction_loss: 142.6315 - KL_loss: 55.0034 - lr: 0.0030\n",
      "Epoch 519/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 197.5279 - reconstruction_loss: 142.7618 - KL_loss: 54.7661 - lr: 0.0030\n",
      "Epoch 520/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 197.5492 - reconstruction_loss: 142.6966 - KL_loss: 54.8526 - lr: 0.0030\n",
      "Epoch 521/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 197.3114 - reconstruction_loss: 142.1589 - KL_loss: 55.1525 - lr: 0.0030\n",
      "Epoch 522/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 197.2435 - reconstruction_loss: 141.9681 - KL_loss: 55.2754 - lr: 0.0030\n",
      "Epoch 523/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 197.3320 - reconstruction_loss: 142.0498 - KL_loss: 55.2822 - lr: 0.0030\n",
      "Epoch 524/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 196.8540 - reconstruction_loss: 141.4531 - KL_loss: 55.4009 - lr: 0.0030\n",
      "Epoch 525/1000\n",
      "2583/2583 [==============================] - 0s 15us/sample - loss: 197.3964 - reconstruction_loss: 141.9256 - KL_loss: 55.4708 - lr: 0.0030\n",
      "Epoch 526/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 196.6994 - reconstruction_loss: 141.7381 - KL_loss: 54.9612 - lr: 0.0030\n",
      "Epoch 527/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 198.4108 - reconstruction_loss: 143.6374 - KL_loss: 54.7734 - lr: 0.0030\n",
      "Epoch 528/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 197.6904 - reconstruction_loss: 142.9021 - KL_loss: 54.7883 - lr: 0.0030\n",
      "Epoch 529/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 197.6205 - reconstruction_loss: 142.4662 - KL_loss: 55.1542 - lr: 0.0030\n",
      "Epoch 530/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 197.8191 - reconstruction_loss: 142.3592 - KL_loss: 55.4599 - lr: 0.0030\n",
      "Epoch 531/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 197.3231 - reconstruction_loss: 142.0077 - KL_loss: 55.3154 - lr: 0.0030\n",
      "Epoch 532/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 197.5797 - reconstruction_loss: 142.1243 - KL_loss: 55.4554 - lr: 0.0030\n",
      "Epoch 533/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 197.1183 - reconstruction_loss: 141.6529 - KL_loss: 55.4654 - lr: 0.0030\n",
      "Epoch 534/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 196.8689 - reconstruction_loss: 141.4004 - KL_loss: 55.4685 - lr: 0.0030\n",
      "Epoch 535/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 196.5429 - reconstruction_loss: 141.1763 - KL_loss: 55.3666 - lr: 0.0030\n",
      "Epoch 536/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 196.9892 - reconstruction_loss: 141.9449 - KL_loss: 55.0443 - lr: 0.0030\n",
      "Epoch 537/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 196.4787 - reconstruction_loss: 141.4669 - KL_loss: 55.0118 - lr: 0.0030\n",
      "Epoch 538/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 196.8461 - reconstruction_loss: 141.7984 - KL_loss: 55.0478 - lr: 0.0030\n",
      "Epoch 539/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 196.4203 - reconstruction_loss: 141.3760 - KL_loss: 55.0443 - lr: 0.0030\n",
      "Epoch 540/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 196.7921 - reconstruction_loss: 141.5819 - KL_loss: 55.2102 - lr: 0.0030\n",
      "Epoch 541/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 196.5897 - reconstruction_loss: 141.0459 - KL_loss: 55.5438 - lr: 0.0030\n",
      "Epoch 542/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 196.1556 - reconstruction_loss: 140.7052 - KL_loss: 55.4504 - lr: 0.0030\n",
      "Epoch 543/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 197.0161 - reconstruction_loss: 141.7338 - KL_loss: 55.2822 - lr: 0.0030\n",
      "Epoch 544/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 196.2332 - reconstruction_loss: 141.1234 - KL_loss: 55.1098 - lr: 0.0030\n",
      "Epoch 545/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 196.8541 - reconstruction_loss: 141.7786 - KL_loss: 55.0754 - lr: 0.0030\n",
      "Epoch 546/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 196.0053 - reconstruction_loss: 140.9692 - KL_loss: 55.0361 - lr: 0.0030\n",
      "Epoch 547/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 196.8934 - reconstruction_loss: 141.9371 - KL_loss: 54.9562 - lr: 0.0030\n",
      "Epoch 548/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 196.2529 - reconstruction_loss: 141.0455 - KL_loss: 55.2075 - lr: 0.0030\n",
      "Epoch 549/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 196.4635 - reconstruction_loss: 140.9620 - KL_loss: 55.5015 - lr: 0.0030\n",
      "Epoch 550/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 195.9350 - reconstruction_loss: 140.3654 - KL_loss: 55.5696 - lr: 0.0030\n",
      "Epoch 551/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 196.4232 - reconstruction_loss: 140.9313 - KL_loss: 55.4919 - lr: 0.0030\n",
      "Epoch 552/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 196.3062 - reconstruction_loss: 140.9967 - KL_loss: 55.3095 - lr: 0.0030\n",
      "Epoch 553/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 196.2166 - reconstruction_loss: 140.8789 - KL_loss: 55.3377 - lr: 0.0030\n",
      "Epoch 554/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 195.9796 - reconstruction_loss: 140.6188 - KL_loss: 55.3608 - lr: 0.0030\n",
      "Epoch 555/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 196.0148 - reconstruction_loss: 140.8469 - KL_loss: 55.1680 - lr: 0.0030\n",
      "Epoch 556/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 195.7630 - reconstruction_loss: 140.4959 - KL_loss: 55.2670 - lr: 0.0030\n",
      "Epoch 557/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 195.8865 - reconstruction_loss: 140.6116 - KL_loss: 55.2749 - lr: 0.0030\n",
      "Epoch 558/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 195.4780 - reconstruction_loss: 140.1633 - KL_loss: 55.3147 - lr: 0.0030\n",
      "Epoch 559/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 195.9059 - reconstruction_loss: 140.5904 - KL_loss: 55.3155 - lr: 0.0030\n",
      "Epoch 560/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 195.2528 - reconstruction_loss: 139.8408 - KL_loss: 55.4120 - lr: 0.0030\n",
      "Epoch 561/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 195.6070 - reconstruction_loss: 140.1958 - KL_loss: 55.4113 - lr: 0.0030\n",
      "Epoch 562/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 195.3304 - reconstruction_loss: 140.0471 - KL_loss: 55.2833 - lr: 0.0030\n",
      "Epoch 563/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 196.1591 - reconstruction_loss: 140.7009 - KL_loss: 55.4581 - lr: 0.0030\n",
      "Epoch 564/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 195.4279 - reconstruction_loss: 139.9682 - KL_loss: 55.4597 - lr: 0.0030\n",
      "Epoch 565/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 196.1191 - reconstruction_loss: 140.6716 - KL_loss: 55.4475 - lr: 0.0030\n",
      "Epoch 566/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 195.9500 - reconstruction_loss: 140.5942 - KL_loss: 55.3558 - lr: 0.0030\n",
      "Epoch 567/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 195.5170 - reconstruction_loss: 139.9884 - KL_loss: 55.5286 - lr: 0.0030\n",
      "Epoch 568/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 195.8315 - reconstruction_loss: 140.2990 - KL_loss: 55.5325 - lr: 0.0030\n",
      "Epoch 569/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 195.2697 - reconstruction_loss: 139.7038 - KL_loss: 55.5659 - lr: 0.0030\n",
      "Epoch 570/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 195.1806 - reconstruction_loss: 139.7160 - KL_loss: 55.4645 - lr: 0.0030\n",
      "Epoch 571/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 195.2420 - reconstruction_loss: 139.8414 - KL_loss: 55.4005 - lr: 0.0030\n",
      "Epoch 572/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 195.3028 - reconstruction_loss: 139.6753 - KL_loss: 55.6275 - lr: 0.0030\n",
      "Epoch 573/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 195.2929 - reconstruction_loss: 139.7511 - KL_loss: 55.5418 - lr: 0.0030\n",
      "Epoch 574/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 195.1596 - reconstruction_loss: 139.7425 - KL_loss: 55.4171 - lr: 0.0030\n",
      "Epoch 575/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 195.3569 - reconstruction_loss: 140.1297 - KL_loss: 55.2272 - lr: 0.0030\n",
      "Epoch 576/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 195.2664 - reconstruction_loss: 139.6215 - KL_loss: 55.6449 - lr: 0.0030\n",
      "Epoch 577/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 194.6642 - reconstruction_loss: 138.8411 - KL_loss: 55.8230 - lr: 0.0030\n",
      "Epoch 578/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 194.8906 - reconstruction_loss: 139.0436 - KL_loss: 55.8470 - lr: 0.0030\n",
      "Epoch 579/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 195.2838 - reconstruction_loss: 139.9145 - KL_loss: 55.3693 - lr: 0.0030\n",
      "Epoch 580/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 194.9190 - reconstruction_loss: 139.5284 - KL_loss: 55.3905 - lr: 0.0030\n",
      "Epoch 581/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 194.6704 - reconstruction_loss: 139.0942 - KL_loss: 55.5762 - lr: 0.0030\n",
      "Epoch 582/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 194.7286 - reconstruction_loss: 139.0796 - KL_loss: 55.6490 - lr: 0.0030\n",
      "Epoch 583/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 194.4017 - reconstruction_loss: 138.8078 - KL_loss: 55.5938 - lr: 0.0030\n",
      "Epoch 584/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 194.3452 - reconstruction_loss: 138.7232 - KL_loss: 55.6220 - lr: 0.0030\n",
      "Epoch 585/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 194.5184 - reconstruction_loss: 139.0658 - KL_loss: 55.4526 - lr: 0.0030\n",
      "Epoch 586/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 194.3531 - reconstruction_loss: 138.6978 - KL_loss: 55.6554 - lr: 0.0030\n",
      "Epoch 587/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 194.3075 - reconstruction_loss: 138.7103 - KL_loss: 55.5972 - lr: 0.0030\n",
      "Epoch 588/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 194.2270 - reconstruction_loss: 138.6513 - KL_loss: 55.5757 - lr: 0.0030\n",
      "Epoch 589/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.9991 - reconstruction_loss: 138.3957 - KL_loss: 55.6033 - lr: 0.0030\n",
      "Epoch 590/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 194.6293 - reconstruction_loss: 139.0251 - KL_loss: 55.6041 - lr: 0.0030\n",
      "Epoch 591/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 194.1514 - reconstruction_loss: 138.7503 - KL_loss: 55.4011 - lr: 0.0030\n",
      "Epoch 592/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 194.2317 - reconstruction_loss: 138.7367 - KL_loss: 55.4950 - lr: 0.0030\n",
      "Epoch 593/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 193.7419 - reconstruction_loss: 138.0678 - KL_loss: 55.6741 - lr: 0.0030\n",
      "Epoch 594/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 194.3257 - reconstruction_loss: 138.5630 - KL_loss: 55.7627 - lr: 0.0030\n",
      "Epoch 595/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.9366 - reconstruction_loss: 138.2984 - KL_loss: 55.6382 - lr: 0.0030\n",
      "Epoch 596/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 194.1921 - reconstruction_loss: 138.5024 - KL_loss: 55.6898 - lr: 0.0030\n",
      "Epoch 597/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 194.3049 - reconstruction_loss: 138.7986 - KL_loss: 55.5063 - lr: 0.0030\n",
      "Epoch 598/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 193.5616 - reconstruction_loss: 137.9155 - KL_loss: 55.6462 - lr: 0.0030\n",
      "Epoch 599/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 194.5318 - reconstruction_loss: 138.5215 - KL_loss: 56.0103 - lr: 0.0030\n",
      "Epoch 600/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.6850 - reconstruction_loss: 137.8043 - KL_loss: 55.8808 - lr: 0.0030\n",
      "Epoch 601/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 194.8498 - reconstruction_loss: 139.0748 - KL_loss: 55.7749 - lr: 0.0030\n",
      "Epoch 602/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 194.4016 - reconstruction_loss: 138.7610 - KL_loss: 55.6406 - lr: 0.0030\n",
      "Epoch 603/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 193.9517 - reconstruction_loss: 138.1788 - KL_loss: 55.7729 - lr: 0.0030\n",
      "Epoch 604/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 194.2014 - reconstruction_loss: 138.4202 - KL_loss: 55.7812 - lr: 0.0030\n",
      "Epoch 605/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 193.8011 - reconstruction_loss: 138.2914 - KL_loss: 55.5097 - lr: 0.0030\n",
      "Epoch 606/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.7975 - reconstruction_loss: 138.3187 - KL_loss: 55.4788 - lr: 0.0030\n",
      "Epoch 607/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 194.2656 - reconstruction_loss: 138.7604 - KL_loss: 55.5053 - lr: 0.0030\n",
      "Epoch 608/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 193.6424 - reconstruction_loss: 137.7720 - KL_loss: 55.8704 - lr: 0.0030\n",
      "Epoch 609/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 193.3962 - reconstruction_loss: 137.3599 - KL_loss: 56.0362 - lr: 0.0030\n",
      "Epoch 610/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 193.5987 - reconstruction_loss: 137.7590 - KL_loss: 55.8397 - lr: 0.0030\n",
      "Epoch 611/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 193.5582 - reconstruction_loss: 137.7947 - KL_loss: 55.7635 - lr: 0.0030\n",
      "Epoch 612/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.2681 - reconstruction_loss: 137.6742 - KL_loss: 55.5939 - lr: 0.0030\n",
      "Epoch 613/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 194.1245 - reconstruction_loss: 138.8260 - KL_loss: 55.2985 - lr: 0.0030\n",
      "Epoch 614/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 193.6052 - reconstruction_loss: 138.2257 - KL_loss: 55.3795 - lr: 0.0030\n",
      "Epoch 615/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 194.1960 - reconstruction_loss: 138.1890 - KL_loss: 56.0070 - lr: 0.0030\n",
      "Epoch 616/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.8958 - reconstruction_loss: 137.8012 - KL_loss: 56.0946 - lr: 0.0030\n",
      "Epoch 617/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 193.5201 - reconstruction_loss: 137.4840 - KL_loss: 56.0362 - lr: 0.0030\n",
      "Epoch 618/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 194.0383 - reconstruction_loss: 138.0921 - KL_loss: 55.9463 - lr: 0.0030\n",
      "Epoch 619/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.0684 - reconstruction_loss: 137.2360 - KL_loss: 55.8323 - lr: 0.0030\n",
      "Epoch 620/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.5947 - reconstruction_loss: 138.0010 - KL_loss: 55.5937 - lr: 0.0030\n",
      "Epoch 621/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 193.4043 - reconstruction_loss: 137.9316 - KL_loss: 55.4727 - lr: 0.0030\n",
      "Epoch 622/1000\n",
      "2583/2583 [==============================] - 0s 15us/sample - loss: 193.6971 - reconstruction_loss: 138.0941 - KL_loss: 55.6030 - lr: 0.0030\n",
      "Epoch 623/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 193.5885 - reconstruction_loss: 137.9463 - KL_loss: 55.6423 - lr: 0.0030\n",
      "Epoch 624/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 193.3386 - reconstruction_loss: 137.4639 - KL_loss: 55.8747 - lr: 0.0030\n",
      "Epoch 625/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 193.3504 - reconstruction_loss: 137.3213 - KL_loss: 56.0291 - lr: 0.0030\n",
      "Epoch 626/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 193.4874 - reconstruction_loss: 137.2320 - KL_loss: 56.2554 - lr: 0.0030\n",
      "Epoch 627/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 192.6186 - reconstruction_loss: 136.5009 - KL_loss: 56.1178 - lr: 0.0030\n",
      "Epoch 628/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 194.2888 - reconstruction_loss: 138.5163 - KL_loss: 55.7725 - lr: 0.0030\n",
      "Epoch 629/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.3307 - reconstruction_loss: 137.6450 - KL_loss: 55.6858 - lr: 0.0030\n",
      "Epoch 630/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.9618 - reconstruction_loss: 138.1313 - KL_loss: 55.8305 - lr: 0.0030\n",
      "Epoch 631/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 193.7373 - reconstruction_loss: 137.8591 - KL_loss: 55.8781 - lr: 0.0030\n",
      "Epoch 632/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 193.3836 - reconstruction_loss: 137.4465 - KL_loss: 55.9371 - lr: 0.0030\n",
      "Epoch 633/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.0777 - reconstruction_loss: 137.4310 - KL_loss: 55.6467 - lr: 0.0030\n",
      "Epoch 634/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 193.9655 - reconstruction_loss: 138.3193 - KL_loss: 55.6463 - lr: 0.0030\n",
      "Epoch 635/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 192.9202 - reconstruction_loss: 137.0007 - KL_loss: 55.9196 - lr: 0.0030\n",
      "Epoch 636/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 193.8908 - reconstruction_loss: 138.0169 - KL_loss: 55.8739 - lr: 0.0030\n",
      "Epoch 637/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 193.3944 - reconstruction_loss: 137.6642 - KL_loss: 55.7302 - lr: 0.0030\n",
      "Epoch 638/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 193.1193 - reconstruction_loss: 137.3767 - KL_loss: 55.7426 - lr: 0.0030\n",
      "Epoch 639/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.1413 - reconstruction_loss: 137.2002 - KL_loss: 55.9411 - lr: 0.0030\n",
      "Epoch 640/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.0430 - reconstruction_loss: 137.0561 - KL_loss: 55.9868 - lr: 0.0030\n",
      "Epoch 641/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 192.7390 - reconstruction_loss: 136.7788 - KL_loss: 55.9602 - lr: 0.0030\n",
      "Epoch 642/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 192.1350 - reconstruction_loss: 136.1272 - KL_loss: 56.0078 - lr: 0.0030\n",
      "Epoch 643/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 192.3869 - reconstruction_loss: 136.5514 - KL_loss: 55.8355 - lr: 0.0030\n",
      "Epoch 644/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 192.5024 - reconstruction_loss: 136.8902 - KL_loss: 55.6122 - lr: 0.0030\n",
      "Epoch 645/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 192.8139 - reconstruction_loss: 137.2641 - KL_loss: 55.5498 - lr: 0.0030\n",
      "Epoch 646/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 192.3643 - reconstruction_loss: 136.5603 - KL_loss: 55.8040 - lr: 0.0030\n",
      "Epoch 647/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 192.4834 - reconstruction_loss: 136.4378 - KL_loss: 56.0456 - lr: 0.0030\n",
      "Epoch 648/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 192.3645 - reconstruction_loss: 136.1791 - KL_loss: 56.1854 - lr: 0.0030\n",
      "Epoch 649/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 192.2695 - reconstruction_loss: 136.3070 - KL_loss: 55.9625 - lr: 0.0030\n",
      "Epoch 650/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 192.0853 - reconstruction_loss: 135.9175 - KL_loss: 56.1678 - lr: 0.0030\n",
      "Epoch 651/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 192.0218 - reconstruction_loss: 135.8789 - KL_loss: 56.1428 - lr: 0.0030\n",
      "Epoch 652/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 192.3788 - reconstruction_loss: 136.3928 - KL_loss: 55.9860 - lr: 0.0030\n",
      "Epoch 653/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 191.9444 - reconstruction_loss: 136.1840 - KL_loss: 55.7604 - lr: 0.0030\n",
      "Epoch 654/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 192.3601 - reconstruction_loss: 136.2658 - KL_loss: 56.0943 - lr: 0.0030\n",
      "Epoch 655/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 191.8617 - reconstruction_loss: 135.8038 - KL_loss: 56.0579 - lr: 0.0030\n",
      "Epoch 656/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 192.2046 - reconstruction_loss: 136.1814 - KL_loss: 56.0232 - lr: 0.0030\n",
      "Epoch 657/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 191.8297 - reconstruction_loss: 135.9179 - KL_loss: 55.9119 - lr: 0.0030\n",
      "Epoch 658/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 191.9593 - reconstruction_loss: 135.9957 - KL_loss: 55.9636 - lr: 0.0030\n",
      "Epoch 659/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 192.1016 - reconstruction_loss: 136.0072 - KL_loss: 56.0944 - lr: 0.0030\n",
      "Epoch 660/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 191.6051 - reconstruction_loss: 135.4266 - KL_loss: 56.1785 - lr: 0.0030\n",
      "Epoch 661/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 192.3713 - reconstruction_loss: 136.2467 - KL_loss: 56.1246 - lr: 0.0030\n",
      "Epoch 662/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 191.4722 - reconstruction_loss: 135.1753 - KL_loss: 56.2969 - lr: 0.0030\n",
      "Epoch 663/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 191.8202 - reconstruction_loss: 135.7775 - KL_loss: 56.0428 - lr: 0.0030\n",
      "Epoch 664/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 191.5926 - reconstruction_loss: 135.5881 - KL_loss: 56.0044 - lr: 0.0030\n",
      "Epoch 665/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 191.9376 - reconstruction_loss: 136.1559 - KL_loss: 55.7816 - lr: 0.0030\n",
      "Epoch 666/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 191.8349 - reconstruction_loss: 136.0896 - KL_loss: 55.7454 - lr: 0.0030\n",
      "Epoch 667/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 192.2208 - reconstruction_loss: 136.2538 - KL_loss: 55.9670 - lr: 0.0030\n",
      "Epoch 668/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 191.5723 - reconstruction_loss: 135.4704 - KL_loss: 56.1019 - lr: 0.0030\n",
      "Epoch 669/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 191.8655 - reconstruction_loss: 135.7066 - KL_loss: 56.1590 - lr: 0.0030\n",
      "Epoch 670/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 191.7286 - reconstruction_loss: 135.6094 - KL_loss: 56.1192 - lr: 0.0030\n",
      "Epoch 671/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 191.9073 - reconstruction_loss: 135.4816 - KL_loss: 56.4257 - lr: 0.0030\n",
      "Epoch 672/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 192.1653 - reconstruction_loss: 135.8300 - KL_loss: 56.3353 - lr: 0.0030\n",
      "Epoch 673/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 191.5404 - reconstruction_loss: 135.4836 - KL_loss: 56.0567 - lr: 0.0030\n",
      "Epoch 674/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 191.4869 - reconstruction_loss: 135.5965 - KL_loss: 55.8904 - lr: 0.0030\n",
      "Epoch 675/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 191.3688 - reconstruction_loss: 135.4284 - KL_loss: 55.9404 - lr: 0.0030\n",
      "Epoch 676/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 191.0842 - reconstruction_loss: 135.0168 - KL_loss: 56.0674 - lr: 0.0030\n",
      "Epoch 677/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 191.9514 - reconstruction_loss: 136.1573 - KL_loss: 55.7941 - lr: 0.0030\n",
      "Epoch 678/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 191.0848 - reconstruction_loss: 135.0875 - KL_loss: 55.9974 - lr: 0.0030\n",
      "Epoch 679/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 192.6962 - reconstruction_loss: 136.2897 - KL_loss: 56.4065 - lr: 0.0030\n",
      "Epoch 680/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 191.6994 - reconstruction_loss: 135.0800 - KL_loss: 56.6194 - lr: 0.0030\n",
      "Epoch 681/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 193.0277 - reconstruction_loss: 136.3363 - KL_loss: 56.6913 - lr: 0.0030\n",
      "Epoch 682/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 192.2471 - reconstruction_loss: 135.8028 - KL_loss: 56.4442 - lr: 0.0030\n",
      "Epoch 683/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 191.6786 - reconstruction_loss: 135.6521 - KL_loss: 56.0265 - lr: 0.0030\n",
      "Epoch 684/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 191.8935 - reconstruction_loss: 136.0448 - KL_loss: 55.8487 - lr: 0.0030\n",
      "Epoch 685/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 191.5435 - reconstruction_loss: 135.8561 - KL_loss: 55.6874 - lr: 0.0030\n",
      "Epoch 686/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 192.0868 - reconstruction_loss: 136.3991 - KL_loss: 55.6877 - lr: 0.0030\n",
      "Epoch 687/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 191.4774 - reconstruction_loss: 135.5615 - KL_loss: 55.9160 - lr: 0.0030\n",
      "Epoch 688/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 191.8227 - reconstruction_loss: 135.4316 - KL_loss: 56.3911 - lr: 0.0030\n",
      "Epoch 689/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 191.0009 - reconstruction_loss: 134.4094 - KL_loss: 56.5915 - lr: 0.0030\n",
      "Epoch 690/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 191.7689 - reconstruction_loss: 135.2083 - KL_loss: 56.5606 - lr: 0.0030\n",
      "Epoch 691/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 191.3816 - reconstruction_loss: 135.0748 - KL_loss: 56.3068 - lr: 0.0030\n",
      "Epoch 692/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 191.7780 - reconstruction_loss: 135.6243 - KL_loss: 56.1537 - lr: 0.0030\n",
      "Epoch 693/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 191.3846 - reconstruction_loss: 135.4539 - KL_loss: 55.9307 - lr: 0.0030\n",
      "Epoch 694/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 191.6442 - reconstruction_loss: 135.7839 - KL_loss: 55.8602 - lr: 0.0030\n",
      "Epoch 695/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 190.9847 - reconstruction_loss: 134.7805 - KL_loss: 56.2042 - lr: 0.0030\n",
      "Epoch 696/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 190.8571 - reconstruction_loss: 134.3417 - KL_loss: 56.5154 - lr: 0.0030\n",
      "Epoch 697/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 191.2289 - reconstruction_loss: 134.6342 - KL_loss: 56.5946 - lr: 0.0030\n",
      "Epoch 698/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 190.9322 - reconstruction_loss: 134.7166 - KL_loss: 56.2156 - lr: 0.0030\n",
      "Epoch 699/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 190.9704 - reconstruction_loss: 134.6813 - KL_loss: 56.2892 - lr: 0.0030\n",
      "Epoch 700/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 190.4745 - reconstruction_loss: 134.2710 - KL_loss: 56.2035 - lr: 0.0030\n",
      "Epoch 701/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 191.2167 - reconstruction_loss: 135.1456 - KL_loss: 56.0711 - lr: 0.0030\n",
      "Epoch 702/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 190.7909 - reconstruction_loss: 134.5581 - KL_loss: 56.2328 - lr: 0.0030\n",
      "Epoch 703/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 190.5795 - reconstruction_loss: 134.3094 - KL_loss: 56.2701 - lr: 0.0030\n",
      "Epoch 704/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 190.5460 - reconstruction_loss: 134.2935 - KL_loss: 56.2526 - lr: 0.0030\n",
      "Epoch 705/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 190.6466 - reconstruction_loss: 134.4800 - KL_loss: 56.1667 - lr: 0.0030\n",
      "Epoch 706/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 190.6404 - reconstruction_loss: 134.4507 - KL_loss: 56.1898 - lr: 0.0030\n",
      "Epoch 707/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 190.3461 - reconstruction_loss: 134.4362 - KL_loss: 55.9098 - lr: 0.0030\n",
      "Epoch 708/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 190.7227 - reconstruction_loss: 134.6037 - KL_loss: 56.1190 - lr: 0.0030\n",
      "Epoch 709/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 190.2879 - reconstruction_loss: 133.9301 - KL_loss: 56.3578 - lr: 0.0030\n",
      "Epoch 710/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 190.6803 - reconstruction_loss: 134.0065 - KL_loss: 56.6737 - lr: 0.0030\n",
      "Epoch 711/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 190.5668 - reconstruction_loss: 134.0457 - KL_loss: 56.5211 - lr: 0.0030\n",
      "Epoch 712/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 190.5042 - reconstruction_loss: 133.9566 - KL_loss: 56.5476 - lr: 0.0030\n",
      "Epoch 713/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 190.2771 - reconstruction_loss: 133.7131 - KL_loss: 56.5640 - lr: 0.0030\n",
      "Epoch 714/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 190.1062 - reconstruction_loss: 133.3802 - KL_loss: 56.7260 - lr: 0.0030\n",
      "Epoch 715/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 190.4483 - reconstruction_loss: 134.0432 - KL_loss: 56.4051 - lr: 0.0030\n",
      "Epoch 716/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 190.0399 - reconstruction_loss: 133.6513 - KL_loss: 56.3887 - lr: 0.0030\n",
      "Epoch 717/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 189.9402 - reconstruction_loss: 133.3961 - KL_loss: 56.5441 - lr: 0.0030\n",
      "Epoch 718/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 190.0378 - reconstruction_loss: 133.6796 - KL_loss: 56.3582 - lr: 0.0030\n",
      "Epoch 719/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 189.7684 - reconstruction_loss: 133.4478 - KL_loss: 56.3206 - lr: 0.0030\n",
      "Epoch 720/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 189.9056 - reconstruction_loss: 133.7424 - KL_loss: 56.1632 - lr: 0.0030\n",
      "Epoch 721/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 189.6768 - reconstruction_loss: 133.3581 - KL_loss: 56.3187 - lr: 0.0030\n",
      "Epoch 722/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 189.8603 - reconstruction_loss: 133.4654 - KL_loss: 56.3949 - lr: 0.0030\n",
      "Epoch 723/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 189.6997 - reconstruction_loss: 133.2402 - KL_loss: 56.4595 - lr: 0.0030\n",
      "Epoch 724/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 189.6078 - reconstruction_loss: 133.2461 - KL_loss: 56.3618 - lr: 0.0030\n",
      "Epoch 725/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 189.6423 - reconstruction_loss: 133.3953 - KL_loss: 56.2470 - lr: 0.0030\n",
      "Epoch 726/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 189.5236 - reconstruction_loss: 133.3707 - KL_loss: 56.1529 - lr: 0.0030\n",
      "Epoch 727/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 189.5829 - reconstruction_loss: 133.2422 - KL_loss: 56.3407 - lr: 0.0030\n",
      "Epoch 728/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.5975 - reconstruction_loss: 133.2320 - KL_loss: 56.3654 - lr: 0.0030\n",
      "Epoch 729/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 189.5654 - reconstruction_loss: 133.1096 - KL_loss: 56.4558 - lr: 0.0030\n",
      "Epoch 730/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 190.1015 - reconstruction_loss: 133.6615 - KL_loss: 56.4400 - lr: 0.0030\n",
      "Epoch 731/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 189.6136 - reconstruction_loss: 133.1266 - KL_loss: 56.4870 - lr: 0.0030\n",
      "Epoch 732/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 190.1842 - reconstruction_loss: 133.5674 - KL_loss: 56.6168 - lr: 0.0030\n",
      "Epoch 733/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 189.6939 - reconstruction_loss: 133.2521 - KL_loss: 56.4418 - lr: 0.0030\n",
      "Epoch 734/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.8722 - reconstruction_loss: 133.2971 - KL_loss: 56.5751 - lr: 0.0030\n",
      "Epoch 735/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 189.5196 - reconstruction_loss: 133.0946 - KL_loss: 56.4250 - lr: 0.0030\n",
      "Epoch 736/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.9232 - reconstruction_loss: 133.6860 - KL_loss: 56.2372 - lr: 0.0030\n",
      "Epoch 737/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 189.7622 - reconstruction_loss: 133.3435 - KL_loss: 56.4187 - lr: 0.0030\n",
      "Epoch 738/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.6029 - reconstruction_loss: 132.9652 - KL_loss: 56.6377 - lr: 0.0030\n",
      "Epoch 739/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.4299 - reconstruction_loss: 132.6602 - KL_loss: 56.7697 - lr: 0.0030\n",
      "Epoch 740/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.2116 - reconstruction_loss: 132.7914 - KL_loss: 56.4202 - lr: 0.0030\n",
      "Epoch 741/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 189.8246 - reconstruction_loss: 133.4892 - KL_loss: 56.3353 - lr: 0.0030\n",
      "Epoch 742/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 188.9643 - reconstruction_loss: 132.7251 - KL_loss: 56.2392 - lr: 0.0030\n",
      "Epoch 743/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 190.5375 - reconstruction_loss: 134.2840 - KL_loss: 56.2534 - lr: 0.0030\n",
      "Epoch 744/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 189.3276 - reconstruction_loss: 132.8786 - KL_loss: 56.4490 - lr: 0.0030\n",
      "Epoch 745/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 190.2916 - reconstruction_loss: 133.7862 - KL_loss: 56.5054 - lr: 0.0030\n",
      "Epoch 746/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 189.8540 - reconstruction_loss: 133.1991 - KL_loss: 56.6549 - lr: 0.0030\n",
      "Epoch 747/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 190.2339 - reconstruction_loss: 133.4616 - KL_loss: 56.7723 - lr: 0.0030\n",
      "Epoch 748/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.6714 - reconstruction_loss: 132.9995 - KL_loss: 56.6719 - lr: 0.0030\n",
      "Epoch 749/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 190.0135 - reconstruction_loss: 133.7016 - KL_loss: 56.3119 - lr: 0.0030\n",
      "Epoch 750/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 189.8169 - reconstruction_loss: 133.4637 - KL_loss: 56.3533 - lr: 0.0030\n",
      "Epoch 751/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 189.8841 - reconstruction_loss: 133.1151 - KL_loss: 56.7690 - lr: 0.0030\n",
      "Epoch 752/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.6046 - reconstruction_loss: 132.8458 - KL_loss: 56.7587 - lr: 0.0030\n",
      "Epoch 753/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 189.5211 - reconstruction_loss: 133.0998 - KL_loss: 56.4212 - lr: 0.0030\n",
      "Epoch 754/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.6996 - reconstruction_loss: 133.3508 - KL_loss: 56.3488 - lr: 0.0030\n",
      "Epoch 755/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 189.1082 - reconstruction_loss: 132.6939 - KL_loss: 56.4144 - lr: 0.0030\n",
      "Epoch 756/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 189.9952 - reconstruction_loss: 133.3169 - KL_loss: 56.6783 - lr: 0.0030\n",
      "Epoch 757/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 188.7784 - reconstruction_loss: 132.0827 - KL_loss: 56.6957 - lr: 0.0030\n",
      "Epoch 758/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 189.6062 - reconstruction_loss: 133.2484 - KL_loss: 56.3578 - lr: 0.0030\n",
      "Epoch 759/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 190.0504 - reconstruction_loss: 133.7225 - KL_loss: 56.3279 - lr: 0.0030\n",
      "Epoch 760/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 188.6897 - reconstruction_loss: 132.1136 - KL_loss: 56.5761 - lr: 0.0030\n",
      "Epoch 761/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.3777 - reconstruction_loss: 132.6071 - KL_loss: 56.7706 - lr: 0.0030\n",
      "Epoch 762/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 188.8263 - reconstruction_loss: 132.2785 - KL_loss: 56.5478 - lr: 0.0030\n",
      "Epoch 763/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.0256 - reconstruction_loss: 132.6180 - KL_loss: 56.4076 - lr: 0.0030\n",
      "Epoch 764/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 189.0061 - reconstruction_loss: 132.5424 - KL_loss: 56.4637 - lr: 0.0030\n",
      "Epoch 765/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 188.4980 - reconstruction_loss: 131.7246 - KL_loss: 56.7734 - lr: 0.0030\n",
      "Epoch 766/1000\n",
      "2583/2583 [==============================] - 0s 15us/sample - loss: 188.8417 - reconstruction_loss: 132.1652 - KL_loss: 56.6765 - lr: 0.0030\n",
      "Epoch 767/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 188.5333 - reconstruction_loss: 132.0074 - KL_loss: 56.5260 - lr: 0.0030\n",
      "Epoch 768/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 188.9127 - reconstruction_loss: 132.6967 - KL_loss: 56.2160 - lr: 0.0030\n",
      "Epoch 769/1000\n",
      "2583/2583 [==============================] - 0s 14us/sample - loss: 188.4474 - reconstruction_loss: 132.0977 - KL_loss: 56.3497 - lr: 0.0030\n",
      "Epoch 770/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.1135 - reconstruction_loss: 132.5097 - KL_loss: 56.6037 - lr: 0.0030\n",
      "Epoch 771/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 188.8279 - reconstruction_loss: 132.2022 - KL_loss: 56.6257 - lr: 0.0030\n",
      "Epoch 772/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 188.2636 - reconstruction_loss: 131.7964 - KL_loss: 56.4672 - lr: 0.0030\n",
      "Epoch 773/1000\n",
      "2583/2583 [==============================] - 0s 14us/sample - loss: 189.0032 - reconstruction_loss: 132.5622 - KL_loss: 56.4410 - lr: 0.0030\n",
      "Epoch 774/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 188.4466 - reconstruction_loss: 131.6984 - KL_loss: 56.7482 - lr: 0.0030\n",
      "Epoch 775/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 189.0445 - reconstruction_loss: 132.3525 - KL_loss: 56.6920 - lr: 0.0030\n",
      "Epoch 776/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 188.4037 - reconstruction_loss: 131.7087 - KL_loss: 56.6949 - lr: 0.0030\n",
      "Epoch 777/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 188.9814 - reconstruction_loss: 132.4363 - KL_loss: 56.5451 - lr: 0.0030\n",
      "Epoch 778/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 188.9662 - reconstruction_loss: 132.4886 - KL_loss: 56.4776 - lr: 0.0030\n",
      "Epoch 779/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 188.6115 - reconstruction_loss: 131.9288 - KL_loss: 56.6826 - lr: 0.0030\n",
      "Epoch 780/1000\n",
      "2583/2583 [==============================] - 0s 15us/sample - loss: 189.0691 - reconstruction_loss: 132.4727 - KL_loss: 56.5964 - lr: 0.0030\n",
      "Epoch 781/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 188.3466 - reconstruction_loss: 131.7452 - KL_loss: 56.6014 - lr: 0.0030\n",
      "Epoch 782/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.6104 - reconstruction_loss: 132.8000 - KL_loss: 56.8104 - lr: 0.0030\n",
      "Epoch 783/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 188.6273 - reconstruction_loss: 131.8286 - KL_loss: 56.7987 - lr: 0.0030\n",
      "Epoch 784/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 190.2085 - reconstruction_loss: 133.3844 - KL_loss: 56.8241 - lr: 0.0030\n",
      "Epoch 785/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.6903 - reconstruction_loss: 132.8503 - KL_loss: 56.8401 - lr: 0.0030\n",
      "Epoch 786/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.8066 - reconstruction_loss: 133.0480 - KL_loss: 56.7587 - lr: 0.0030\n",
      "Epoch 787/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 189.0435 - reconstruction_loss: 132.5502 - KL_loss: 56.4934 - lr: 0.0030\n",
      "Epoch 788/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 189.7772 - reconstruction_loss: 133.5457 - KL_loss: 56.2315 - lr: 0.0030\n",
      "Epoch 789/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 188.9462 - reconstruction_loss: 132.5833 - KL_loss: 56.3630 - lr: 0.0030\n",
      "Epoch 790/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 189.4932 - reconstruction_loss: 132.6729 - KL_loss: 56.8203 - lr: 0.0030\n",
      "Epoch 791/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 188.9937 - reconstruction_loss: 132.1475 - KL_loss: 56.8462 - lr: 0.0030\n",
      "Epoch 792/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 189.1981 - reconstruction_loss: 132.5080 - KL_loss: 56.6901 - lr: 0.0030\n",
      "Epoch 793/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 188.9203 - reconstruction_loss: 132.2681 - KL_loss: 56.6521 - lr: 0.0030\n",
      "Epoch 794/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 189.2569 - reconstruction_loss: 132.4267 - KL_loss: 56.8302 - lr: 0.0030\n",
      "Epoch 795/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 189.1363 - reconstruction_loss: 132.2438 - KL_loss: 56.8925 - lr: 0.0030\n",
      "Epoch 796/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 188.6044 - reconstruction_loss: 131.9030 - KL_loss: 56.7014 - lr: 0.0030\n",
      "Epoch 797/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 188.5507 - reconstruction_loss: 131.8873 - KL_loss: 56.6634 - lr: 0.0030\n",
      "Epoch 798/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 188.5347 - reconstruction_loss: 131.8748 - KL_loss: 56.6599 - lr: 0.0030\n",
      "Epoch 799/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 187.9374 - reconstruction_loss: 131.2648 - KL_loss: 56.6726 - lr: 0.0030\n",
      "Epoch 800/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 188.5785 - reconstruction_loss: 131.9893 - KL_loss: 56.5892 - lr: 0.0030\n",
      "Epoch 801/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 188.3935 - reconstruction_loss: 131.8801 - KL_loss: 56.5135 - lr: 0.0030\n",
      "Epoch 802/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 188.8201 - reconstruction_loss: 132.0363 - KL_loss: 56.7838 - lr: 0.0030\n",
      "Epoch 803/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 188.7489 - reconstruction_loss: 131.7586 - KL_loss: 56.9903 - lr: 0.0030\n",
      "Epoch 804/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 187.7633 - reconstruction_loss: 130.8565 - KL_loss: 56.9069 - lr: 0.0030\n",
      "Epoch 805/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 188.2833 - reconstruction_loss: 131.4546 - KL_loss: 56.8288 - lr: 0.0030\n",
      "Epoch 806/1000\n",
      "2583/2583 [==============================] - 0s 19us/sample - loss: 187.4995 - reconstruction_loss: 130.9091 - KL_loss: 56.5904 - lr: 0.0030\n",
      "Epoch 807/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 187.5963 - reconstruction_loss: 131.1116 - KL_loss: 56.4847 - lr: 0.0030\n",
      "Epoch 808/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 188.1015 - reconstruction_loss: 131.8194 - KL_loss: 56.2821 - lr: 0.0030\n",
      "Epoch 809/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 187.6591 - reconstruction_loss: 131.1287 - KL_loss: 56.5304 - lr: 0.0030\n",
      "Epoch 810/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 188.4993 - reconstruction_loss: 131.5665 - KL_loss: 56.9328 - lr: 0.0030\n",
      "Epoch 811/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 187.6479 - reconstruction_loss: 130.4920 - KL_loss: 57.1559 - lr: 0.0030\n",
      "Epoch 812/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 188.7787 - reconstruction_loss: 132.0131 - KL_loss: 56.7656 - lr: 0.0030\n",
      "Epoch 813/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 188.0170 - reconstruction_loss: 131.0571 - KL_loss: 56.9599 - lr: 0.0030\n",
      "Epoch 814/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 188.0891 - reconstruction_loss: 130.8228 - KL_loss: 57.2663 - lr: 0.0030\n",
      "Epoch 815/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 188.5134 - reconstruction_loss: 131.5406 - KL_loss: 56.9728 - lr: 0.0030\n",
      "Epoch 816/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 187.9015 - reconstruction_loss: 131.4212 - KL_loss: 56.4803 - lr: 0.0030\n",
      "Epoch 817/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 188.1906 - reconstruction_loss: 132.1531 - KL_loss: 56.0375 - lr: 0.0030\n",
      "Epoch 818/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 187.7636 - reconstruction_loss: 131.5725 - KL_loss: 56.1911 - lr: 0.0030\n",
      "Epoch 819/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 187.8630 - reconstruction_loss: 131.0714 - KL_loss: 56.7915 - lr: 0.0030\n",
      "Epoch 820/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 187.8903 - reconstruction_loss: 130.7309 - KL_loss: 57.1594 - lr: 0.0030\n",
      "Epoch 821/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 187.3770 - reconstruction_loss: 130.4090 - KL_loss: 56.9680 - lr: 0.0030\n",
      "Epoch 822/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 188.0694 - reconstruction_loss: 131.2862 - KL_loss: 56.7832 - lr: 0.0030\n",
      "Epoch 823/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 187.5246 - reconstruction_loss: 130.8385 - KL_loss: 56.6861 - lr: 0.0030\n",
      "Epoch 824/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 187.7191 - reconstruction_loss: 130.6792 - KL_loss: 57.0398 - lr: 0.0030\n",
      "Epoch 825/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 187.9785 - reconstruction_loss: 131.0246 - KL_loss: 56.9539 - lr: 0.0030\n",
      "Epoch 826/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 187.0734 - reconstruction_loss: 130.2487 - KL_loss: 56.8247 - lr: 0.0030\n",
      "Epoch 827/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 187.6801 - reconstruction_loss: 130.8104 - KL_loss: 56.8697 - lr: 0.0030\n",
      "Epoch 828/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 187.1684 - reconstruction_loss: 130.3854 - KL_loss: 56.7830 - lr: 0.0030\n",
      "Epoch 829/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 187.8261 - reconstruction_loss: 130.8905 - KL_loss: 56.9356 - lr: 0.0030\n",
      "Epoch 830/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 188.3583 - reconstruction_loss: 131.5206 - KL_loss: 56.8376 - lr: 0.0030\n",
      "Epoch 831/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 187.1552 - reconstruction_loss: 130.4089 - KL_loss: 56.7463 - lr: 0.0030\n",
      "Epoch 832/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 187.7727 - reconstruction_loss: 131.2903 - KL_loss: 56.4824 - lr: 0.0030\n",
      "Epoch 833/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 186.9805 - reconstruction_loss: 130.3051 - KL_loss: 56.6754 - lr: 0.0030\n",
      "Epoch 834/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 188.2581 - reconstruction_loss: 131.3161 - KL_loss: 56.9419 - lr: 0.0030\n",
      "Epoch 835/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 187.8808 - reconstruction_loss: 130.8240 - KL_loss: 57.0568 - lr: 0.0030\n",
      "Epoch 836/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 187.2944 - reconstruction_loss: 130.5112 - KL_loss: 56.7832 - lr: 0.0030\n",
      "Epoch 837/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 187.7582 - reconstruction_loss: 130.6413 - KL_loss: 57.1169 - lr: 0.0030\n",
      "Epoch 838/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 187.0614 - reconstruction_loss: 129.8307 - KL_loss: 57.2307 - lr: 0.0030\n",
      "Epoch 839/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 187.0017 - reconstruction_loss: 129.7374 - KL_loss: 57.2642 - lr: 0.0030\n",
      "Epoch 840/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 187.8093 - reconstruction_loss: 130.8083 - KL_loss: 57.0009 - lr: 0.0030\n",
      "Epoch 841/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 187.1694 - reconstruction_loss: 130.3271 - KL_loss: 56.8423 - lr: 0.0030\n",
      "Epoch 842/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.9591 - reconstruction_loss: 130.3716 - KL_loss: 56.5875 - lr: 0.0030\n",
      "Epoch 843/1000\n",
      "2583/2583 [==============================] - 0s 14us/sample - loss: 187.1398 - reconstruction_loss: 130.4015 - KL_loss: 56.7383 - lr: 0.0030\n",
      "Epoch 844/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.6441 - reconstruction_loss: 129.8525 - KL_loss: 56.7916 - lr: 0.0030\n",
      "Epoch 845/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 187.2778 - reconstruction_loss: 130.1424 - KL_loss: 57.1353 - lr: 0.0030\n",
      "Epoch 846/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 186.6913 - reconstruction_loss: 129.7094 - KL_loss: 56.9819 - lr: 0.0030\n",
      "Epoch 847/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 186.7839 - reconstruction_loss: 129.8406 - KL_loss: 56.9434 - lr: 0.0030\n",
      "Epoch 848/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.4163 - reconstruction_loss: 129.3322 - KL_loss: 57.0841 - lr: 0.0030\n",
      "Epoch 849/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 187.1736 - reconstruction_loss: 130.1913 - KL_loss: 56.9823 - lr: 0.0030\n",
      "Epoch 850/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 187.1366 - reconstruction_loss: 130.4843 - KL_loss: 56.6523 - lr: 0.0030\n",
      "Epoch 851/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.7114 - reconstruction_loss: 129.6854 - KL_loss: 57.0260 - lr: 0.0030\n",
      "Epoch 852/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.6542 - reconstruction_loss: 129.6078 - KL_loss: 57.0464 - lr: 0.0030\n",
      "Epoch 853/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.3568 - reconstruction_loss: 129.3259 - KL_loss: 57.0309 - lr: 0.0030\n",
      "Epoch 854/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 186.5735 - reconstruction_loss: 129.8286 - KL_loss: 56.7449 - lr: 0.0030\n",
      "Epoch 855/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 186.7334 - reconstruction_loss: 130.1864 - KL_loss: 56.5470 - lr: 0.0030\n",
      "Epoch 856/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.7327 - reconstruction_loss: 130.2376 - KL_loss: 56.4951 - lr: 0.0030\n",
      "Epoch 857/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 186.5300 - reconstruction_loss: 129.7190 - KL_loss: 56.8110 - lr: 0.0030\n",
      "Epoch 858/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.1153 - reconstruction_loss: 129.1115 - KL_loss: 57.0039 - lr: 0.0030\n",
      "Epoch 859/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.7905 - reconstruction_loss: 129.6919 - KL_loss: 57.0986 - lr: 0.0030\n",
      "Epoch 860/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.6328 - reconstruction_loss: 129.6580 - KL_loss: 56.9748 - lr: 0.0030\n",
      "Epoch 861/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 186.7338 - reconstruction_loss: 129.8267 - KL_loss: 56.9071 - lr: 0.0030\n",
      "Epoch 862/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 186.6277 - reconstruction_loss: 129.4219 - KL_loss: 57.2058 - lr: 0.0030\n",
      "Epoch 863/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 186.4617 - reconstruction_loss: 129.2112 - KL_loss: 57.2505 - lr: 0.0030\n",
      "Epoch 864/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.1027 - reconstruction_loss: 128.8645 - KL_loss: 57.2382 - lr: 0.0030\n",
      "Epoch 865/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.7726 - reconstruction_loss: 129.8422 - KL_loss: 56.9304 - lr: 0.0030\n",
      "Epoch 866/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 186.1145 - reconstruction_loss: 129.2456 - KL_loss: 56.8689 - lr: 0.0030\n",
      "Epoch 867/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 187.5568 - reconstruction_loss: 130.4633 - KL_loss: 57.0935 - lr: 0.0030\n",
      "Epoch 868/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.4053 - reconstruction_loss: 129.1086 - KL_loss: 57.2967 - lr: 0.0030\n",
      "Epoch 869/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 188.0296 - reconstruction_loss: 131.0780 - KL_loss: 56.9516 - lr: 0.0030\n",
      "Epoch 870/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 187.4962 - reconstruction_loss: 130.7982 - KL_loss: 56.6981 - lr: 0.0030\n",
      "Epoch 871/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 187.1295 - reconstruction_loss: 130.1889 - KL_loss: 56.9407 - lr: 0.0030\n",
      "Epoch 872/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 187.2851 - reconstruction_loss: 130.0343 - KL_loss: 57.2508 - lr: 0.0030\n",
      "Epoch 873/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 186.4826 - reconstruction_loss: 129.5963 - KL_loss: 56.8864 - lr: 0.0030\n",
      "Epoch 874/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 187.1765 - reconstruction_loss: 130.5827 - KL_loss: 56.5937 - lr: 0.0030\n",
      "Epoch 875/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 186.4090 - reconstruction_loss: 129.5576 - KL_loss: 56.8515 - lr: 0.0030\n",
      "Epoch 876/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 187.3843 - reconstruction_loss: 129.8698 - KL_loss: 57.5146 - lr: 0.0030\n",
      "Epoch 877/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 186.5453 - reconstruction_loss: 129.0323 - KL_loss: 57.5129 - lr: 0.0030\n",
      "Epoch 878/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 186.4407 - reconstruction_loss: 129.2341 - KL_loss: 57.2065 - lr: 0.0030\n",
      "Epoch 879/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 186.1116 - reconstruction_loss: 129.1712 - KL_loss: 56.9405 - lr: 0.0030\n",
      "Epoch 880/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 186.4565 - reconstruction_loss: 129.8136 - KL_loss: 56.6429 - lr: 0.0030\n",
      "Epoch 881/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 186.4221 - reconstruction_loss: 129.7976 - KL_loss: 56.6245 - lr: 0.0030\n",
      "Epoch 882/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 186.5837 - reconstruction_loss: 130.0100 - KL_loss: 56.5737 - lr: 0.0030\n",
      "Epoch 883/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.4174 - reconstruction_loss: 129.6091 - KL_loss: 56.8083 - lr: 0.0030\n",
      "Epoch 884/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.5089 - reconstruction_loss: 129.1788 - KL_loss: 57.3300 - lr: 0.0030\n",
      "Epoch 885/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.2754 - reconstruction_loss: 128.8144 - KL_loss: 57.4611 - lr: 0.0030\n",
      "Epoch 886/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 186.0722 - reconstruction_loss: 128.7771 - KL_loss: 57.2951 - lr: 0.0030\n",
      "Epoch 887/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.0810 - reconstruction_loss: 129.2651 - KL_loss: 56.8159 - lr: 0.0030\n",
      "Epoch 888/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.8128 - reconstruction_loss: 128.9941 - KL_loss: 56.8187 - lr: 0.0030\n",
      "Epoch 889/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 186.5441 - reconstruction_loss: 129.6012 - KL_loss: 56.9430 - lr: 0.0030\n",
      "Epoch 890/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 186.1575 - reconstruction_loss: 129.1825 - KL_loss: 56.9750 - lr: 0.0030\n",
      "Epoch 891/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 185.7612 - reconstruction_loss: 128.4265 - KL_loss: 57.3347 - lr: 0.0030\n",
      "Epoch 892/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 186.0695 - reconstruction_loss: 128.6796 - KL_loss: 57.3899 - lr: 0.0030\n",
      "Epoch 893/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.4610 - reconstruction_loss: 128.0254 - KL_loss: 57.4357 - lr: 0.0030\n",
      "Epoch 894/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.7591 - reconstruction_loss: 128.6584 - KL_loss: 57.1006 - lr: 0.0030\n",
      "Epoch 895/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 185.4853 - reconstruction_loss: 128.5352 - KL_loss: 56.9501 - lr: 0.0030\n",
      "Epoch 896/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 186.2759 - reconstruction_loss: 129.5517 - KL_loss: 56.7242 - lr: 0.0030\n",
      "Epoch 897/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.1778 - reconstruction_loss: 128.2593 - KL_loss: 56.9185 - lr: 0.0030\n",
      "Epoch 898/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 186.2223 - reconstruction_loss: 129.0645 - KL_loss: 57.1578 - lr: 0.0030\n",
      "Epoch 899/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.5901 - reconstruction_loss: 128.2332 - KL_loss: 57.3570 - lr: 0.0030\n",
      "Epoch 900/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 186.4267 - reconstruction_loss: 129.2930 - KL_loss: 57.1336 - lr: 0.0030\n",
      "Epoch 901/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.8479 - reconstruction_loss: 128.4966 - KL_loss: 57.3513 - lr: 0.0030\n",
      "Epoch 902/1000\n",
      "2583/2583 [==============================] - 0s 14us/sample - loss: 186.1444 - reconstruction_loss: 128.7326 - KL_loss: 57.4118 - lr: 0.0030\n",
      "Epoch 903/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.0136 - reconstruction_loss: 128.9800 - KL_loss: 57.0335 - lr: 0.0030\n",
      "Epoch 904/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 185.9132 - reconstruction_loss: 129.0006 - KL_loss: 56.9126 - lr: 0.0030\n",
      "Epoch 905/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.1123 - reconstruction_loss: 129.3532 - KL_loss: 56.7591 - lr: 0.0030\n",
      "Epoch 906/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 185.9061 - reconstruction_loss: 128.8388 - KL_loss: 57.0674 - lr: 0.0030\n",
      "Epoch 907/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 185.5274 - reconstruction_loss: 128.1462 - KL_loss: 57.3812 - lr: 0.0030\n",
      "Epoch 908/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 185.5400 - reconstruction_loss: 128.1607 - KL_loss: 57.3794 - lr: 0.0030\n",
      "Epoch 909/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 186.3346 - reconstruction_loss: 129.1036 - KL_loss: 57.2310 - lr: 0.0030\n",
      "Epoch 910/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 185.4786 - reconstruction_loss: 128.2658 - KL_loss: 57.2128 - lr: 0.0030\n",
      "Epoch 911/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.5796 - reconstruction_loss: 129.1672 - KL_loss: 57.4124 - lr: 0.0030\n",
      "Epoch 912/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 185.8848 - reconstruction_loss: 128.6942 - KL_loss: 57.1907 - lr: 0.0030\n",
      "Epoch 913/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 185.8762 - reconstruction_loss: 128.9374 - KL_loss: 56.9388 - lr: 0.0030\n",
      "Epoch 914/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 185.7724 - reconstruction_loss: 128.6402 - KL_loss: 57.1321 - lr: 0.0030\n",
      "Epoch 915/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.0180 - reconstruction_loss: 128.6483 - KL_loss: 57.3697 - lr: 0.0030\n",
      "Epoch 916/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.2839 - reconstruction_loss: 127.6901 - KL_loss: 57.5938 - lr: 0.0030\n",
      "Epoch 917/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.9692 - reconstruction_loss: 128.6275 - KL_loss: 57.3417 - lr: 0.0030\n",
      "Epoch 918/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 185.6854 - reconstruction_loss: 128.5805 - KL_loss: 57.1049 - lr: 0.0030\n",
      "Epoch 919/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.9027 - reconstruction_loss: 128.9811 - KL_loss: 56.9217 - lr: 0.0030\n",
      "Epoch 920/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.0124 - reconstruction_loss: 128.0679 - KL_loss: 56.9445 - lr: 0.0030\n",
      "Epoch 921/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 186.0369 - reconstruction_loss: 129.1442 - KL_loss: 56.8926 - lr: 0.0030\n",
      "Epoch 922/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 185.3375 - reconstruction_loss: 128.4432 - KL_loss: 56.8944 - lr: 0.0030\n",
      "Epoch 923/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.1921 - reconstruction_loss: 128.7269 - KL_loss: 57.4652 - lr: 0.0030\n",
      "Epoch 924/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.4928 - reconstruction_loss: 127.8661 - KL_loss: 57.6267 - lr: 0.0030\n",
      "Epoch 925/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.9852 - reconstruction_loss: 128.4164 - KL_loss: 57.5688 - lr: 0.0030\n",
      "Epoch 926/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 186.0704 - reconstruction_loss: 128.7066 - KL_loss: 57.3638 - lr: 0.0030\n",
      "Epoch 927/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 185.6967 - reconstruction_loss: 128.5016 - KL_loss: 57.1951 - lr: 0.0030\n",
      "Epoch 928/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 186.1334 - reconstruction_loss: 129.1262 - KL_loss: 57.0072 - lr: 0.0030\n",
      "Epoch 929/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.0458 - reconstruction_loss: 128.1460 - KL_loss: 56.8997 - lr: 0.0030\n",
      "Epoch 930/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 185.9091 - reconstruction_loss: 128.7755 - KL_loss: 57.1336 - lr: 0.0030\n",
      "Epoch 931/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.0106 - reconstruction_loss: 127.6553 - KL_loss: 57.3553 - lr: 0.0030\n",
      "Epoch 932/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 185.5652 - reconstruction_loss: 128.1495 - KL_loss: 57.4157 - lr: 0.0030\n",
      "Epoch 933/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.2277 - reconstruction_loss: 127.8671 - KL_loss: 57.3606 - lr: 0.0030\n",
      "Epoch 934/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.1795 - reconstruction_loss: 127.7737 - KL_loss: 57.4058 - lr: 0.0030\n",
      "Epoch 935/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.3182 - reconstruction_loss: 128.0947 - KL_loss: 57.2235 - lr: 0.0030\n",
      "Epoch 936/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.0997 - reconstruction_loss: 127.9077 - KL_loss: 57.1920 - lr: 0.0030\n",
      "Epoch 937/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 185.8127 - reconstruction_loss: 128.8321 - KL_loss: 56.9805 - lr: 0.0030\n",
      "Epoch 938/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 184.9501 - reconstruction_loss: 127.7745 - KL_loss: 57.1755 - lr: 0.0030\n",
      "Epoch 939/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.8640 - reconstruction_loss: 128.3677 - KL_loss: 57.4963 - lr: 0.0030\n",
      "Epoch 940/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 185.0164 - reconstruction_loss: 127.4010 - KL_loss: 57.6154 - lr: 0.0030\n",
      "Epoch 941/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 185.9400 - reconstruction_loss: 128.8259 - KL_loss: 57.1141 - lr: 0.0030\n",
      "Epoch 942/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.6639 - reconstruction_loss: 128.6180 - KL_loss: 57.0460 - lr: 0.0030\n",
      "Epoch 943/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 185.5307 - reconstruction_loss: 128.2678 - KL_loss: 57.2629 - lr: 0.0030\n",
      "Epoch 944/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.5730 - reconstruction_loss: 128.3753 - KL_loss: 57.1977 - lr: 0.0030\n",
      "Epoch 945/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.6000 - reconstruction_loss: 128.5262 - KL_loss: 57.0737 - lr: 0.0030\n",
      "Epoch 946/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 184.8851 - reconstruction_loss: 127.6083 - KL_loss: 57.2768 - lr: 0.0030\n",
      "Epoch 947/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 185.5824 - reconstruction_loss: 128.1152 - KL_loss: 57.4672 - lr: 0.0030\n",
      "Epoch 948/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.0221 - reconstruction_loss: 127.4890 - KL_loss: 57.5331 - lr: 0.0030\n",
      "Epoch 949/1000\n",
      "2583/2583 [==============================] - 0s 8us/sample - loss: 185.7697 - reconstruction_loss: 128.4692 - KL_loss: 57.3006 - lr: 0.0030\n",
      "Epoch 950/1000\n",
      "2583/2583 [==============================] - 0s 7us/sample - loss: 184.8949 - reconstruction_loss: 127.7318 - KL_loss: 57.1631 - lr: 0.0030\n",
      "Epoch 951/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.7273 - reconstruction_loss: 128.4881 - KL_loss: 57.2393 - lr: 0.0030\n",
      "Epoch 952/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.6766 - reconstruction_loss: 128.4055 - KL_loss: 57.2711 - lr: 0.0030\n",
      "Epoch 953/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 184.9134 - reconstruction_loss: 127.7829 - KL_loss: 57.1305 - lr: 0.0030\n",
      "Epoch 954/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.6759 - reconstruction_loss: 128.5449 - KL_loss: 57.1310 - lr: 0.0030\n",
      "Epoch 955/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 184.8113 - reconstruction_loss: 127.4551 - KL_loss: 57.3563 - lr: 0.0030\n",
      "Epoch 956/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 185.7036 - reconstruction_loss: 128.1992 - KL_loss: 57.5045 - lr: 0.0030\n",
      "Epoch 957/1000\n",
      "2583/2583 [==============================] - 0s 14us/sample - loss: 185.1534 - reconstruction_loss: 127.9258 - KL_loss: 57.2275 - lr: 0.0030\n",
      "Epoch 958/1000\n",
      "2583/2583 [==============================] - 0s 15us/sample - loss: 185.1326 - reconstruction_loss: 127.9125 - KL_loss: 57.2201 - lr: 0.0030\n",
      "Epoch 959/1000\n",
      "2583/2583 [==============================] - 0s 20us/sample - loss: 184.9863 - reconstruction_loss: 127.6422 - KL_loss: 57.3441 - lr: 0.0030\n",
      "Epoch 960/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 184.5497 - reconstruction_loss: 127.1883 - KL_loss: 57.3614 - lr: 0.0030\n",
      "Epoch 961/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 184.9052 - reconstruction_loss: 127.4918 - KL_loss: 57.4133 - lr: 0.0030\n",
      "Epoch 962/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 185.0109 - reconstruction_loss: 127.8635 - KL_loss: 57.1474 - lr: 0.0030\n",
      "Epoch 963/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 184.7366 - reconstruction_loss: 127.6538 - KL_loss: 57.0828 - lr: 0.0030\n",
      "Epoch 964/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 185.2926 - reconstruction_loss: 128.0758 - KL_loss: 57.2168 - lr: 0.0030\n",
      "Epoch 965/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 184.1981 - reconstruction_loss: 126.6016 - KL_loss: 57.5964 - lr: 0.0030\n",
      "Epoch 966/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 185.1853 - reconstruction_loss: 127.6186 - KL_loss: 57.5667 - lr: 0.0030\n",
      "Epoch 967/1000\n",
      "2583/2583 [==============================] - 0s 15us/sample - loss: 184.4220 - reconstruction_loss: 126.9979 - KL_loss: 57.4242 - lr: 0.0030\n",
      "Epoch 968/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 185.1044 - reconstruction_loss: 127.7311 - KL_loss: 57.3732 - lr: 0.0030\n",
      "Epoch 969/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 185.0114 - reconstruction_loss: 127.7076 - KL_loss: 57.3037 - lr: 0.0030\n",
      "Epoch 970/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 184.9063 - reconstruction_loss: 127.4762 - KL_loss: 57.4301 - lr: 0.0030\n",
      "Epoch 971/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 184.7774 - reconstruction_loss: 127.5668 - KL_loss: 57.2106 - lr: 0.0030\n",
      "Epoch 972/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 184.7532 - reconstruction_loss: 127.2708 - KL_loss: 57.4824 - lr: 0.0030\n",
      "Epoch 973/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 184.1198 - reconstruction_loss: 126.5698 - KL_loss: 57.5501 - lr: 0.0030\n",
      "Epoch 974/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 185.3738 - reconstruction_loss: 127.8727 - KL_loss: 57.5011 - lr: 0.0030\n",
      "Epoch 975/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 184.7724 - reconstruction_loss: 127.4412 - KL_loss: 57.3312 - lr: 0.0030\n",
      "Epoch 976/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 184.7087 - reconstruction_loss: 127.3948 - KL_loss: 57.3139 - lr: 0.0030\n",
      "Epoch 977/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 185.1213 - reconstruction_loss: 127.7337 - KL_loss: 57.3876 - lr: 0.0030\n",
      "Epoch 978/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 184.6833 - reconstruction_loss: 127.3469 - KL_loss: 57.3363 - lr: 0.0030\n",
      "Epoch 979/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 184.5298 - reconstruction_loss: 127.2475 - KL_loss: 57.2823 - lr: 0.0030\n",
      "Epoch 980/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 184.6887 - reconstruction_loss: 127.4675 - KL_loss: 57.2212 - lr: 0.0030\n",
      "Epoch 981/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 184.7954 - reconstruction_loss: 127.3252 - KL_loss: 57.4703 - lr: 0.0030\n",
      "Epoch 982/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 184.2677 - reconstruction_loss: 126.7170 - KL_loss: 57.5507 - lr: 0.0030\n",
      "Epoch 983/1000\n",
      "2583/2583 [==============================] - 0s 9us/sample - loss: 184.4120 - reconstruction_loss: 126.8233 - KL_loss: 57.5887 - lr: 0.0030\n",
      "Epoch 984/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 184.1851 - reconstruction_loss: 126.8841 - KL_loss: 57.3010 - lr: 0.0030\n",
      "Epoch 985/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 184.6154 - reconstruction_loss: 127.3483 - KL_loss: 57.2671 - lr: 0.0030\n",
      "Epoch 986/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 184.0855 - reconstruction_loss: 126.7203 - KL_loss: 57.3652 - lr: 0.0030\n",
      "Epoch 987/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 184.4628 - reconstruction_loss: 126.9565 - KL_loss: 57.5064 - lr: 0.0030\n",
      "Epoch 988/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 183.7253 - reconstruction_loss: 126.2239 - KL_loss: 57.5014 - lr: 0.0030\n",
      "Epoch 989/1000\n",
      "2583/2583 [==============================] - 0s 13us/sample - loss: 184.8327 - reconstruction_loss: 127.5798 - KL_loss: 57.2530 - lr: 0.0030\n",
      "Epoch 990/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 184.1759 - reconstruction_loss: 126.7487 - KL_loss: 57.4271 - lr: 0.0030\n",
      "Epoch 991/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 184.7063 - reconstruction_loss: 127.3693 - KL_loss: 57.3371 - lr: 0.0030\n",
      "Epoch 992/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 184.1857 - reconstruction_loss: 127.0019 - KL_loss: 57.1837 - lr: 0.0030\n",
      "Epoch 993/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 184.0467 - reconstruction_loss: 126.7671 - KL_loss: 57.2796 - lr: 0.0030\n",
      "Epoch 994/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 184.8728 - reconstruction_loss: 127.3660 - KL_loss: 57.5068 - lr: 0.0030\n",
      "Epoch 995/1000\n",
      "2583/2583 [==============================] - 0s 12us/sample - loss: 183.8372 - reconstruction_loss: 126.1007 - KL_loss: 57.7366 - lr: 0.0030\n",
      "Epoch 996/1000\n",
      "2583/2583 [==============================] - 0s 14us/sample - loss: 184.6198 - reconstruction_loss: 127.0173 - KL_loss: 57.6026 - lr: 0.0030\n",
      "Epoch 997/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 183.6947 - reconstruction_loss: 126.2398 - KL_loss: 57.4549 - lr: 0.0030\n",
      "Epoch 998/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 185.0956 - reconstruction_loss: 127.3931 - KL_loss: 57.7025 - lr: 0.0030\n",
      "Epoch 999/1000\n",
      "2583/2583 [==============================] - 0s 10us/sample - loss: 184.5218 - reconstruction_loss: 126.9703 - KL_loss: 57.5515 - lr: 0.0030\n",
      "Epoch 1000/1000\n",
      "2583/2583 [==============================] - 0s 11us/sample - loss: 184.5344 - reconstruction_loss: 127.5132 - KL_loss: 57.0213 - lr: 0.0030\n",
      "\n",
      "training finished in 1000 epochs (reach max pre-specified epoches), transform data to adjust the platform effect...\n",
      "\n",
      "WARNING: when transforming data, after reversed Min-Max Scaling, apply exp transformation then multiple the factor and round to integer\n",
      "\n",
      "re-run DE on CVAE transformed scRNA-seq data!\n",
      "filtering genes present in <10 cells: 6 genes removed\n",
      "\n",
      "Differential analysis across cell-types on scRNA-seq data...\n",
      "0%...8%...WARNING: only 1 genes passing filtering (<20) for Endo vs Smc\n",
      "17%...25%...33%...WARNING: only 14 genes passing filtering (<20) for PVALB vs SST\n",
      "42%...WARNING: only 18 genes passing filtering (<20) for SST vs VIP\n",
      "50%...WARNING: only 19 genes passing filtering (<20) for Smc vs Endo\n",
      "58%...WARNING: only 16 genes passing filtering (<20) for VIP vs SST\n",
      "67%...WARNING: only 16 genes passing filtering (<20) for eL2/3 vs eL4\n",
      "WARNING: only 16 genes passing filtering (<20) for eL2/3 vs eL5\n",
      "75%...WARNING: only 11 genes passing filtering (<20) for eL4 vs eL2/3\n",
      "WARNING: only 8 genes passing filtering (<20) for eL4 vs eL5\n",
      "83%...WARNING: only 14 genes passing filtering (<20) for eL5 vs eL2/3\n",
      "WARNING: only 15 genes passing filtering (<20) for eL5 vs eL4\n",
      "WARNING: only 14 genes passing filtering (<20) for eL5 vs eL6\n",
      "92%...WARNING: only 9 genes passing filtering (<20) for eL6 vs eL5\n",
      "finally selected 392 cell-type marker genes\n",
      "\n",
      "\n",
      "platform effect adjustment by CVAE finished. Elapsed time: 4.61 minutes.\n",
      "\n",
      "\n",
      "use the marker genes derived from CVAE transformed scRNA-seq for downstream regression!\n",
      "\n",
      "gene filtering before modeling...\n",
      "all genes passed filtering\n",
      "\n",
      "spot filtering before modeling...\n",
      "all spots passed filtering\n",
      "\n",
      "\n",
      "######### Start GLRM modeling... #########\n",
      "\n",
      "GLRM settings:\n",
      "use SciPy minimize method:  L-BFGS-B\n",
      "global optimization turned off, local minimum will be used in GLRM\n",
      "use hybrid version of GLRM\n",
      "Numba detected total 64 available CPU cores. Use 64 CPU cores\n",
      "use 2001 points to calculate the heavy-tail density\n",
      "use weight threshold for Adaptive Lasso:  0.001\n",
      "total 308 unique nUMIs, min: 0.0, max: 1147.0\n",
      "\n",
      "Build graph: \n",
      " Graph with 581 nodes and 1029 edges\n",
      "\n",
      "estimation of gene-specific platform effect gamma_g is skipped as already using CVAE to adjust platform effect\n",
      "\n",
      "\n",
      "Start GLRM fitting...\n",
      "\n",
      "first estimate MLE theta and corresponding e^alpha and sigma^2...\n",
      "\n",
      "GLRM model initialization...\n",
      "calculate MLE theta and sigma^2 iteratively...\n",
      "  iter | time_opt | time_sig | sigma2\n",
      "     0 |   13.289 |    1.670 |  1.003\n",
      "     1 |    3.360 |    1.402 |  1.004\n",
      "MLE theta and sigma^2 calculation finished. Elapsed time: 0.33 minutes.\n",
      "MLE theta estimation finished. Elapsed time: 0.33 minutes.\n",
      "\n",
      "calculate weights of Adaptive Lasso...\n",
      "\n",
      "Stage 1: variable selection using Adaptive Lasso starts with the MLE theta and e^alpha, using already estimated sigma^2 and gamma_g...\n",
      "specified hyper-parameter for Adaptive Lasso is: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "hyper-parameter for Adaptive Lasso: use cross-validation to find the optimal value from 8 candidates...\n",
      "\n",
      "Start cross-validation for hyper-parameter lambda_r...\n",
      "directly estimate theta by Adaptive Lasso loss function as NO Graph Laplacian constrain!\n",
      "0%...11%...22%...33%...44%...56%...67%...78%...early stop\n",
      "find optimal lambda_r 1.931 with average negative log-likelihood 47739.3287 by 5 fold cross-validation. Elapsed time: 5.10 minutes.\n",
      "\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |     14.932 |     14.932 |      0.133 |      0.133 |       1.00 |       1.00 |    2.428 |    0.000 |    0.004 |   0.250102 |   0.125051\n",
      "     1 |     14.907 |      0.106 |      0.133 |      0.148 |       1.00 |       2.00 |    5.115 |    0.000 |    0.003 |   0.249640 |   0.124820\n",
      "     2 |     14.690 |      0.653 |      0.133 |      0.177 |       2.00 |       4.00 |    5.050 |    0.000 |    0.003 |   0.246299 |   0.123150\n",
      "     3 |      9.865 |     23.768 |      0.142 |      0.216 |       4.00 |       4.00 |    5.497 |    0.000 |    0.003 |   0.164808 |   0.082404\n",
      "     4 |      5.806 |     26.842 |      0.145 |      0.232 |       4.00 |       4.00 |    4.253 |    0.000 |    0.003 |   0.068295 |   0.034148\n",
      "     5 |      5.292 |     17.677 |      0.136 |      0.235 |       4.00 |       4.00 |    5.062 |    0.000 |    0.003 |   0.083079 |   0.041540\n",
      "     6 |      4.927 |      9.223 |      0.127 |      0.235 |       4.00 |       4.00 |    5.243 |    0.000 |    0.003 |   0.080895 |   0.040447\n",
      "     7 |      3.816 |      8.377 |      0.126 |      0.238 |       4.00 |       8.00 |    4.958 |    0.000 |    0.003 |   0.057735 |   0.028868\n",
      "     8 |      2.849 |     12.228 |      0.130 |      0.249 |       8.00 |       8.00 |    4.949 |    0.000 |    0.003 |   0.036757 |   0.018379\n",
      "     9 |      2.352 |     10.920 |      0.129 |      0.261 |       8.00 |       8.00 |    3.962 |    0.000 |    0.003 |   0.030571 |   0.015285\n",
      "    10 |      2.159 |      7.252 |      0.125 |      0.273 |       8.00 |      16.00 |    4.106 |    0.000 |    0.003 |   0.029721 |   0.014861\n",
      "    11 |      1.826 |      9.815 |      0.128 |      0.294 |      16.00 |      16.00 |    4.369 |    0.000 |    0.003 |   0.023781 |   0.011890\n",
      "    12 |      1.564 |     10.992 |      0.129 |      0.311 |      16.00 |      16.00 |    4.304 |    0.000 |    0.003 |   0.018131 |   0.009065\n",
      "    13 |      1.409 |      8.897 |      0.127 |      0.324 |      16.00 |      32.00 |    4.363 |    0.000 |    0.003 |   0.017027 |   0.008514\n",
      "    14 |      1.202 |     11.410 |      0.129 |      0.349 |      32.00 |      32.00 |    4.804 |    0.000 |    0.003 |   0.014433 |   0.007216\n",
      "    15 |      1.019 |     13.331 |      0.131 |      0.371 |      32.00 |      32.00 |    9.132 |    0.000 |    0.003 |   0.011671 |   0.005835\n",
      "    16 |      0.910 |     11.603 |      0.130 |      0.390 |      32.00 |      64.00 |    4.419 |    0.000 |    0.003 |   0.010312 |   0.005156\n",
      "    17 |      0.793 |     13.981 |      0.132 |      0.424 |      64.00 |      64.00 |    4.719 |    0.000 |    0.003 |   0.009048 |   0.004524\n",
      "    18 |      0.684 |     16.495 |      0.135 |      0.455 |      64.00 |      64.00 |    4.482 |    0.000 |    0.003 |   0.007677 |   0.003838\n",
      "    19 |      0.609 |     15.721 |      0.134 |      0.481 |      64.00 |     128.00 |    4.413 |    0.000 |    0.003 |   0.006777 |   0.003389\n",
      "    20 |      0.536 |     19.935 |      0.138 |      0.528 |     128.00 |     128.00 |    4.722 |    0.000 |    0.003 |   0.005912 |   0.002956\n",
      "    21 |      0.474 |     21.465 |      0.140 |      0.572 |     128.00 |     128.00 |    4.420 |    0.000 |    0.003 |   0.005188 |   0.002594\n",
      "    22 |      0.421 |     20.586 |      0.139 |      0.609 |     128.00 |     256.00 |    4.439 |    0.000 |    0.003 |   0.004656 |   0.002328\n",
      "    23 |      0.362 |     31.514 |      0.150 |      0.677 |     256.00 |     256.00 |    4.425 |    0.000 |    0.003 |   0.003988 |   0.001994\n",
      "    24 |      0.312 |     33.562 |      0.152 |      0.734 |     256.00 |     256.00 |    4.438 |    0.000 |    0.003 |   0.003396 |   0.001698\n",
      "    25 |      0.280 |     29.210 |      0.147 |      0.782 |     256.00 |     512.00 |    4.250 |    0.000 |    0.003 |   0.003043 |   0.001521\n",
      "    26 |      0.246 |     37.240 |      0.155 |      0.870 |     512.00 |     512.00 |    4.392 |    0.000 |    0.003 |   0.002666 |   0.001333\n",
      "    27 |      0.217 |     40.551 |      0.159 |      0.949 |     512.00 |     512.00 |    4.347 |    0.000 |    0.003 |   0.002329 |   0.001164\n",
      "    28 |      0.195 |     35.840 |      0.154 |      1.019 |     512.00 |    1024.00 |    4.351 |    0.000 |    0.003 |   0.002097 |   0.001049\n",
      "    29 |      0.170 |     54.252 |      0.172 |      1.146 |    1024.00 |    1024.00 |    4.205 |    0.000 |    0.003 |   0.001836 |   0.000918\n",
      "    30 |      0.151 |     65.318 |      0.183 |      1.257 |    1024.00 |    1024.00 |    4.265 |    0.000 |    0.003 |   0.001593 |   0.000797\n",
      "    31 |      0.139 |     54.745 |      0.173 |      1.356 |    1024.00 |    2048.00 |    4.092 |    0.000 |    0.003 |   0.001455 |   0.000727\n",
      "    32 |      0.123 |     63.075 |      0.181 |      1.544 |    2048.00 |    2048.00 |    4.272 |    0.000 |    0.003 |   0.001288 |   0.000644\n",
      "    33 |      0.109 |     87.505 |      0.206 |      1.714 |    2048.00 |    2048.00 |    4.120 |    0.000 |    0.003 |   0.001130 |   0.000565\n",
      "    34 |      0.096 |     71.650 |      0.190 |      1.863 |    2048.00 |    4096.00 |    4.244 |    0.000 |    0.003 |   0.001008 |   0.000504\n",
      "    35 |      0.080 |    134.808 |      0.253 |      2.109 |    4096.00 |    4096.00 |    4.121 |    0.000 |    0.003 |   0.000841 |   0.000420\n",
      "    36 |      0.068 |    132.500 |      0.251 |      2.307 |    4096.00 |    4096.00 |    4.062 |    0.000 |    0.003 |   0.000691 |   0.000345\n",
      "    37 |      0.061 |    119.276 |      0.237 |      2.466 |    4096.00 |    8192.00 |    3.996 |    0.000 |    0.003 |   0.000617 |   0.000308\n",
      "    38 |      0.053 |    151.531 |      0.270 |      2.764 |    8192.00 |    8192.00 |    3.930 |    0.000 |    0.003 |   0.000533 |   0.000266\n",
      "    39 |      0.046 |    151.036 |      0.269 |      3.032 |    8192.00 |    8192.00 |    4.059 |    0.000 |    0.003 |   0.000451 |   0.000225\n",
      "    40 |      0.041 |    138.151 |      0.256 |      3.262 |    8192.00 |   16384.00 |    3.809 |    0.000 |    0.003 |   0.000392 |   0.000196\n",
      "    41 |      0.034 |    182.135 |      0.300 |      3.664 |   16384.00 |   16384.00 |    3.884 |    0.000 |    0.003 |   0.000326 |   0.000163\n",
      "    42 |      0.030 |    203.849 |      0.322 |      4.011 |   16384.00 |   16384.00 |    3.670 |    0.000 |    0.003 |   0.000276 |   0.000138\n",
      "    43 |      0.027 |    204.863 |      0.323 |      4.310 |   16384.00 |   32768.00 |    3.819 |    0.000 |    0.003 |   0.000247 |   0.000123\n",
      "    44 |      0.023 |    240.216 |      0.358 |      4.838 |   32768.00 |   32768.00 |    3.563 |    0.000 |    0.003 |   0.000202 |   0.000101\n",
      "    45 |      0.019 |    288.183 |      0.406 |      5.279 |   32768.00 |   32768.00 |    3.337 |    0.000 |    0.003 |   0.000161 |   0.000081\n",
      "    46 |      0.017 |    240.370 |      0.358 |      5.648 |   32768.00 |   65536.00 |    3.496 |    0.000 |    0.003 |   0.000141 |   0.000071\n",
      "    47 |      0.014 |    341.164 |      0.459 |      6.300 |   65536.00 |   65536.00 |    3.360 |    0.000 |    0.003 |   0.000116 |   0.000058\n",
      "    48 |      0.012 |    377.321 |      0.495 |      6.844 |   65536.00 |   65536.00 |    3.161 |    0.000 |    0.003 |   0.000093 |   0.000047\n",
      "    49 |      0.010 |    365.637 |      0.484 |      7.274 |   65536.00 |  131072.00 |    3.035 |    0.000 |    0.003 |   0.000076 |   0.000038\n",
      "    50 |      0.008 |    438.095 |      0.556 |      8.001 |  131072.00 |          / |    3.144 |    0.000 |    0.003 |   0.000061 |   0.000030\n",
      "early stop!\n",
      "Terminated (optimal) in 51 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 3.67 minutes.\n",
      "\n",
      "Stage 1 variable selection finished. Elapsed time: 8.77 minutes.\n",
      "\n",
      "Stage 2: final theta estimation with Graph Laplacian Constrain using already estimated sigma^2 and gamma_g\n",
      "specified hyper-parameter for Graph Laplacian Constrain is: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "hyper-parameter for Graph Laplacian Constrain: use cross-validation to find the optimal value from 8 candidates...\n",
      "\n",
      "Start cross-validation for hyper-parameter lambda_g...\n",
      "still use ADMM even NO Graph Laplacian constrain (lambda_g=0)\n",
      "0%...11%...22%...33%...44%...56%...67%...78%...89%...early stop\n",
      "find optimal lambda_g 5.179 with average negative log-likelihood 50381.4299 by 5 fold cross-validation. Elapsed time: 38.31 minutes.\n",
      "\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |     15.921 |     16.537 |      0.135 |      0.134 |       1.00 |       1.00 |    5.568 |    0.000 |    0.007 |   0.109654 |   0.138881\n",
      "     1 |     12.775 |      9.467 |      0.131 |      0.142 |       1.00 |       1.00 |    2.453 |    0.000 |    0.007 |   0.179022 |   0.119665\n",
      "     2 |      9.787 |     11.161 |      0.129 |      0.150 |       1.00 |       1.00 |    2.610 |    0.000 |    0.007 |   0.119642 |   0.098314\n",
      "     3 |      8.345 |     12.428 |      0.131 |      0.158 |       1.00 |       2.00 |    2.449 |    0.000 |    0.007 |   0.087008 |   0.082530\n",
      "     4 |      7.692 |     19.086 |      0.137 |      0.173 |       2.00 |       2.00 |    2.691 |    0.000 |    0.007 |   0.076657 |   0.076697\n",
      "     5 |      6.596 |     20.602 |      0.139 |      0.185 |       2.00 |       2.00 |    2.757 |    0.000 |    0.007 |   0.072683 |   0.065394\n",
      "     6 |      5.546 |     22.240 |      0.140 |      0.196 |       2.00 |       4.00 |    2.607 |    0.000 |    0.007 |   0.055130 |   0.055105\n",
      "     7 |      5.035 |     32.490 |      0.151 |      0.215 |       4.00 |       4.00 |    2.805 |    0.000 |    0.007 |   0.049804 |   0.049712\n",
      "     8 |      4.083 |     34.353 |      0.152 |      0.231 |       4.00 |       4.00 |    2.956 |    0.000 |    0.007 |   0.046151 |   0.039775\n",
      "     9 |      3.114 |     36.407 |      0.154 |      0.243 |       4.00 |       8.00 |    2.634 |    0.000 |    0.007 |   0.030380 |   0.030693\n",
      "    10 |      2.684 |     49.655 |      0.168 |      0.263 |       8.00 |       8.00 |    2.828 |    0.000 |    0.007 |   0.025914 |   0.026056\n",
      "    11 |      1.964 |     51.651 |      0.170 |      0.278 |       8.00 |       8.00 |    2.759 |    0.000 |    0.007 |   0.022792 |   0.018650\n",
      "    12 |      1.264 |     53.550 |      0.172 |      0.287 |       8.00 |      16.00 |    2.630 |    0.000 |    0.007 |   0.011706 |   0.012302\n",
      "    13 |      1.054 |     67.347 |      0.185 |      0.302 |      16.00 |      16.00 |    2.663 |    0.000 |    0.006 |   0.009072 |   0.009976\n",
      "    14 |      0.686 |     68.714 |      0.187 |      0.311 |      16.00 |      16.00 |    2.595 |    0.000 |    0.006 |   0.008448 |   0.006230\n",
      "    15 |      0.330 |     69.907 |      0.188 |      0.315 |      16.00 |      32.00 |    2.041 |    0.000 |    0.006 |   0.002877 |   0.003131\n",
      "    16 |      0.312 |     81.435 |      0.200 |      0.322 |      32.00 |      32.00 |    2.375 |    0.000 |    0.005 |   0.001647 |   0.002710\n",
      "    17 |      0.192 |     81.994 |      0.200 |      0.326 |      32.00 |      32.00 |    2.314 |    0.000 |    0.005 |   0.002534 |   0.001559\n",
      "    18 |      0.067 |     82.532 |      0.201 |      0.327 |      32.00 |      64.00 |    1.910 |    0.000 |    0.005 |   0.000701 |   0.000561\n",
      "    19 |      0.093 |     90.685 |      0.209 |      0.330 |      64.00 |      64.00 |    1.640 |    0.000 |    0.005 |   0.000248 |   0.000655\n",
      "    20 |      0.055 |     90.823 |      0.209 |      0.331 |      64.00 |      64.00 |    2.071 |    0.000 |    0.005 |   0.000747 |   0.000401\n",
      "    21 |      0.016 |     91.001 |      0.209 |      0.332 |      64.00 |     128.00 |    1.625 |    0.000 |    0.004 |   0.000210 |   0.000133\n",
      "    22 |      0.027 |     96.059 |      0.214 |      0.333 |     128.00 |     128.00 |    1.481 |    0.000 |    0.004 |   0.000052 |   0.000180\n",
      "    23 |      0.016 |     96.083 |      0.214 |      0.334 |     128.00 |     128.00 |    1.833 |    0.000 |    0.004 |   0.000218 |   0.000112\n",
      "    24 |      0.005 |     96.131 |      0.214 |      0.335 |     128.00 |     256.00 |    1.578 |    0.000 |    0.004 |   0.000066 |   0.000036\n",
      "    25 |      0.008 |     98.985 |      0.217 |      0.336 |     256.00 |          / |    1.461 |    0.000 |    0.004 |   0.000017 |   0.000049\n",
      "early stop!\n",
      "Terminated (optimal) in 26 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 1.07 minutes.\n",
      "\n",
      "\n",
      "stage 2 finished. Elapsed time: 39.38 minutes.\n",
      "\n",
      "GLRM fitting finished. Elapsed time: 48.47 minutes.\n",
      "\n",
      "\n",
      "Post-processing estimated cell-type proportion theta...\n",
      "hard thresholding small theta values with threshold 0\n",
      "\n",
      "\n",
      "cell type deconvolution finished. Estimate results saved in /home/exouser/Spatial/celltype_proportions.csv. Elapsed time: 0.88 hours.\n",
      "\n",
      "\n",
      "######### No imputation #########\n",
      "\n",
      "\n",
      "whole pipeline finished. Total elapsed time: 0.88 hours.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='runDeconvolution -q sim_seq_based_spatial_spot_nUMI.csv                           -r GSE102827_scRNA_cell_nUMI.csv                           -c GSE102827_scRNA_cell_celltype.csv                           -a sim_spatial_spot_adjacency_matrix.csv                           --n_marker_per_cmp 20                           -n 64                           --cvae_init_lr 0.003                           --num_hidden_layer 1                           --use_batch_norm false                           --cvae_train_epoch 1000                           --n_pseudo_spot 0\\n', returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = '''runDeconvolution -q sim_seq_based_spatial_spot_nUMI.csv \\\n",
    "                          -r GSE102827_scRNA_cell_nUMI.csv \\\n",
    "                          -c GSE102827_scRNA_cell_celltype.csv \\\n",
    "                          -a sim_spatial_spot_adjacency_matrix.csv \\\n",
    "                          --n_marker_per_cmp 20 \\\n",
    "                          -n 64 \\\n",
    "                          --cvae_init_lr 0.003 \\\n",
    "                          --num_hidden_layer 1 \\\n",
    "                          --use_batch_norm false \\\n",
    "                          --cvae_train_epoch 1000 \\\n",
    "                          --n_pseudo_spot 0\n",
    "'''\n",
    "\n",
    "subprocess.run(cmd, check=True, text=True, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
