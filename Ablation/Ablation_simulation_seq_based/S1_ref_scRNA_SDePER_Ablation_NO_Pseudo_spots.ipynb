{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db496b92",
   "metadata": {},
   "source": [
    "# Run *SDePER* on sequencing-based simulated data: Scenario 1 + scRNA-seq data as reference + WITHOUT pseudo-spots\n",
    "\n",
    "In this Notebook we run ablation test on SDePER on simulated data. For generating **sequencing-based** simulated data via coarse-graining procedure please refer [generate_simulated_spatial_data.nb.html](https://rawcdn.githack.com/az7jh2/SDePER_Analysis/c963d08f74f4591c2ef6f132177795297793d878/Simulation_seq_based/Generate_simulation_data/generate_simulated_spatial_data.nb.html) in [Generate_simulation_data](https://github.com/az7jh2/SDePER_Analysis/tree/main/Simulation_seq_based/Generate_simulation_data) folder.\n",
    "\n",
    "**Scenario 1** means the reference data for deconvolution includes all single cells with the **matched 12 cell types**.\n",
    "\n",
    "**scRNA-seq data as reference** means the reference data is another scRNA-seq data ([GSE115746](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115746)) from the same tissue with simulated spatial data, therefore **platform effect exists**.\n",
    "\n",
    "**WITHOUT pseudo-spots** means we train the Conditional Variational Autoencoder (CVAE) without incorporating pseudo-spots in the training data.\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "So here we use the **4 input files** as shown below:\n",
    "\n",
    "1. raw nUMI counts of simulated spatial transcriptomic data (spots × genes): [sim_seq_based_spatial_spot_nUMI.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation_seq_based/Generate_simulation_data/sim_seq_based_spatial_spot_nUMI.csv)\n",
    "2. raw nUMI counts of reference scRNA-seq data (cells × genes): `scRNA_data_full.csv`. Since the file size of csv file of raw nUMI matrix of all 23,178 cells and 45,768 genes is up to 2.3 GB, we do not provide this file in our repository. It's just a **matrix transpose** of [GSE115746_cells_exon_counts.csv.gz](https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE115746&format=file&file=GSE115746%5Fcells%5Fexon%5Fcounts%2Ecsv%2Egz) in [GSE115746](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115746) to satisty the file format requirement that rows as cells and columns as genes.\n",
    "3. cell type annotations for cells of **the matched 12 cell types** in reference scRNA-seq data (cells × 1): [ref_scRNA_cell_celltype.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Run_SDePER_on_simulation_data/Scenario_1/ref_scRNA_seq/ref_scRNA_cell_celltype.csv)\n",
    "4. adjacency matrix of spots in simulated spatial transcriptomic data (spots × spots): [sim_spatial_spot_adjacency_matrix.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Generate_simulation_data/sim_spatial_spot_adjacency_matrix.csv)\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "SDePER settings are the same as baseline run [S1_ref_scRNA_SDePER_WITH_CVAE.ipynb](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation_seq_based/Run_SDePER_on_simulation_data/Scenario_1/ref_scRNA_seq/S1_ref_scRNA_SDePER_WITH_CVAE.ipynb), and we discarded unneeded command-line options:\n",
    "\n",
    "* number of selected TOP marker genes for each comparison in Differential `n_marker_per_cmp`: 20\n",
    "* number of used CPU cores `n_core`: 64\n",
    "* initial learning rate for training CVAE `cvae_init_lr`: 0.003\n",
    "* number of hidden layers in encoder and decoder of CVAE `num_hidden_layer`: 1\n",
    "* whether to use Batch Normalization `use_batch_norm`: false\n",
    "* CVAE training epochs `cvae_train_epoch`: 1000\n",
    "\n",
    "ALL other options are left as default.\n",
    "\n",
    "**For ablation test, set the number of pseudo-spots during CVAE training `n_pseudo_spot` as 0**.\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "the `bash` command to start cell type deconvolution is\n",
    "\n",
    "`runDeconvolution -q sim_seq_based_spatial_spot_nUMI.csv -r scRNA_data_full.csv -c ref_scRNA_cell_celltype.csv -a sim_spatial_spot_adjacency_matrix.csv --n_marker_per_cmp 20 -n 64 --cvae_init_lr 0.003 --num_hidden_layer 1 --use_batch_norm false --cvae_train_epoch 1000 --n_pseudo_spot 0`\n",
    "\n",
    "Note this Notebook uses **SDePER v1.2.1**. Cell type deconvolution result is renamed as [S1_ref_scRNA_SDePER_Ablation_NO_Pseudo_spots_celltype_proportions.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Ablation/Ablation_simulation_seq_based/S1_ref_scRNA_SDePER_Ablation_NO_Pseudo_spots_celltype_proportions.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9e3939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SDePER (Spatial Deconvolution method with Platform Effect Removal) v1.2.1\n",
      "\n",
      "\n",
      "running options:\n",
      "spatial_file: /home/exouser/Spatial/sim_seq_based_spatial_spot_nUMI.csv\n",
      "ref_file: /home/exouser/Spatial/scRNA_data_full.csv\n",
      "ref_celltype_file: /home/exouser/Spatial/ref_scRNA_cell_celltype.csv\n",
      "marker_file: None\n",
      "loc_file: None\n",
      "A_file: /home/exouser/Spatial/sim_spatial_spot_adjacency_matrix.csv\n",
      "n_cores: 64\n",
      "threshold: 0\n",
      "use_cvae: True\n",
      "use_imputation: False\n",
      "diagnosis: False\n",
      "verbose: True\n",
      "use_fdr: True\n",
      "p_val_cutoff: 0.05\n",
      "fc_cutoff: 1.2\n",
      "pct1_cutoff: 0.3\n",
      "pct2_cutoff: 0.1\n",
      "sortby_fc: True\n",
      "n_marker_per_cmp: 20\n",
      "filter_cell: True\n",
      "filter_gene: True\n",
      "n_hv_gene: 200\n",
      "n_pseudo_spot: 0\n",
      "pseudo_spot_min_cell: 2\n",
      "pseudo_spot_max_cell: 8\n",
      "seq_depth_scaler: 10000\n",
      "cvae_input_scaler: 10\n",
      "cvae_init_lr: 0.003\n",
      "num_hidden_layer: 1\n",
      "use_batch_norm: False\n",
      "cvae_train_epoch: 1000\n",
      "use_spatial_pseudo: False\n",
      "redo_de: True\n",
      "seed: 383\n",
      "lambda_r: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "lambda_g: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "diameter: 200\n",
      "impute_diameter: [160, 114, 80]\n",
      "\n",
      "\n",
      "######### Preprocessing... #########\n",
      "\n",
      "first build CVAE...\n",
      "\n",
      "read spatial data from file /home/exouser/Spatial/sim_seq_based_spatial_spot_nUMI.csv\n",
      "total 581 spots; 25187 genes\n",
      "\n",
      "filtering genes present in <3 spots: 9510 genes removed\n",
      "\n",
      "read scRNA-seq data from file /home/exouser/Spatial/scRNA_data_full.csv\n",
      "total 23178 cells; 45768 genes\n",
      "read scRNA-seq cell-type annotation from file /home/exouser/Spatial/ref_scRNA_cell_celltype.csv\n",
      "total 12 cell-types\n",
      "subset cells with cell-type annotation, finally keep 11835 cells; 45768 genes\n",
      "\n",
      "filtering cells with <200 genes: No cells removed\n",
      "\n",
      "filtering genes present in <10 cells: 12063 genes removed\n",
      "\n",
      "total 14682 overlapped genes\n",
      "\n",
      "identify 200 highly variable genes from scRNA-seq data...\n",
      "\n",
      "identify cell-type marker genes...\n",
      "no marker gene profile provided. Perform DE to get cell-type marker genes on scRNA-seq data...\n",
      "\n",
      "Differential analysis across cell-types on scRNA-seq data...\n",
      "0%...8%...17%...25%...33%...42%...50%...58%...67%...75%...83%...92%...finally selected 748 cell-type marker genes\n",
      "\n",
      "\n",
      "use union of highly variable gene list and cell-type marker gene list derived from scRNA-seq data, finally get 829 genes for downstream analysis\n",
      "\n",
      "start CVAE building...\n",
      "\n",
      "generate 0 pseudo-spots containing 2 to 8 cells from scRNA-seq cells...\n",
      "generate 0 pseudo-spots containing 2 to 6 spots from spatial spots...\n",
      "\n",
      "WARNING: first apply log transformation on sequencing depth normalized gene expressions, followed by Min-Max scaling\n",
      "\n",
      "                         |  training | validation\n",
      "spatial spots            |       581 |         0\n",
      "spatial pseudo-spots     |         0 |         0\n",
      "scRNA-seq cells          |     11835 |         0\n",
      "scRNA-seq pseudo-spots   |         0 |         0\n",
      "\n",
      "scaling inputs to range 0 to 10\n",
      "\n",
      "CVAE structure:\n",
      "Encoder: 830 - 172 - 36\n",
      "Decoder: 37 - 172 - 829\n",
      "\n",
      "\n",
      "Start training without validation data...\n",
      "\n",
      "Train on 12416 samples\n",
      "Epoch 1/1000\n",
      "12416/12416 [==============================] - 0s 23us/sample - loss: 1043.1133 - reconstruction_loss: 897.8052 - KL_loss: 51.4353 - lr: 0.0030\n",
      "Epoch 2/1000\n",
      "12416/12416 [==============================] - 0s 11us/sample - loss: 1009.9285 - reconstruction_loss: 869.5657 - KL_loss: 50.9869 - lr: 0.0030\n",
      "Epoch 3/1000\n",
      "12416/12416 [==============================] - 0s 12us/sample - loss: 901.4122 - reconstruction_loss: 762.3065 - KL_loss: 56.8840 - lr: 0.0030\n",
      "Epoch 4/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 820.6034 - reconstruction_loss: 654.7513 - KL_loss: 88.1723 - lr: 0.0030\n",
      "Epoch 5/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 743.6669 - reconstruction_loss: 594.6785 - KL_loss: 75.8789 - lr: 0.0030\n",
      "Epoch 6/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 687.6343 - reconstruction_loss: 559.8549 - KL_loss: 57.5756 - lr: 0.0030\n",
      "Epoch 7/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 642.8727 - reconstruction_loss: 513.2599 - KL_loss: 61.7357 - lr: 0.0030\n",
      "Epoch 8/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 602.4619 - reconstruction_loss: 485.1099 - KL_loss: 51.4179 - lr: 0.0030\n",
      "Epoch 9/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 581.0317 - reconstruction_loss: 476.4494 - KL_loss: 39.7573 - lr: 0.0030\n",
      "Epoch 10/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 552.2048 - reconstruction_loss: 447.3533 - KL_loss: 41.8965 - lr: 0.0030\n",
      "Epoch 11/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 535.1785 - reconstruction_loss: 432.0447 - KL_loss: 41.2311 - lr: 0.0030\n",
      "Epoch 12/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 516.4369 - reconstruction_loss: 420.8587 - KL_loss: 35.3679 - lr: 0.0030\n",
      "Epoch 13/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 502.2039 - reconstruction_loss: 406.8704 - KL_loss: 36.1927 - lr: 0.0030\n",
      "Epoch 14/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 491.7232 - reconstruction_loss: 400.3363 - KL_loss: 33.2784 - lr: 0.0030\n",
      "Epoch 15/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 484.1817 - reconstruction_loss: 392.3064 - KL_loss: 34.9195 - lr: 0.0030\n",
      "Epoch 16/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 471.5403 - reconstruction_loss: 382.6472 - KL_loss: 32.8806 - lr: 0.0030\n",
      "Epoch 17/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 462.2080 - reconstruction_loss: 373.5557 - KL_loss: 33.6303 - lr: 0.0030\n",
      "Epoch 18/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 458.6284 - reconstruction_loss: 371.8837 - KL_loss: 32.4500 - lr: 0.0030\n",
      "Epoch 19/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 446.6144 - reconstruction_loss: 358.3452 - KL_loss: 34.7296 - lr: 0.0030\n",
      "Epoch 20/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 439.4609 - reconstruction_loss: 351.7346 - KL_loss: 34.8427 - lr: 0.0030\n",
      "Epoch 21/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 441.1181 - reconstruction_loss: 355.2835 - KL_loss: 32.9360 - lr: 0.0030\n",
      "Epoch 22/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 430.5132 - reconstruction_loss: 344.2920 - KL_loss: 34.0612 - lr: 0.0030\n",
      "Epoch 23/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 438.5160 - reconstruction_loss: 348.9032 - KL_loss: 37.2255 - lr: 0.0030\n",
      "Epoch 24/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 435.1038 - reconstruction_loss: 345.7863 - KL_loss: 37.1094 - lr: 0.0030\n",
      "Epoch 25/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 417.6282 - reconstruction_loss: 331.8860 - KL_loss: 34.3584 - lr: 0.0030\n",
      "Epoch 26/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 413.5515 - reconstruction_loss: 329.3830 - KL_loss: 33.2194 - lr: 0.0030\n",
      "Epoch 27/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 412.2307 - reconstruction_loss: 326.8218 - KL_loss: 34.6829 - lr: 0.0030\n",
      "Epoch 28/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 406.5077 - reconstruction_loss: 322.6277 - KL_loss: 33.5968 - lr: 0.0030\n",
      "Epoch 29/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 412.2335 - reconstruction_loss: 330.8840 - KL_loss: 31.0368 - lr: 0.0030\n",
      "Epoch 30/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 410.3714 - reconstruction_loss: 329.0761 - KL_loss: 31.1638 - lr: 0.0030\n",
      "Epoch 31/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 400.1297 - reconstruction_loss: 317.0796 - KL_loss: 33.5013 - lr: 0.0030\n",
      "Epoch 32/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 400.2674 - reconstruction_loss: 316.4688 - KL_loss: 34.1978 - lr: 0.0030\n",
      "Epoch 33/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 394.5839 - reconstruction_loss: 312.3374 - KL_loss: 33.0499 - lr: 0.0030\n",
      "Epoch 34/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 396.4928 - reconstruction_loss: 314.7480 - KL_loss: 32.5211 - lr: 0.0030\n",
      "Epoch 35/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 391.1112 - reconstruction_loss: 309.7918 - KL_loss: 32.4224 - lr: 0.0030\n",
      "Epoch 36/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 392.1892 - reconstruction_loss: 310.4203 - KL_loss: 32.9637 - lr: 0.0030\n",
      "Epoch 37/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 389.5304 - reconstruction_loss: 307.2474 - KL_loss: 33.6607 - lr: 0.0030\n",
      "Epoch 38/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 383.5104 - reconstruction_loss: 301.4931 - KL_loss: 33.8523 - lr: 0.0030\n",
      "Epoch 39/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 382.1441 - reconstruction_loss: 300.7690 - KL_loss: 33.3440 - lr: 0.0030\n",
      "Epoch 40/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 379.5013 - reconstruction_loss: 297.6570 - KL_loss: 33.9659 - lr: 0.0030\n",
      "Epoch 41/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 376.1063 - reconstruction_loss: 295.0294 - KL_loss: 33.5404 - lr: 0.0030\n",
      "Epoch 42/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 376.1385 - reconstruction_loss: 295.4485 - KL_loss: 33.1601 - lr: 0.0030\n",
      "Epoch 43/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 373.6584 - reconstruction_loss: 293.3576 - KL_loss: 32.9181 - lr: 0.0030\n",
      "Epoch 44/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 371.3452 - reconstruction_loss: 290.8432 - KL_loss: 33.4577 - lr: 0.0030\n",
      "Epoch 45/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 369.7689 - reconstruction_loss: 288.9823 - KL_loss: 33.7955 - lr: 0.0030\n",
      "Epoch 46/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 367.8558 - reconstruction_loss: 287.2415 - KL_loss: 33.8336 - lr: 0.0030\n",
      "Epoch 47/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 365.9622 - reconstruction_loss: 284.4935 - KL_loss: 34.8315 - lr: 0.0030\n",
      "Epoch 48/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 364.8190 - reconstruction_loss: 283.5448 - KL_loss: 34.7618 - lr: 0.0030\n",
      "Epoch 49/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 361.9745 - reconstruction_loss: 280.8133 - KL_loss: 34.9752 - lr: 0.0030\n",
      "Epoch 50/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 361.7075 - reconstruction_loss: 281.2104 - KL_loss: 34.2943 - lr: 0.0030\n",
      "Epoch 51/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 359.0210 - reconstruction_loss: 278.0874 - KL_loss: 35.0089 - lr: 0.0030\n",
      "Epoch 52/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 358.5562 - reconstruction_loss: 277.0153 - KL_loss: 35.7193 - lr: 0.0030\n",
      "Epoch 53/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 356.6073 - reconstruction_loss: 275.5316 - KL_loss: 35.4686 - lr: 0.0030\n",
      "Epoch 54/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 354.8230 - reconstruction_loss: 274.6371 - KL_loss: 34.7357 - lr: 0.0030\n",
      "Epoch 55/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 353.5557 - reconstruction_loss: 273.4092 - KL_loss: 34.8231 - lr: 0.0030\n",
      "Epoch 56/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 351.9917 - reconstruction_loss: 270.8458 - KL_loss: 36.0045 - lr: 0.0030\n",
      "Epoch 57/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 350.7087 - reconstruction_loss: 269.6580 - KL_loss: 35.9801 - lr: 0.0030\n",
      "Epoch 58/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 349.6074 - reconstruction_loss: 268.3316 - KL_loss: 36.3369 - lr: 0.0030\n",
      "Epoch 59/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 347.9668 - reconstruction_loss: 267.6441 - KL_loss: 35.5335 - lr: 0.0030\n",
      "Epoch 60/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 346.5999 - reconstruction_loss: 265.7598 - KL_loss: 36.2048 - lr: 0.0030\n",
      "Epoch 61/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 345.0285 - reconstruction_loss: 265.0590 - KL_loss: 35.5829 - lr: 0.0030\n",
      "Epoch 62/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 343.6404 - reconstruction_loss: 263.1963 - KL_loss: 36.1671 - lr: 0.0030\n",
      "Epoch 63/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 343.2538 - reconstruction_loss: 263.3566 - KL_loss: 35.6814 - lr: 0.0030\n",
      "Epoch 64/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 341.7296 - reconstruction_loss: 261.1125 - KL_loss: 36.5301 - lr: 0.0030\n",
      "Epoch 65/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 339.9070 - reconstruction_loss: 259.6326 - KL_loss: 36.4189 - lr: 0.0030\n",
      "Epoch 66/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 339.1440 - reconstruction_loss: 258.4462 - KL_loss: 36.9706 - lr: 0.0030\n",
      "Epoch 67/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 338.7577 - reconstruction_loss: 259.1380 - KL_loss: 35.9510 - lr: 0.0030\n",
      "Epoch 68/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 336.8448 - reconstruction_loss: 256.8706 - KL_loss: 36.5344 - lr: 0.0030\n",
      "Epoch 69/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 335.5042 - reconstruction_loss: 255.3695 - KL_loss: 36.8102 - lr: 0.0030\n",
      "Epoch 70/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 337.0295 - reconstruction_loss: 257.4832 - KL_loss: 36.2065 - lr: 0.0030\n",
      "Epoch 71/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 334.0038 - reconstruction_loss: 253.9451 - KL_loss: 36.9460 - lr: 0.0030\n",
      "Epoch 72/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 339.0403 - reconstruction_loss: 257.5809 - KL_loss: 38.1549 - lr: 0.0030\n",
      "Epoch 73/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 338.7245 - reconstruction_loss: 257.6221 - KL_loss: 37.8185 - lr: 0.0030\n",
      "Epoch 74/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 334.3157 - reconstruction_loss: 255.1784 - KL_loss: 36.1509 - lr: 0.0030\n",
      "Epoch 75/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 332.4160 - reconstruction_loss: 253.5253 - KL_loss: 36.1885 - lr: 0.0030\n",
      "Epoch 76/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 332.8399 - reconstruction_loss: 252.4618 - KL_loss: 37.6413 - lr: 0.0030\n",
      "Epoch 77/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 331.4165 - reconstruction_loss: 251.1196 - KL_loss: 37.6470 - lr: 0.0030\n",
      "Epoch 78/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 331.8238 - reconstruction_loss: 252.7883 - KL_loss: 36.4827 - lr: 0.0030\n",
      "Epoch 79/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 329.8241 - reconstruction_loss: 250.7700 - KL_loss: 36.6020 - lr: 0.0030\n",
      "Epoch 80/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 331.6039 - reconstruction_loss: 251.1661 - KL_loss: 38.0650 - lr: 0.0030\n",
      "Epoch 81/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 330.9118 - reconstruction_loss: 250.4852 - KL_loss: 38.1638 - lr: 0.0030\n",
      "Epoch 82/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 326.5817 - reconstruction_loss: 247.6905 - KL_loss: 36.9173 - lr: 0.0030\n",
      "Epoch 83/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 326.5654 - reconstruction_loss: 248.0981 - KL_loss: 36.6014 - lr: 0.0030\n",
      "Epoch 84/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 326.5550 - reconstruction_loss: 247.3280 - KL_loss: 37.3767 - lr: 0.0030\n",
      "Epoch 85/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 324.1550 - reconstruction_loss: 245.0430 - KL_loss: 37.4191 - lr: 0.0030\n",
      "Epoch 86/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 326.3210 - reconstruction_loss: 247.4247 - KL_loss: 37.1166 - lr: 0.0030\n",
      "Epoch 87/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 324.9643 - reconstruction_loss: 245.7695 - KL_loss: 37.5280 - lr: 0.0030\n",
      "Epoch 88/1000\n",
      "12416/12416 [==============================] - 0s 11us/sample - loss: 325.2332 - reconstruction_loss: 245.6859 - KL_loss: 38.0614 - lr: 0.0030\n",
      "Epoch 89/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 324.4120 - reconstruction_loss: 244.9060 - KL_loss: 38.0070 - lr: 0.0030\n",
      "Epoch 90/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 324.4553 - reconstruction_loss: 245.6905 - KL_loss: 37.3579 - lr: 0.0030\n",
      "Epoch 91/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 323.1948 - reconstruction_loss: 244.6780 - KL_loss: 37.2359 - lr: 0.0030\n",
      "Epoch 92/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 321.8306 - reconstruction_loss: 242.9995 - KL_loss: 37.7203 - lr: 0.0030\n",
      "Epoch 93/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 321.0901 - reconstruction_loss: 242.4059 - KL_loss: 37.6817 - lr: 0.0030\n",
      "Epoch 94/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 320.6754 - reconstruction_loss: 242.3094 - KL_loss: 37.4771 - lr: 0.0030\n",
      "Epoch 95/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 319.5525 - reconstruction_loss: 240.9810 - KL_loss: 37.8283 - lr: 0.0030\n",
      "Epoch 96/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 319.7405 - reconstruction_loss: 241.1375 - KL_loss: 37.8742 - lr: 0.0030\n",
      "Epoch 97/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 318.5677 - reconstruction_loss: 240.3636 - KL_loss: 37.6275 - lr: 0.0030\n",
      "Epoch 98/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 319.1540 - reconstruction_loss: 240.9066 - KL_loss: 37.6690 - lr: 0.0030\n",
      "Epoch 99/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 317.7168 - reconstruction_loss: 239.7250 - KL_loss: 37.5394 - lr: 0.0030\n",
      "Epoch 100/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 318.4749 - reconstruction_loss: 240.4626 - KL_loss: 37.5648 - lr: 0.0030\n",
      "Epoch 101/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 317.4209 - reconstruction_loss: 239.3548 - KL_loss: 37.7242 - lr: 0.0030\n",
      "Epoch 102/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 316.1119 - reconstruction_loss: 238.0658 - KL_loss: 37.8561 - lr: 0.0030\n",
      "Epoch 103/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 315.3475 - reconstruction_loss: 237.5688 - KL_loss: 37.7699 - lr: 0.0030\n",
      "Epoch 104/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 315.3805 - reconstruction_loss: 237.5312 - KL_loss: 37.8538 - lr: 0.0030\n",
      "Epoch 105/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 313.8475 - reconstruction_loss: 236.3361 - KL_loss: 37.6991 - lr: 0.0030\n",
      "Epoch 106/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 315.1239 - reconstruction_loss: 237.3342 - KL_loss: 37.9644 - lr: 0.0030\n",
      "Epoch 107/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 313.8248 - reconstruction_loss: 236.1384 - KL_loss: 37.9892 - lr: 0.0030\n",
      "Epoch 108/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 312.6973 - reconstruction_loss: 235.3855 - KL_loss: 37.7473 - lr: 0.0030\n",
      "Epoch 109/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 312.3860 - reconstruction_loss: 235.0751 - KL_loss: 37.8282 - lr: 0.0030\n",
      "Epoch 110/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 311.7367 - reconstruction_loss: 234.3049 - KL_loss: 38.0623 - lr: 0.0030\n",
      "Epoch 111/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 310.8726 - reconstruction_loss: 233.4398 - KL_loss: 38.2100 - lr: 0.0030\n",
      "Epoch 112/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 310.7108 - reconstruction_loss: 233.5017 - KL_loss: 38.0247 - lr: 0.0030\n",
      "Epoch 113/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 309.9455 - reconstruction_loss: 232.9498 - KL_loss: 37.9158 - lr: 0.0030\n",
      "Epoch 114/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 309.8467 - reconstruction_loss: 232.4648 - KL_loss: 38.4036 - lr: 0.0030\n",
      "Epoch 115/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 309.1430 - reconstruction_loss: 232.1263 - KL_loss: 38.1888 - lr: 0.0030\n",
      "Epoch 116/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 308.3240 - reconstruction_loss: 231.3226 - KL_loss: 38.2454 - lr: 0.0030\n",
      "Epoch 117/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 307.6919 - reconstruction_loss: 231.0182 - KL_loss: 38.0396 - lr: 0.0030\n",
      "Epoch 118/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 307.3038 - reconstruction_loss: 230.4238 - KL_loss: 38.3090 - lr: 0.0030\n",
      "Epoch 119/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 306.1295 - reconstruction_loss: 229.6536 - KL_loss: 38.0992 - lr: 0.0030\n",
      "Epoch 120/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 306.1694 - reconstruction_loss: 229.7145 - KL_loss: 38.1457 - lr: 0.0030\n",
      "Epoch 121/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 305.1312 - reconstruction_loss: 228.9508 - KL_loss: 38.0140 - lr: 0.0030\n",
      "Epoch 122/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 304.5598 - reconstruction_loss: 228.1479 - KL_loss: 38.3636 - lr: 0.0030\n",
      "Epoch 123/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 304.2873 - reconstruction_loss: 228.2395 - KL_loss: 38.0569 - lr: 0.0030\n",
      "Epoch 124/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 303.6895 - reconstruction_loss: 227.3768 - KL_loss: 38.4334 - lr: 0.0030\n",
      "Epoch 125/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 302.8678 - reconstruction_loss: 226.8348 - KL_loss: 38.3058 - lr: 0.0030\n",
      "Epoch 126/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 302.8132 - reconstruction_loss: 226.4871 - KL_loss: 38.6497 - lr: 0.0030\n",
      "Epoch 127/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 301.8673 - reconstruction_loss: 226.2420 - KL_loss: 38.1128 - lr: 0.0030\n",
      "Epoch 128/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 301.7773 - reconstruction_loss: 225.8119 - KL_loss: 38.4961 - lr: 0.0030\n",
      "Epoch 129/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 300.7350 - reconstruction_loss: 225.0343 - KL_loss: 38.3898 - lr: 0.0030\n",
      "Epoch 130/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 300.9261 - reconstruction_loss: 224.9950 - KL_loss: 38.6866 - lr: 0.0030\n",
      "Epoch 131/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 300.0689 - reconstruction_loss: 224.5515 - KL_loss: 38.3821 - lr: 0.0030\n",
      "Epoch 132/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 299.4798 - reconstruction_loss: 224.0262 - KL_loss: 38.4393 - lr: 0.0030\n",
      "Epoch 133/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 299.4573 - reconstruction_loss: 223.8309 - KL_loss: 38.6957 - lr: 0.0030\n",
      "Epoch 134/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 298.0757 - reconstruction_loss: 222.7712 - KL_loss: 38.5431 - lr: 0.0030\n",
      "Epoch 135/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 298.4704 - reconstruction_loss: 222.9955 - KL_loss: 38.7819 - lr: 0.0030\n",
      "Epoch 136/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 297.2675 - reconstruction_loss: 222.3030 - KL_loss: 38.4193 - lr: 0.0030\n",
      "Epoch 137/1000\n",
      "12416/12416 [==============================] - 0s 11us/sample - loss: 297.4187 - reconstruction_loss: 221.9677 - KL_loss: 38.9448 - lr: 0.0030\n",
      "Epoch 138/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 297.0242 - reconstruction_loss: 221.6166 - KL_loss: 39.0067 - lr: 0.0030\n",
      "Epoch 139/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 295.5237 - reconstruction_loss: 220.5188 - KL_loss: 38.7646 - lr: 0.0030\n",
      "Epoch 140/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 296.4477 - reconstruction_loss: 221.5415 - KL_loss: 38.6657 - lr: 0.0030\n",
      "Epoch 141/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 294.9086 - reconstruction_loss: 220.2338 - KL_loss: 38.6212 - lr: 0.0030\n",
      "Epoch 142/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 295.7552 - reconstruction_loss: 220.5120 - KL_loss: 39.2238 - lr: 0.0030\n",
      "Epoch 143/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 294.8125 - reconstruction_loss: 219.6705 - KL_loss: 39.2594 - lr: 0.0030\n",
      "Epoch 144/1000\n",
      "12416/12416 [==============================] - 0s 11us/sample - loss: 294.2236 - reconstruction_loss: 219.5404 - KL_loss: 38.8373 - lr: 0.0030\n",
      "Epoch 145/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 294.5902 - reconstruction_loss: 219.8110 - KL_loss: 38.9642 - lr: 0.0030\n",
      "Epoch 146/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 293.2886 - reconstruction_loss: 218.8638 - KL_loss: 38.8455 - lr: 0.0030\n",
      "Epoch 147/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 292.6011 - reconstruction_loss: 218.0031 - KL_loss: 39.1006 - lr: 0.0030\n",
      "Epoch 148/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 292.9332 - reconstruction_loss: 218.3134 - KL_loss: 39.1311 - lr: 0.0030\n",
      "Epoch 149/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 291.8741 - reconstruction_loss: 217.5379 - KL_loss: 39.0128 - lr: 0.0030\n",
      "Epoch 150/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 291.7067 - reconstruction_loss: 217.5422 - KL_loss: 38.8821 - lr: 0.0030\n",
      "Epoch 151/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 291.3322 - reconstruction_loss: 216.8371 - KL_loss: 39.3209 - lr: 0.0030\n",
      "Epoch 152/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 290.3699 - reconstruction_loss: 216.0741 - KL_loss: 39.2605 - lr: 0.0030\n",
      "Epoch 153/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 290.5815 - reconstruction_loss: 216.3972 - KL_loss: 39.2093 - lr: 0.0030\n",
      "Epoch 154/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 290.0098 - reconstruction_loss: 216.1647 - KL_loss: 38.9161 - lr: 0.0030\n",
      "Epoch 155/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 289.0114 - reconstruction_loss: 215.0789 - KL_loss: 39.1593 - lr: 0.0030\n",
      "Epoch 156/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 289.5534 - reconstruction_loss: 215.5755 - KL_loss: 39.2316 - lr: 0.0030\n",
      "Epoch 157/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 288.1161 - reconstruction_loss: 214.4211 - KL_loss: 39.1254 - lr: 0.0030\n",
      "Epoch 158/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 289.2258 - reconstruction_loss: 215.2310 - KL_loss: 39.3256 - lr: 0.0030\n",
      "Epoch 159/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 288.3951 - reconstruction_loss: 214.6566 - KL_loss: 39.2294 - lr: 0.0030\n",
      "Epoch 160/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 288.4253 - reconstruction_loss: 214.4285 - KL_loss: 39.4941 - lr: 0.0030\n",
      "Epoch 161/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 287.5133 - reconstruction_loss: 213.6907 - KL_loss: 39.5157 - lr: 0.0030\n",
      "Epoch 162/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 287.5954 - reconstruction_loss: 213.9435 - KL_loss: 39.3894 - lr: 0.0030\n",
      "Epoch 163/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 286.3296 - reconstruction_loss: 212.8508 - KL_loss: 39.3445 - lr: 0.0030\n",
      "Epoch 164/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 287.6928 - reconstruction_loss: 214.0484 - KL_loss: 39.4425 - lr: 0.0030\n",
      "Epoch 165/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 287.2289 - reconstruction_loss: 213.3384 - KL_loss: 39.7747 - lr: 0.0030\n",
      "Epoch 166/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 285.4857 - reconstruction_loss: 212.0610 - KL_loss: 39.5332 - lr: 0.0030\n",
      "Epoch 167/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 286.3858 - reconstruction_loss: 213.0820 - KL_loss: 39.4031 - lr: 0.0030\n",
      "Epoch 168/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 285.1021 - reconstruction_loss: 212.1322 - KL_loss: 39.2272 - lr: 0.0030\n",
      "Epoch 169/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 285.2255 - reconstruction_loss: 211.7076 - KL_loss: 39.7761 - lr: 0.0030\n",
      "Epoch 170/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 285.3197 - reconstruction_loss: 211.9745 - KL_loss: 39.6599 - lr: 0.0030\n",
      "Epoch 171/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 283.6888 - reconstruction_loss: 210.7215 - KL_loss: 39.4336 - lr: 0.0030\n",
      "Epoch 172/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 284.7453 - reconstruction_loss: 211.9541 - KL_loss: 39.1631 - lr: 0.0030\n",
      "Epoch 173/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 284.2595 - reconstruction_loss: 211.3712 - KL_loss: 39.4096 - lr: 0.0030\n",
      "Epoch 174/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 283.1960 - reconstruction_loss: 210.1170 - KL_loss: 39.7294 - lr: 0.0030\n",
      "Epoch 175/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 283.5004 - reconstruction_loss: 210.0530 - KL_loss: 40.1340 - lr: 0.0030\n",
      "Epoch 176/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 283.1494 - reconstruction_loss: 209.9560 - KL_loss: 39.8875 - lr: 0.0030\n",
      "Epoch 177/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 282.4828 - reconstruction_loss: 210.0952 - KL_loss: 39.3120 - lr: 0.0030\n",
      "Epoch 178/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 282.3032 - reconstruction_loss: 210.2233 - KL_loss: 39.0097 - lr: 0.0030\n",
      "Epoch 179/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 282.6406 - reconstruction_loss: 210.0416 - KL_loss: 39.5733 - lr: 0.0030\n",
      "Epoch 180/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 281.0714 - reconstruction_loss: 208.2057 - KL_loss: 39.9924 - lr: 0.0030\n",
      "Epoch 181/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 282.5876 - reconstruction_loss: 209.2328 - KL_loss: 40.3741 - lr: 0.0030\n",
      "Epoch 182/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 281.7277 - reconstruction_loss: 208.6639 - KL_loss: 40.1461 - lr: 0.0030\n",
      "Epoch 183/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 281.0892 - reconstruction_loss: 208.6386 - KL_loss: 39.6462 - lr: 0.0030\n",
      "Epoch 184/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 280.5704 - reconstruction_loss: 208.6129 - KL_loss: 39.2519 - lr: 0.0030\n",
      "Epoch 185/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 281.5623 - reconstruction_loss: 209.3307 - KL_loss: 39.5408 - lr: 0.0030\n",
      "Epoch 186/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 280.2264 - reconstruction_loss: 207.6192 - KL_loss: 40.0409 - lr: 0.0030\n",
      "Epoch 187/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 280.8813 - reconstruction_loss: 208.1435 - KL_loss: 40.1722 - lr: 0.0030\n",
      "Epoch 188/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 280.5261 - reconstruction_loss: 208.0175 - KL_loss: 39.9550 - lr: 0.0030\n",
      "Epoch 189/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 279.8079 - reconstruction_loss: 207.7003 - KL_loss: 39.7096 - lr: 0.0030\n",
      "Epoch 190/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 280.2200 - reconstruction_loss: 208.0982 - KL_loss: 39.7117 - lr: 0.0030\n",
      "Epoch 191/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 278.6814 - reconstruction_loss: 206.5899 - KL_loss: 39.8006 - lr: 0.0030\n",
      "Epoch 192/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 280.1344 - reconstruction_loss: 207.9106 - KL_loss: 39.8922 - lr: 0.0030\n",
      "Epoch 193/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 279.3806 - reconstruction_loss: 206.9959 - KL_loss: 40.1137 - lr: 0.0030\n",
      "Epoch 194/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 278.6726 - reconstruction_loss: 206.4657 - KL_loss: 40.0691 - lr: 0.0030\n",
      "Epoch 195/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 278.9315 - reconstruction_loss: 206.7428 - KL_loss: 40.0375 - lr: 0.0030\n",
      "Epoch 196/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 278.1863 - reconstruction_loss: 206.5240 - KL_loss: 39.6334 - lr: 0.0030\n",
      "Epoch 197/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 278.0820 - reconstruction_loss: 206.3772 - KL_loss: 39.6961 - lr: 0.0030\n",
      "Epoch 198/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 277.1534 - reconstruction_loss: 205.2128 - KL_loss: 40.0738 - lr: 0.0030\n",
      "Epoch 199/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 277.8710 - reconstruction_loss: 205.7184 - KL_loss: 40.1795 - lr: 0.0030\n",
      "Epoch 200/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 277.7138 - reconstruction_loss: 205.5414 - KL_loss: 40.2887 - lr: 0.0030\n",
      "Epoch 201/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 276.4658 - reconstruction_loss: 204.8194 - KL_loss: 39.9439 - lr: 0.0030\n",
      "Epoch 202/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 277.5673 - reconstruction_loss: 205.8292 - KL_loss: 39.9966 - lr: 0.0030\n",
      "Epoch 203/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 276.9655 - reconstruction_loss: 205.3680 - KL_loss: 39.9308 - lr: 0.0030\n",
      "Epoch 204/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 275.8263 - reconstruction_loss: 204.0901 - KL_loss: 40.2575 - lr: 0.0030\n",
      "Epoch 205/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 276.5695 - reconstruction_loss: 204.7137 - KL_loss: 40.2637 - lr: 0.0030\n",
      "Epoch 206/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 275.6942 - reconstruction_loss: 204.0367 - KL_loss: 40.2144 - lr: 0.0030\n",
      "Epoch 207/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 275.8120 - reconstruction_loss: 204.2928 - KL_loss: 40.1356 - lr: 0.0030\n",
      "Epoch 208/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 275.5797 - reconstruction_loss: 204.4155 - KL_loss: 39.7493 - lr: 0.0030\n",
      "Epoch 209/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 275.4382 - reconstruction_loss: 204.1527 - KL_loss: 40.0285 - lr: 0.0030\n",
      "Epoch 210/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 274.9942 - reconstruction_loss: 203.3451 - KL_loss: 40.3799 - lr: 0.0030\n",
      "Epoch 211/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 275.1573 - reconstruction_loss: 203.5517 - KL_loss: 40.4346 - lr: 0.0030\n",
      "Epoch 212/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 274.0877 - reconstruction_loss: 202.7535 - KL_loss: 40.2167 - lr: 0.0030\n",
      "Epoch 213/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 275.6408 - reconstruction_loss: 204.3255 - KL_loss: 40.1778 - lr: 0.0030\n",
      "Epoch 214/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 274.1499 - reconstruction_loss: 202.9766 - KL_loss: 40.0989 - lr: 0.0030\n",
      "Epoch 215/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 276.5995 - reconstruction_loss: 205.0183 - KL_loss: 40.2797 - lr: 0.0030\n",
      "Epoch 216/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 275.6124 - reconstruction_loss: 203.9967 - KL_loss: 40.4438 - lr: 0.0030\n",
      "Epoch 217/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 274.7600 - reconstruction_loss: 203.2898 - KL_loss: 40.4466 - lr: 0.0030\n",
      "Epoch 218/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 274.9174 - reconstruction_loss: 203.4547 - KL_loss: 40.4469 - lr: 0.0030\n",
      "Epoch 219/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 273.6371 - reconstruction_loss: 202.6865 - KL_loss: 40.0952 - lr: 0.0030\n",
      "Epoch 220/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 274.3501 - reconstruction_loss: 203.2948 - KL_loss: 40.1781 - lr: 0.0030\n",
      "Epoch 221/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 273.4623 - reconstruction_loss: 202.3740 - KL_loss: 40.3281 - lr: 0.0030\n",
      "Epoch 222/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 273.6657 - reconstruction_loss: 202.6869 - KL_loss: 40.2184 - lr: 0.0030\n",
      "Epoch 223/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 273.0979 - reconstruction_loss: 202.2774 - KL_loss: 40.1646 - lr: 0.0030\n",
      "Epoch 224/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 273.5830 - reconstruction_loss: 202.4578 - KL_loss: 40.4750 - lr: 0.0030\n",
      "Epoch 225/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 272.9744 - reconstruction_loss: 201.8745 - KL_loss: 40.5180 - lr: 0.0030\n",
      "Epoch 226/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 272.5669 - reconstruction_loss: 201.6903 - KL_loss: 40.3742 - lr: 0.0030\n",
      "Epoch 227/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 272.5404 - reconstruction_loss: 201.6352 - KL_loss: 40.3761 - lr: 0.0030\n",
      "Epoch 228/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 272.1090 - reconstruction_loss: 201.4552 - KL_loss: 40.2017 - lr: 0.0030\n",
      "Epoch 229/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 272.1594 - reconstruction_loss: 201.5342 - KL_loss: 40.1499 - lr: 0.0030\n",
      "Epoch 230/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 271.9377 - reconstruction_loss: 200.9343 - KL_loss: 40.6398 - lr: 0.0030\n",
      "Epoch 231/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 271.0687 - reconstruction_loss: 200.2094 - KL_loss: 40.6020 - lr: 0.0030\n",
      "Epoch 232/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 272.0407 - reconstruction_loss: 201.3100 - KL_loss: 40.3843 - lr: 0.0030\n",
      "Epoch 233/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 270.7379 - reconstruction_loss: 200.6437 - KL_loss: 39.9326 - lr: 0.0030\n",
      "Epoch 234/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 271.5241 - reconstruction_loss: 200.9585 - KL_loss: 40.3340 - lr: 0.0030\n",
      "Epoch 235/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 271.3853 - reconstruction_loss: 200.4738 - KL_loss: 40.7098 - lr: 0.0030\n",
      "Epoch 236/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 270.2468 - reconstruction_loss: 199.4343 - KL_loss: 40.7665 - lr: 0.0030\n",
      "Epoch 237/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 270.6189 - reconstruction_loss: 200.0695 - KL_loss: 40.5285 - lr: 0.0030\n",
      "Epoch 238/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 270.7512 - reconstruction_loss: 200.4030 - KL_loss: 40.2910 - lr: 0.0030\n",
      "Epoch 239/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 269.5940 - reconstruction_loss: 199.5553 - KL_loss: 40.1202 - lr: 0.0030\n",
      "Epoch 240/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 270.3277 - reconstruction_loss: 199.7470 - KL_loss: 40.6297 - lr: 0.0030\n",
      "Epoch 241/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 269.9610 - reconstruction_loss: 199.3095 - KL_loss: 40.7287 - lr: 0.0030\n",
      "Epoch 242/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 269.5794 - reconstruction_loss: 199.0820 - KL_loss: 40.7100 - lr: 0.0030\n",
      "Epoch 243/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 269.3287 - reconstruction_loss: 199.1212 - KL_loss: 40.4482 - lr: 0.0030\n",
      "Epoch 244/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 269.5871 - reconstruction_loss: 199.3584 - KL_loss: 40.4459 - lr: 0.0030\n",
      "Epoch 245/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 268.8978 - reconstruction_loss: 198.8553 - KL_loss: 40.3871 - lr: 0.0030\n",
      "Epoch 246/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 269.0310 - reconstruction_loss: 198.6826 - KL_loss: 40.6555 - lr: 0.0030\n",
      "Epoch 247/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 268.8147 - reconstruction_loss: 198.3460 - KL_loss: 40.8137 - lr: 0.0030\n",
      "Epoch 248/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 268.3936 - reconstruction_loss: 198.3212 - KL_loss: 40.5068 - lr: 0.0030\n",
      "Epoch 249/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 268.5840 - reconstruction_loss: 198.4523 - KL_loss: 40.5890 - lr: 0.0030\n",
      "Epoch 250/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 267.8647 - reconstruction_loss: 197.8036 - KL_loss: 40.5812 - lr: 0.0030\n",
      "Epoch 251/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 267.9962 - reconstruction_loss: 197.7501 - KL_loss: 40.7750 - lr: 0.0030\n",
      "Epoch 252/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 267.4114 - reconstruction_loss: 197.3474 - KL_loss: 40.7109 - lr: 0.0030\n",
      "Epoch 253/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 267.7894 - reconstruction_loss: 198.0334 - KL_loss: 40.4111 - lr: 0.0030\n",
      "Epoch 254/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 267.2113 - reconstruction_loss: 197.3230 - KL_loss: 40.5999 - lr: 0.0030\n",
      "Epoch 255/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 267.7350 - reconstruction_loss: 197.7184 - KL_loss: 40.7210 - lr: 0.0030\n",
      "Epoch 256/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 266.6003 - reconstruction_loss: 196.4618 - KL_loss: 40.9783 - lr: 0.0030\n",
      "Epoch 257/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 267.1847 - reconstruction_loss: 197.1669 - KL_loss: 40.8178 - lr: 0.0030\n",
      "Epoch 258/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 266.8129 - reconstruction_loss: 196.9708 - KL_loss: 40.6850 - lr: 0.0030\n",
      "Epoch 259/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 266.8390 - reconstruction_loss: 197.0235 - KL_loss: 40.7119 - lr: 0.0030\n",
      "Epoch 260/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 266.3112 - reconstruction_loss: 196.5441 - KL_loss: 40.7036 - lr: 0.0030\n",
      "Epoch 261/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 267.0277 - reconstruction_loss: 197.0645 - KL_loss: 40.8535 - lr: 0.0030\n",
      "Epoch 262/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 265.9004 - reconstruction_loss: 195.9001 - KL_loss: 41.0475 - lr: 0.0030\n",
      "Epoch 263/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 266.8471 - reconstruction_loss: 197.0308 - KL_loss: 40.8119 - lr: 0.0030\n",
      "Epoch 264/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 266.2760 - reconstruction_loss: 196.5532 - KL_loss: 40.7669 - lr: 0.0030\n",
      "Epoch 265/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 265.7118 - reconstruction_loss: 196.2796 - KL_loss: 40.6001 - lr: 0.0030\n",
      "Epoch 266/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 266.0479 - reconstruction_loss: 196.1801 - KL_loss: 41.0170 - lr: 0.0030\n",
      "Epoch 267/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 265.3535 - reconstruction_loss: 195.8004 - KL_loss: 40.7560 - lr: 0.0030\n",
      "Epoch 268/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 265.6317 - reconstruction_loss: 196.1266 - KL_loss: 40.7481 - lr: 0.0030\n",
      "Epoch 269/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 265.1482 - reconstruction_loss: 195.6842 - KL_loss: 40.7647 - lr: 0.0030\n",
      "Epoch 270/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 265.2955 - reconstruction_loss: 195.6340 - KL_loss: 40.9824 - lr: 0.0030\n",
      "Epoch 271/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 264.7267 - reconstruction_loss: 195.0808 - KL_loss: 41.0319 - lr: 0.0030\n",
      "Epoch 272/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 265.1729 - reconstruction_loss: 195.9402 - KL_loss: 40.5919 - lr: 0.0030\n",
      "Epoch 273/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 264.3929 - reconstruction_loss: 195.1358 - KL_loss: 40.7153 - lr: 0.0030\n",
      "Epoch 274/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 264.8633 - reconstruction_loss: 195.2275 - KL_loss: 41.0617 - lr: 0.0030\n",
      "Epoch 275/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 265.0994 - reconstruction_loss: 195.3222 - KL_loss: 41.2349 - lr: 0.0030\n",
      "Epoch 276/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 264.0441 - reconstruction_loss: 194.6923 - KL_loss: 40.9534 - lr: 0.0030\n",
      "Epoch 277/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 264.8108 - reconstruction_loss: 195.6885 - KL_loss: 40.6842 - lr: 0.0030\n",
      "Epoch 278/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 263.9787 - reconstruction_loss: 194.7220 - KL_loss: 40.8947 - lr: 0.0030\n",
      "Epoch 279/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 264.4751 - reconstruction_loss: 194.9972 - KL_loss: 41.1279 - lr: 0.0030\n",
      "Epoch 280/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 263.6218 - reconstruction_loss: 194.2000 - KL_loss: 41.1152 - lr: 0.0030\n",
      "Epoch 281/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 264.0457 - reconstruction_loss: 194.9629 - KL_loss: 40.8095 - lr: 0.0030\n",
      "Epoch 282/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 263.5289 - reconstruction_loss: 194.6100 - KL_loss: 40.6494 - lr: 0.0030\n",
      "Epoch 283/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 264.1928 - reconstruction_loss: 194.9358 - KL_loss: 41.0221 - lr: 0.0030\n",
      "Epoch 284/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 263.3455 - reconstruction_loss: 193.7061 - KL_loss: 41.5039 - lr: 0.0030\n",
      "Epoch 285/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 263.6693 - reconstruction_loss: 194.2510 - KL_loss: 41.2464 - lr: 0.0030\n",
      "Epoch 286/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 263.4807 - reconstruction_loss: 194.3498 - KL_loss: 40.9911 - lr: 0.0030\n",
      "Epoch 287/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 262.7812 - reconstruction_loss: 194.0568 - KL_loss: 40.6604 - lr: 0.0030\n",
      "Epoch 288/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 263.3031 - reconstruction_loss: 194.2527 - KL_loss: 40.9171 - lr: 0.0030\n",
      "Epoch 289/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 262.5684 - reconstruction_loss: 193.4737 - KL_loss: 41.0961 - lr: 0.0030\n",
      "Epoch 290/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 262.5853 - reconstruction_loss: 193.5432 - KL_loss: 41.0276 - lr: 0.0030\n",
      "Epoch 291/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 263.0754 - reconstruction_loss: 194.0227 - KL_loss: 41.0311 - lr: 0.0030\n",
      "Epoch 292/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 261.5415 - reconstruction_loss: 192.7763 - KL_loss: 40.9057 - lr: 0.0030\n",
      "Epoch 293/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 263.9552 - reconstruction_loss: 194.9352 - KL_loss: 40.9835 - lr: 0.0030\n",
      "Epoch 294/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 262.2643 - reconstruction_loss: 193.1164 - KL_loss: 41.2663 - lr: 0.0030\n",
      "Epoch 295/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 264.5923 - reconstruction_loss: 195.0853 - KL_loss: 41.4241 - lr: 0.0030\n",
      "Epoch 296/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 264.7104 - reconstruction_loss: 195.2374 - KL_loss: 41.3456 - lr: 0.0030\n",
      "Epoch 297/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 262.5993 - reconstruction_loss: 193.5431 - KL_loss: 41.2278 - lr: 0.0030\n",
      "Epoch 298/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 263.6562 - reconstruction_loss: 194.4284 - KL_loss: 41.3011 - lr: 0.0030\n",
      "Epoch 299/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 263.3608 - reconstruction_loss: 194.4768 - KL_loss: 40.9667 - lr: 0.0030\n",
      "Epoch 300/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 261.8152 - reconstruction_loss: 193.6098 - KL_loss: 40.5118 - lr: 0.0030\n",
      "Epoch 301/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 263.3477 - reconstruction_loss: 194.5439 - KL_loss: 40.9792 - lr: 0.0030\n",
      "Epoch 302/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 262.1920 - reconstruction_loss: 193.0353 - KL_loss: 41.4504 - lr: 0.0030\n",
      "Epoch 303/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 262.4238 - reconstruction_loss: 193.1840 - KL_loss: 41.5742 - lr: 0.0030\n",
      "Epoch 304/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 262.9486 - reconstruction_loss: 193.9579 - KL_loss: 41.2064 - lr: 0.0030\n",
      "Epoch 305/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 261.7340 - reconstruction_loss: 193.3950 - KL_loss: 40.7185 - lr: 0.0030\n",
      "Epoch 306/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 261.7895 - reconstruction_loss: 193.5786 - KL_loss: 40.5663 - lr: 0.0030\n",
      "Epoch 307/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 261.7868 - reconstruction_loss: 193.1638 - KL_loss: 41.0333 - lr: 0.0030\n",
      "Epoch 308/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 260.8851 - reconstruction_loss: 191.7853 - KL_loss: 41.6242 - lr: 0.0030\n",
      "Epoch 309/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 261.7898 - reconstruction_loss: 192.6575 - KL_loss: 41.5839 - lr: 0.0030\n",
      "Epoch 310/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 261.2350 - reconstruction_loss: 192.7653 - KL_loss: 41.0045 - lr: 0.0030\n",
      "Epoch 311/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 260.8513 - reconstruction_loss: 193.0911 - KL_loss: 40.3390 - lr: 0.0030\n",
      "Epoch 312/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 261.0869 - reconstruction_loss: 193.0864 - KL_loss: 40.5892 - lr: 0.0030\n",
      "Epoch 313/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 260.8714 - reconstruction_loss: 192.1537 - KL_loss: 41.3065 - lr: 0.0030\n",
      "Epoch 314/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 260.4140 - reconstruction_loss: 191.3192 - KL_loss: 41.7465 - lr: 0.0030\n",
      "Epoch 315/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 260.9313 - reconstruction_loss: 191.8067 - KL_loss: 41.7225 - lr: 0.0030\n",
      "Epoch 316/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 259.8896 - reconstruction_loss: 191.6820 - KL_loss: 40.9651 - lr: 0.0030\n",
      "Epoch 317/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 260.4503 - reconstruction_loss: 192.4856 - KL_loss: 40.6849 - lr: 0.0030\n",
      "Epoch 318/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 260.3042 - reconstruction_loss: 192.0652 - KL_loss: 40.9807 - lr: 0.0030\n",
      "Epoch 319/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 259.4305 - reconstruction_loss: 190.8692 - KL_loss: 41.4416 - lr: 0.0030\n",
      "Epoch 320/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 260.7671 - reconstruction_loss: 191.8798 - KL_loss: 41.6162 - lr: 0.0030\n",
      "Epoch 321/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 259.4868 - reconstruction_loss: 191.0621 - KL_loss: 41.3151 - lr: 0.0030\n",
      "Epoch 322/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 260.5542 - reconstruction_loss: 192.3011 - KL_loss: 41.0135 - lr: 0.0030\n",
      "Epoch 323/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 260.0346 - reconstruction_loss: 192.0761 - KL_loss: 40.8439 - lr: 0.0030\n",
      "Epoch 324/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 260.4505 - reconstruction_loss: 191.9418 - KL_loss: 41.3448 - lr: 0.0030\n",
      "Epoch 325/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 259.1388 - reconstruction_loss: 190.3965 - KL_loss: 41.7528 - lr: 0.0030\n",
      "Epoch 326/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 260.0484 - reconstruction_loss: 191.2223 - KL_loss: 41.7151 - lr: 0.0030\n",
      "Epoch 327/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 259.5190 - reconstruction_loss: 191.5010 - KL_loss: 40.9531 - lr: 0.0030\n",
      "Epoch 328/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 259.6643 - reconstruction_loss: 191.8253 - KL_loss: 40.7778 - lr: 0.0030\n",
      "Epoch 329/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 259.1497 - reconstruction_loss: 191.0058 - KL_loss: 41.1852 - lr: 0.0030\n",
      "Epoch 330/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 259.2523 - reconstruction_loss: 190.6031 - KL_loss: 41.6862 - lr: 0.0030\n",
      "Epoch 331/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 259.5388 - reconstruction_loss: 191.0512 - KL_loss: 41.5274 - lr: 0.0030\n",
      "Epoch 332/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 258.4243 - reconstruction_loss: 190.6234 - KL_loss: 40.9784 - lr: 0.0030\n",
      "Epoch 333/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 259.7046 - reconstruction_loss: 191.7824 - KL_loss: 40.9861 - lr: 0.0030\n",
      "Epoch 334/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 258.4194 - reconstruction_loss: 190.4615 - KL_loss: 41.1476 - lr: 0.0030\n",
      "Epoch 335/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 259.6942 - reconstruction_loss: 191.0860 - KL_loss: 41.6476 - lr: 0.0030\n",
      "Epoch 336/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 259.6222 - reconstruction_loss: 191.0616 - KL_loss: 41.6292 - lr: 0.0030\n",
      "Epoch 337/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 258.2735 - reconstruction_loss: 190.2552 - KL_loss: 41.2564 - lr: 0.0030\n",
      "Epoch 338/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 259.4026 - reconstruction_loss: 191.5677 - KL_loss: 40.9724 - lr: 0.0030\n",
      "Epoch 339/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 258.6265 - reconstruction_loss: 190.8216 - KL_loss: 41.0395 - lr: 0.0030\n",
      "Epoch 340/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 258.4211 - reconstruction_loss: 190.2945 - KL_loss: 41.3765 - lr: 0.0030\n",
      "Epoch 341/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 258.9635 - reconstruction_loss: 190.4227 - KL_loss: 41.7335 - lr: 0.0030\n",
      "Epoch 342/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 258.2846 - reconstruction_loss: 190.2469 - KL_loss: 41.3105 - lr: 0.0030\n",
      "Epoch 343/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 258.1012 - reconstruction_loss: 190.2314 - KL_loss: 41.2551 - lr: 0.0030\n",
      "Epoch 344/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 258.1635 - reconstruction_loss: 190.5592 - KL_loss: 40.9591 - lr: 0.0030\n",
      "Epoch 345/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 257.6296 - reconstruction_loss: 190.0466 - KL_loss: 40.9870 - lr: 0.0030\n",
      "Epoch 346/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 257.8514 - reconstruction_loss: 189.8996 - KL_loss: 41.3276 - lr: 0.0030\n",
      "Epoch 347/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 257.9632 - reconstruction_loss: 189.7615 - KL_loss: 41.6058 - lr: 0.0030\n",
      "Epoch 348/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 257.1221 - reconstruction_loss: 189.2601 - KL_loss: 41.3983 - lr: 0.0030\n",
      "Epoch 349/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 258.5165 - reconstruction_loss: 190.7892 - KL_loss: 41.0904 - lr: 0.0030\n",
      "Epoch 350/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 257.3181 - reconstruction_loss: 189.7099 - KL_loss: 41.0971 - lr: 0.0030\n",
      "Epoch 351/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 258.8333 - reconstruction_loss: 190.6408 - KL_loss: 41.5504 - lr: 0.0030\n",
      "Epoch 352/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 257.8906 - reconstruction_loss: 189.7315 - KL_loss: 41.7021 - lr: 0.0030\n",
      "Epoch 353/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 258.2900 - reconstruction_loss: 190.3370 - KL_loss: 41.3884 - lr: 0.0030\n",
      "Epoch 354/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 257.3495 - reconstruction_loss: 189.8113 - KL_loss: 41.1211 - lr: 0.0030\n",
      "Epoch 355/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 258.8951 - reconstruction_loss: 190.6265 - KL_loss: 41.6538 - lr: 0.0030\n",
      "Epoch 356/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 257.7973 - reconstruction_loss: 189.7051 - KL_loss: 41.5672 - lr: 0.0030\n",
      "Epoch 357/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 258.4277 - reconstruction_loss: 190.9024 - KL_loss: 40.9536 - lr: 0.0030\n",
      "Epoch 358/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 258.9954 - reconstruction_loss: 190.9896 - KL_loss: 41.4071 - lr: 0.0030\n",
      "Epoch 359/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 256.9804 - reconstruction_loss: 188.9605 - KL_loss: 41.6799 - lr: 0.0030\n",
      "Epoch 360/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 258.5750 - reconstruction_loss: 190.4529 - KL_loss: 41.5734 - lr: 0.0030\n",
      "Epoch 361/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 257.5876 - reconstruction_loss: 189.7038 - KL_loss: 41.4614 - lr: 0.0030\n",
      "Epoch 362/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 257.2672 - reconstruction_loss: 189.6576 - KL_loss: 41.2296 - lr: 0.0030\n",
      "Epoch 363/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 257.2188 - reconstruction_loss: 189.6862 - KL_loss: 41.2015 - lr: 0.0030\n",
      "Epoch 364/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 257.4586 - reconstruction_loss: 189.7121 - KL_loss: 41.4218 - lr: 0.0030\n",
      "Epoch 365/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 256.5843 - reconstruction_loss: 188.8502 - KL_loss: 41.5403 - lr: 0.0030\n",
      "Epoch 366/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 256.7564 - reconstruction_loss: 189.2541 - KL_loss: 41.2821 - lr: 0.0030\n",
      "Epoch 367/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 257.1361 - reconstruction_loss: 189.4757 - KL_loss: 41.3740 - lr: 0.0030\n",
      "Epoch 368/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 256.2568 - reconstruction_loss: 188.9390 - KL_loss: 41.1477 - lr: 0.0030\n",
      "Epoch 369/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 256.6040 - reconstruction_loss: 189.2024 - KL_loss: 41.2194 - lr: 0.0030\n",
      "Epoch 370/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 256.5984 - reconstruction_loss: 188.9812 - KL_loss: 41.4418 - lr: 0.0030\n",
      "Epoch 371/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 255.5720 - reconstruction_loss: 188.2341 - KL_loss: 41.2907 - lr: 0.0030\n",
      "Epoch 372/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 256.4413 - reconstruction_loss: 189.0558 - KL_loss: 41.2935 - lr: 0.0030\n",
      "Epoch 373/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 255.8862 - reconstruction_loss: 188.6373 - KL_loss: 41.1954 - lr: 0.0030\n",
      "Epoch 374/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 255.8628 - reconstruction_loss: 188.4164 - KL_loss: 41.4407 - lr: 0.0030\n",
      "Epoch 375/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 255.7876 - reconstruction_loss: 188.2673 - KL_loss: 41.5341 - lr: 0.0030\n",
      "Epoch 376/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 255.5856 - reconstruction_loss: 188.2027 - KL_loss: 41.4085 - lr: 0.0030\n",
      "Epoch 377/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 255.5034 - reconstruction_loss: 188.2757 - KL_loss: 41.2625 - lr: 0.0030\n",
      "Epoch 378/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 255.3761 - reconstruction_loss: 188.1547 - KL_loss: 41.2728 - lr: 0.0030\n",
      "Epoch 379/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 255.4691 - reconstruction_loss: 188.1561 - KL_loss: 41.3812 - lr: 0.0030\n",
      "Epoch 380/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.7602 - reconstruction_loss: 187.2569 - KL_loss: 41.6395 - lr: 0.0030\n",
      "Epoch 381/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 255.8706 - reconstruction_loss: 188.3693 - KL_loss: 41.5585 - lr: 0.0030\n",
      "Epoch 382/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 255.1206 - reconstruction_loss: 187.7380 - KL_loss: 41.5383 - lr: 0.0030\n",
      "Epoch 383/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 255.0905 - reconstruction_loss: 187.7359 - KL_loss: 41.5127 - lr: 0.0030\n",
      "Epoch 384/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 255.4088 - reconstruction_loss: 188.0455 - KL_loss: 41.4989 - lr: 0.0030\n",
      "Epoch 385/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 255.3140 - reconstruction_loss: 188.0496 - KL_loss: 41.4351 - lr: 0.0030\n",
      "Epoch 386/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 254.5815 - reconstruction_loss: 187.4812 - KL_loss: 41.3677 - lr: 0.0030\n",
      "Epoch 387/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 255.4910 - reconstruction_loss: 188.0198 - KL_loss: 41.6653 - lr: 0.0030\n",
      "Epoch 388/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 254.2152 - reconstruction_loss: 186.9295 - KL_loss: 41.5979 - lr: 0.0030\n",
      "Epoch 389/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 255.0965 - reconstruction_loss: 187.7695 - KL_loss: 41.5595 - lr: 0.0030\n",
      "Epoch 390/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.3291 - reconstruction_loss: 187.4026 - KL_loss: 41.2459 - lr: 0.0030\n",
      "Epoch 391/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.8758 - reconstruction_loss: 187.5549 - KL_loss: 41.5849 - lr: 0.0030\n",
      "Epoch 392/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.2551 - reconstruction_loss: 187.0559 - KL_loss: 41.5756 - lr: 0.0030\n",
      "Epoch 393/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 255.0926 - reconstruction_loss: 187.9114 - KL_loss: 41.4230 - lr: 0.0030\n",
      "Epoch 394/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 254.4561 - reconstruction_loss: 187.3604 - KL_loss: 41.4455 - lr: 0.0030\n",
      "Epoch 395/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.7365 - reconstruction_loss: 187.3725 - KL_loss: 41.6856 - lr: 0.0030\n",
      "Epoch 396/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.8184 - reconstruction_loss: 187.1436 - KL_loss: 41.9857 - lr: 0.0030\n",
      "Epoch 397/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 253.9168 - reconstruction_loss: 186.6790 - KL_loss: 41.6978 - lr: 0.0030\n",
      "Epoch 398/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.5997 - reconstruction_loss: 187.6585 - KL_loss: 41.3365 - lr: 0.0030\n",
      "Epoch 399/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.8478 - reconstruction_loss: 187.2233 - KL_loss: 41.1693 - lr: 0.0030\n",
      "Epoch 400/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.8284 - reconstruction_loss: 186.7211 - KL_loss: 41.6130 - lr: 0.0030\n",
      "Epoch 401/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.2589 - reconstruction_loss: 186.8919 - KL_loss: 41.8552 - lr: 0.0030\n",
      "Epoch 402/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.3056 - reconstruction_loss: 186.4925 - KL_loss: 41.4198 - lr: 0.0030\n",
      "Epoch 403/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.1593 - reconstruction_loss: 187.1573 - KL_loss: 41.5248 - lr: 0.0030\n",
      "Epoch 404/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.8654 - reconstruction_loss: 186.9553 - KL_loss: 41.4521 - lr: 0.0030\n",
      "Epoch 405/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.0609 - reconstruction_loss: 186.0004 - KL_loss: 41.7181 - lr: 0.0030\n",
      "Epoch 406/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.8476 - reconstruction_loss: 187.7222 - KL_loss: 41.6189 - lr: 0.0030\n",
      "Epoch 407/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 252.9405 - reconstruction_loss: 186.0647 - KL_loss: 41.5538 - lr: 0.0030\n",
      "Epoch 408/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 256.1591 - reconstruction_loss: 188.6065 - KL_loss: 41.8913 - lr: 0.0030\n",
      "Epoch 409/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 256.9632 - reconstruction_loss: 189.5726 - KL_loss: 41.5270 - lr: 0.0030\n",
      "Epoch 410/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 255.0144 - reconstruction_loss: 188.2805 - KL_loss: 41.1954 - lr: 0.0030\n",
      "Epoch 411/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.6206 - reconstruction_loss: 187.3223 - KL_loss: 41.8007 - lr: 0.0030\n",
      "Epoch 412/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 256.2305 - reconstruction_loss: 188.4243 - KL_loss: 42.1210 - lr: 0.0030\n",
      "Epoch 413/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.5598 - reconstruction_loss: 186.6159 - KL_loss: 41.6236 - lr: 0.0030\n",
      "Epoch 414/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 256.4575 - reconstruction_loss: 189.4714 - KL_loss: 41.3746 - lr: 0.0030\n",
      "Epoch 415/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 257.0290 - reconstruction_loss: 189.9319 - KL_loss: 41.4101 - lr: 0.0030\n",
      "Epoch 416/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.8094 - reconstruction_loss: 187.7089 - KL_loss: 41.6386 - lr: 0.0030\n",
      "Epoch 417/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.5950 - reconstruction_loss: 187.1443 - KL_loss: 42.0234 - lr: 0.0030\n",
      "Epoch 418/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.5071 - reconstruction_loss: 187.3905 - KL_loss: 41.7175 - lr: 0.0030\n",
      "Epoch 419/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.1809 - reconstruction_loss: 187.5599 - KL_loss: 41.2479 - lr: 0.0030\n",
      "Epoch 420/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.3412 - reconstruction_loss: 186.9470 - KL_loss: 41.1654 - lr: 0.0030\n",
      "Epoch 421/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.0445 - reconstruction_loss: 187.3773 - KL_loss: 41.3364 - lr: 0.0030\n",
      "Epoch 422/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.6558 - reconstruction_loss: 186.6533 - KL_loss: 41.7373 - lr: 0.0030\n",
      "Epoch 423/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 252.8417 - reconstruction_loss: 185.6952 - KL_loss: 41.9080 - lr: 0.0030\n",
      "Epoch 424/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 253.1949 - reconstruction_loss: 186.4483 - KL_loss: 41.5858 - lr: 0.0030\n",
      "Epoch 425/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 252.9025 - reconstruction_loss: 186.2850 - KL_loss: 41.4572 - lr: 0.0030\n",
      "Epoch 426/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 252.9336 - reconstruction_loss: 186.3561 - KL_loss: 41.4403 - lr: 0.0030\n",
      "Epoch 427/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 252.5608 - reconstruction_loss: 185.9331 - KL_loss: 41.4838 - lr: 0.0030\n",
      "Epoch 428/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 252.4164 - reconstruction_loss: 185.7044 - KL_loss: 41.6428 - lr: 0.0030\n",
      "Epoch 429/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 252.3525 - reconstruction_loss: 185.7367 - KL_loss: 41.5490 - lr: 0.0030\n",
      "Epoch 430/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 252.1710 - reconstruction_loss: 185.3933 - KL_loss: 41.7399 - lr: 0.0030\n",
      "Epoch 431/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 252.2402 - reconstruction_loss: 185.6010 - KL_loss: 41.6069 - lr: 0.0030\n",
      "Epoch 432/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 251.6005 - reconstruction_loss: 185.3621 - KL_loss: 41.2713 - lr: 0.0030\n",
      "Epoch 433/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 252.4925 - reconstruction_loss: 185.9814 - KL_loss: 41.4971 - lr: 0.0030\n",
      "Epoch 434/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 251.4983 - reconstruction_loss: 185.0191 - KL_loss: 41.5571 - lr: 0.0030\n",
      "Epoch 435/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.2973 - reconstruction_loss: 186.3129 - KL_loss: 41.8584 - lr: 0.0030\n",
      "Epoch 436/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 251.9271 - reconstruction_loss: 184.9657 - KL_loss: 41.9958 - lr: 0.0030\n",
      "Epoch 437/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.1152 - reconstruction_loss: 186.3367 - KL_loss: 41.6689 - lr: 0.0030\n",
      "Epoch 438/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.4730 - reconstruction_loss: 187.0041 - KL_loss: 41.3450 - lr: 0.0030\n",
      "Epoch 439/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 251.7366 - reconstruction_loss: 185.4626 - KL_loss: 41.3842 - lr: 0.0030\n",
      "Epoch 440/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.5112 - reconstruction_loss: 186.5173 - KL_loss: 41.8599 - lr: 0.0030\n",
      "Epoch 441/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 252.4275 - reconstruction_loss: 185.5129 - KL_loss: 41.9353 - lr: 0.0030\n",
      "Epoch 442/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.8280 - reconstruction_loss: 186.9300 - KL_loss: 41.7857 - lr: 0.0030\n",
      "Epoch 443/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.0424 - reconstruction_loss: 186.4035 - KL_loss: 41.6090 - lr: 0.0030\n",
      "Epoch 444/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.5311 - reconstruction_loss: 186.7517 - KL_loss: 41.7114 - lr: 0.0030\n",
      "Epoch 445/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.2195 - reconstruction_loss: 186.4390 - KL_loss: 41.7653 - lr: 0.0030\n",
      "Epoch 446/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.5062 - reconstruction_loss: 186.7724 - KL_loss: 41.6654 - lr: 0.0030\n",
      "Epoch 447/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 252.3259 - reconstruction_loss: 185.7973 - KL_loss: 41.6227 - lr: 0.0030\n",
      "Epoch 448/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 254.4333 - reconstruction_loss: 187.4608 - KL_loss: 41.8147 - lr: 0.0030\n",
      "Epoch 449/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.2830 - reconstruction_loss: 186.5772 - KL_loss: 41.6638 - lr: 0.0030\n",
      "Epoch 450/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 252.5480 - reconstruction_loss: 186.2305 - KL_loss: 41.3846 - lr: 0.0030\n",
      "Epoch 451/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 253.3917 - reconstruction_loss: 186.7218 - KL_loss: 41.6740 - lr: 0.0030\n",
      "Epoch 452/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 251.6763 - reconstruction_loss: 185.0122 - KL_loss: 41.8214 - lr: 0.0030\n",
      "Epoch 453/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 252.2947 - reconstruction_loss: 185.7415 - KL_loss: 41.7000 - lr: 0.0030\n",
      "Epoch 454/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 251.7334 - reconstruction_loss: 185.2778 - KL_loss: 41.6414 - lr: 0.0030\n",
      "Epoch 455/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 252.0853 - reconstruction_loss: 185.6943 - KL_loss: 41.5391 - lr: 0.0030\n",
      "Epoch 456/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 251.3821 - reconstruction_loss: 185.0459 - KL_loss: 41.5616 - lr: 0.0030\n",
      "Epoch 457/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 251.8472 - reconstruction_loss: 185.4486 - KL_loss: 41.6111 - lr: 0.0030\n",
      "Epoch 458/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 251.6667 - reconstruction_loss: 185.3784 - KL_loss: 41.5088 - lr: 0.0030\n",
      "Epoch 459/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 251.2743 - reconstruction_loss: 184.9340 - KL_loss: 41.6379 - lr: 0.0030\n",
      "Epoch 460/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 251.2377 - reconstruction_loss: 184.7437 - KL_loss: 41.8259 - lr: 0.0030\n",
      "Epoch 461/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 250.9731 - reconstruction_loss: 184.7451 - KL_loss: 41.5604 - lr: 0.0030\n",
      "Epoch 462/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 251.6059 - reconstruction_loss: 185.1631 - KL_loss: 41.7500 - lr: 0.0030\n",
      "Epoch 463/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 250.5874 - reconstruction_loss: 184.2949 - KL_loss: 41.6926 - lr: 0.0030\n",
      "Epoch 464/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 251.1964 - reconstruction_loss: 184.9656 - KL_loss: 41.6426 - lr: 0.0030\n",
      "Epoch 465/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 251.0186 - reconstruction_loss: 184.6956 - KL_loss: 41.7212 - lr: 0.0030\n",
      "Epoch 466/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 250.1167 - reconstruction_loss: 183.9234 - KL_loss: 41.7132 - lr: 0.0030\n",
      "Epoch 467/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 250.8382 - reconstruction_loss: 184.6641 - KL_loss: 41.5694 - lr: 0.0030\n",
      "Epoch 468/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 250.4833 - reconstruction_loss: 184.4270 - KL_loss: 41.5533 - lr: 0.0030\n",
      "Epoch 469/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 250.3291 - reconstruction_loss: 184.0689 - KL_loss: 41.7508 - lr: 0.0030\n",
      "Epoch 470/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.9627 - reconstruction_loss: 183.6656 - KL_loss: 41.8434 - lr: 0.0030\n",
      "Epoch 471/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 250.9021 - reconstruction_loss: 184.8181 - KL_loss: 41.4774 - lr: 0.0030\n",
      "Epoch 472/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.7821 - reconstruction_loss: 183.7016 - KL_loss: 41.6406 - lr: 0.0030\n",
      "Epoch 473/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 250.7853 - reconstruction_loss: 184.3663 - KL_loss: 41.8937 - lr: 0.0030\n",
      "Epoch 474/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 250.1026 - reconstruction_loss: 183.9315 - KL_loss: 41.7029 - lr: 0.0030\n",
      "Epoch 475/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 250.1552 - reconstruction_loss: 184.3749 - KL_loss: 41.3217 - lr: 0.0030\n",
      "Epoch 476/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 250.7175 - reconstruction_loss: 184.5031 - KL_loss: 41.6376 - lr: 0.0030\n",
      "Epoch 477/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 250.2164 - reconstruction_loss: 183.5608 - KL_loss: 42.2207 - lr: 0.0030\n",
      "Epoch 478/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 250.2523 - reconstruction_loss: 183.5624 - KL_loss: 42.2788 - lr: 0.0030\n",
      "Epoch 479/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.6903 - reconstruction_loss: 183.6292 - KL_loss: 41.7316 - lr: 0.0030\n",
      "Epoch 480/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 250.0797 - reconstruction_loss: 184.1371 - KL_loss: 41.5210 - lr: 0.0030\n",
      "Epoch 481/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.9749 - reconstruction_loss: 184.2314 - KL_loss: 41.3131 - lr: 0.0030\n",
      "Epoch 482/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.6395 - reconstruction_loss: 183.7571 - KL_loss: 41.5579 - lr: 0.0030\n",
      "Epoch 483/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.7377 - reconstruction_loss: 183.2113 - KL_loss: 42.1968 - lr: 0.0030\n",
      "Epoch 484/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.4442 - reconstruction_loss: 182.7620 - KL_loss: 42.4167 - lr: 0.0030\n",
      "Epoch 485/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.5779 - reconstruction_loss: 183.3597 - KL_loss: 41.9450 - lr: 0.0030\n",
      "Epoch 486/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.2367 - reconstruction_loss: 183.4445 - KL_loss: 41.5586 - lr: 0.0030\n",
      "Epoch 487/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.2935 - reconstruction_loss: 183.4897 - KL_loss: 41.5609 - lr: 0.0030\n",
      "Epoch 488/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.3640 - reconstruction_loss: 183.3709 - KL_loss: 41.7689 - lr: 0.0030\n",
      "Epoch 489/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.8359 - reconstruction_loss: 182.6889 - KL_loss: 41.9804 - lr: 0.0030\n",
      "Epoch 490/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.0159 - reconstruction_loss: 182.9291 - KL_loss: 41.9225 - lr: 0.0030\n",
      "Epoch 491/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.0163 - reconstruction_loss: 182.9456 - KL_loss: 41.8893 - lr: 0.0030\n",
      "Epoch 492/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.8729 - reconstruction_loss: 183.1677 - KL_loss: 41.5524 - lr: 0.0030\n",
      "Epoch 493/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.0334 - reconstruction_loss: 183.1876 - KL_loss: 41.6952 - lr: 0.0030\n",
      "Epoch 494/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.5381 - reconstruction_loss: 182.4112 - KL_loss: 42.0340 - lr: 0.0030\n",
      "Epoch 495/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.3273 - reconstruction_loss: 183.1074 - KL_loss: 42.0658 - lr: 0.0030\n",
      "Epoch 496/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.3127 - reconstruction_loss: 182.2427 - KL_loss: 42.0231 - lr: 0.0030\n",
      "Epoch 497/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.8097 - reconstruction_loss: 183.9773 - KL_loss: 41.7013 - lr: 0.0030\n",
      "Epoch 498/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.7003 - reconstruction_loss: 182.9963 - KL_loss: 41.6424 - lr: 0.0030\n",
      "Epoch 499/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.3736 - reconstruction_loss: 183.4288 - KL_loss: 41.7881 - lr: 0.0030\n",
      "Epoch 500/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.1220 - reconstruction_loss: 183.0987 - KL_loss: 41.8918 - lr: 0.0030\n",
      "Epoch 501/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.5914 - reconstruction_loss: 182.4610 - KL_loss: 42.0589 - lr: 0.0030\n",
      "Epoch 502/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.9255 - reconstruction_loss: 183.1319 - KL_loss: 41.7301 - lr: 0.0030\n",
      "Epoch 503/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.6656 - reconstruction_loss: 182.7119 - KL_loss: 41.9347 - lr: 0.0030\n",
      "Epoch 504/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.5642 - reconstruction_loss: 182.6403 - KL_loss: 41.9102 - lr: 0.0030\n",
      "Epoch 505/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.6721 - reconstruction_loss: 182.8227 - KL_loss: 41.7900 - lr: 0.0030\n",
      "Epoch 506/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.6224 - reconstruction_loss: 182.7283 - KL_loss: 41.8986 - lr: 0.0030\n",
      "Epoch 507/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.2426 - reconstruction_loss: 182.4674 - KL_loss: 41.8427 - lr: 0.0030\n",
      "Epoch 508/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.0333 - reconstruction_loss: 182.1133 - KL_loss: 41.9998 - lr: 0.0030\n",
      "Epoch 509/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.5976 - reconstruction_loss: 182.8026 - KL_loss: 41.8398 - lr: 0.0030\n",
      "Epoch 510/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.9370 - reconstruction_loss: 182.3016 - KL_loss: 41.7449 - lr: 0.0030\n",
      "Epoch 511/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.0233 - reconstruction_loss: 183.2046 - KL_loss: 41.7635 - lr: 0.0030\n",
      "Epoch 512/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.9843 - reconstruction_loss: 182.2051 - KL_loss: 41.8726 - lr: 0.0030\n",
      "Epoch 513/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.3935 - reconstruction_loss: 182.3091 - KL_loss: 42.1677 - lr: 0.0030\n",
      "Epoch 514/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.1027 - reconstruction_loss: 182.2087 - KL_loss: 42.0099 - lr: 0.0030\n",
      "Epoch 515/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.7227 - reconstruction_loss: 182.9731 - KL_loss: 41.8344 - lr: 0.0030\n",
      "Epoch 516/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.7988 - reconstruction_loss: 181.9928 - KL_loss: 41.9017 - lr: 0.0030\n",
      "Epoch 517/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.6767 - reconstruction_loss: 183.4249 - KL_loss: 42.0684 - lr: 0.0030\n",
      "Epoch 518/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.3252 - reconstruction_loss: 181.9921 - KL_loss: 42.4439 - lr: 0.0030\n",
      "Epoch 519/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.3887 - reconstruction_loss: 183.0812 - KL_loss: 42.1958 - lr: 0.0030\n",
      "Epoch 520/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.2711 - reconstruction_loss: 183.6692 - KL_loss: 41.5544 - lr: 0.0030\n",
      "Epoch 521/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.7149 - reconstruction_loss: 183.0123 - KL_loss: 41.7665 - lr: 0.0030\n",
      "Epoch 522/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.2820 - reconstruction_loss: 182.0674 - KL_loss: 42.3421 - lr: 0.0030\n",
      "Epoch 523/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.9998 - reconstruction_loss: 182.8154 - KL_loss: 42.2123 - lr: 0.0030\n",
      "Epoch 524/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.1345 - reconstruction_loss: 182.5340 - KL_loss: 41.7401 - lr: 0.0030\n",
      "Epoch 525/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.9152 - reconstruction_loss: 183.2837 - KL_loss: 41.7250 - lr: 0.0030\n",
      "Epoch 526/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.3607 - reconstruction_loss: 182.5195 - KL_loss: 41.9579 - lr: 0.0030\n",
      "Epoch 527/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.7180 - reconstruction_loss: 182.1011 - KL_loss: 41.8771 - lr: 0.0030\n",
      "Epoch 528/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.2676 - reconstruction_loss: 182.7510 - KL_loss: 41.6811 - lr: 0.0030\n",
      "Epoch 529/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.6655 - reconstruction_loss: 182.2420 - KL_loss: 41.6924 - lr: 0.0030\n",
      "Epoch 530/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.9455 - reconstruction_loss: 182.1877 - KL_loss: 41.9685 - lr: 0.0030\n",
      "Epoch 531/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 247.3087 - reconstruction_loss: 181.5867 - KL_loss: 42.0418 - lr: 0.0030\n",
      "Epoch 532/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 247.8636 - reconstruction_loss: 182.2735 - KL_loss: 41.8588 - lr: 0.0030\n",
      "Epoch 533/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.5984 - reconstruction_loss: 182.2065 - KL_loss: 41.7027 - lr: 0.0030\n",
      "Epoch 534/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.2404 - reconstruction_loss: 181.6515 - KL_loss: 41.9568 - lr: 0.0030\n",
      "Epoch 535/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.1482 - reconstruction_loss: 182.1485 - KL_loss: 42.2144 - lr: 0.0030\n",
      "Epoch 536/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.2220 - reconstruction_loss: 181.4964 - KL_loss: 42.0583 - lr: 0.0030\n",
      "Epoch 537/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.5962 - reconstruction_loss: 182.1825 - KL_loss: 41.7562 - lr: 0.0030\n",
      "Epoch 538/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.8414 - reconstruction_loss: 182.5726 - KL_loss: 41.6126 - lr: 0.0030\n",
      "Epoch 539/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 246.8680 - reconstruction_loss: 181.4499 - KL_loss: 41.8179 - lr: 0.0030\n",
      "Epoch 540/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 247.5366 - reconstruction_loss: 181.6163 - KL_loss: 42.2404 - lr: 0.0030\n",
      "Epoch 541/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.8598 - reconstruction_loss: 181.3043 - KL_loss: 42.0140 - lr: 0.0030\n",
      "Epoch 542/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.1489 - reconstruction_loss: 181.6175 - KL_loss: 41.9552 - lr: 0.0030\n",
      "Epoch 543/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 247.4515 - reconstruction_loss: 182.1561 - KL_loss: 41.6960 - lr: 0.0030\n",
      "Epoch 544/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 246.5558 - reconstruction_loss: 181.0901 - KL_loss: 41.9774 - lr: 0.0030\n",
      "Epoch 545/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.4945 - reconstruction_loss: 182.6936 - KL_loss: 42.1013 - lr: 0.0030\n",
      "Epoch 546/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.5626 - reconstruction_loss: 181.0420 - KL_loss: 42.0053 - lr: 0.0030\n",
      "Epoch 547/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.3171 - reconstruction_loss: 182.6486 - KL_loss: 41.9772 - lr: 0.0030\n",
      "Epoch 548/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.2375 - reconstruction_loss: 182.8097 - KL_loss: 41.7113 - lr: 0.0030\n",
      "Epoch 549/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.3761 - reconstruction_loss: 182.7346 - KL_loss: 41.9320 - lr: 0.0030\n",
      "Epoch 550/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.3759 - reconstruction_loss: 181.5476 - KL_loss: 42.2234 - lr: 0.0030\n",
      "Epoch 551/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.4403 - reconstruction_loss: 183.2246 - KL_loss: 42.3429 - lr: 0.0030\n",
      "Epoch 552/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.6483 - reconstruction_loss: 182.1004 - KL_loss: 41.9346 - lr: 0.0030\n",
      "Epoch 553/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.7291 - reconstruction_loss: 184.0791 - KL_loss: 41.8283 - lr: 0.0030\n",
      "Epoch 554/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 250.0917 - reconstruction_loss: 184.4040 - KL_loss: 41.7741 - lr: 0.0030\n",
      "Epoch 555/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.5907 - reconstruction_loss: 181.3262 - KL_loss: 41.7932 - lr: 0.0030\n",
      "Epoch 556/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.8476 - reconstruction_loss: 183.6132 - KL_loss: 42.4123 - lr: 0.0030\n",
      "Epoch 557/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.6527 - reconstruction_loss: 182.4810 - KL_loss: 42.4514 - lr: 0.0030\n",
      "Epoch 558/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.4818 - reconstruction_loss: 182.8147 - KL_loss: 41.9769 - lr: 0.0030\n",
      "Epoch 559/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.4902 - reconstruction_loss: 183.6990 - KL_loss: 41.9262 - lr: 0.0030\n",
      "Epoch 560/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 247.5999 - reconstruction_loss: 182.3749 - KL_loss: 41.6262 - lr: 0.0030\n",
      "Epoch 561/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.8846 - reconstruction_loss: 183.4227 - KL_loss: 41.7598 - lr: 0.0030\n",
      "Epoch 562/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.0110 - reconstruction_loss: 182.2234 - KL_loss: 42.1283 - lr: 0.0030\n",
      "Epoch 563/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 248.2154 - reconstruction_loss: 182.1992 - KL_loss: 42.4016 - lr: 0.0030\n",
      "Epoch 564/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.4284 - reconstruction_loss: 182.3576 - KL_loss: 42.3203 - lr: 0.0030\n",
      "Epoch 565/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.8320 - reconstruction_loss: 181.6613 - KL_loss: 41.7098 - lr: 0.0030\n",
      "Epoch 566/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.5574 - reconstruction_loss: 183.2619 - KL_loss: 41.5927 - lr: 0.0030\n",
      "Epoch 567/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.2402 - reconstruction_loss: 182.1704 - KL_loss: 41.5489 - lr: 0.0030\n",
      "Epoch 568/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.8109 - reconstruction_loss: 183.1593 - KL_loss: 41.9364 - lr: 0.0030\n",
      "Epoch 569/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.9177 - reconstruction_loss: 182.6436 - KL_loss: 42.4869 - lr: 0.0030\n",
      "Epoch 570/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.4489 - reconstruction_loss: 182.3550 - KL_loss: 42.4371 - lr: 0.0030\n",
      "Epoch 571/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.2067 - reconstruction_loss: 181.7864 - KL_loss: 41.8875 - lr: 0.0030\n",
      "Epoch 572/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.8486 - reconstruction_loss: 184.5325 - KL_loss: 41.4537 - lr: 0.0030\n",
      "Epoch 573/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.8923 - reconstruction_loss: 182.8652 - KL_loss: 41.4513 - lr: 0.0030\n",
      "Epoch 574/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 249.3720 - reconstruction_loss: 183.4743 - KL_loss: 42.1388 - lr: 0.0030\n",
      "Epoch 575/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 249.2014 - reconstruction_loss: 183.0581 - KL_loss: 42.4090 - lr: 0.0030\n",
      "Epoch 576/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 247.0897 - reconstruction_loss: 181.6365 - KL_loss: 42.0050 - lr: 0.0030\n",
      "Epoch 577/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.5372 - reconstruction_loss: 182.2974 - KL_loss: 41.7678 - lr: 0.0030\n",
      "Epoch 578/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.4518 - reconstruction_loss: 182.2363 - KL_loss: 41.7422 - lr: 0.0030\n",
      "Epoch 579/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.3015 - reconstruction_loss: 181.9154 - KL_loss: 41.8835 - lr: 0.0030\n",
      "Epoch 580/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.3783 - reconstruction_loss: 181.2978 - KL_loss: 41.7518 - lr: 0.0030\n",
      "Epoch 581/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.1144 - reconstruction_loss: 182.5060 - KL_loss: 42.0789 - lr: 0.0030\n",
      "Epoch 582/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.0173 - reconstruction_loss: 181.4893 - KL_loss: 42.1158 - lr: 0.0030\n",
      "Epoch 583/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.3921 - reconstruction_loss: 181.8666 - KL_loss: 42.1019 - lr: 0.0030\n",
      "Epoch 584/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.5150 - reconstruction_loss: 182.1302 - KL_loss: 41.9115 - lr: 0.0030\n",
      "Epoch 585/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.3493 - reconstruction_loss: 181.4090 - KL_loss: 41.6260 - lr: 0.0030\n",
      "Epoch 586/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.7336 - reconstruction_loss: 181.5099 - KL_loss: 41.8070 - lr: 0.0030\n",
      "Epoch 587/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.6213 - reconstruction_loss: 181.0068 - KL_loss: 42.2716 - lr: 0.0030\n",
      "Epoch 588/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 246.6784 - reconstruction_loss: 180.9881 - KL_loss: 42.3309 - lr: 0.0030\n",
      "Epoch 589/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 245.5797 - reconstruction_loss: 180.5968 - KL_loss: 41.7250 - lr: 0.0030\n",
      "Epoch 590/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.3818 - reconstruction_loss: 182.5037 - KL_loss: 41.4305 - lr: 0.0030\n",
      "Epoch 591/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.3111 - reconstruction_loss: 181.2709 - KL_loss: 41.7301 - lr: 0.0030\n",
      "Epoch 592/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.5384 - reconstruction_loss: 180.8495 - KL_loss: 42.3711 - lr: 0.0030\n",
      "Epoch 593/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 247.4536 - reconstruction_loss: 181.6112 - KL_loss: 42.4725 - lr: 0.0030\n",
      "Epoch 594/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 245.8916 - reconstruction_loss: 180.7839 - KL_loss: 41.8877 - lr: 0.0030\n",
      "Epoch 595/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 247.1317 - reconstruction_loss: 182.1271 - KL_loss: 41.6360 - lr: 0.0030\n",
      "Epoch 596/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.3119 - reconstruction_loss: 182.1491 - KL_loss: 41.7554 - lr: 0.0030\n",
      "Epoch 597/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 246.0687 - reconstruction_loss: 180.9153 - KL_loss: 41.9164 - lr: 0.0030\n",
      "Epoch 598/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.8141 - reconstruction_loss: 181.2753 - KL_loss: 42.2769 - lr: 0.0030\n",
      "Epoch 599/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 246.1275 - reconstruction_loss: 180.7006 - KL_loss: 42.1576 - lr: 0.0030\n",
      "Epoch 600/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 246.9388 - reconstruction_loss: 181.9570 - KL_loss: 41.7044 - lr: 0.0030\n",
      "Epoch 601/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.2874 - reconstruction_loss: 181.4735 - KL_loss: 41.5427 - lr: 0.0030\n",
      "Epoch 602/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.1010 - reconstruction_loss: 181.6242 - KL_loss: 42.1823 - lr: 0.0030\n",
      "Epoch 603/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.7285 - reconstruction_loss: 181.0596 - KL_loss: 42.3726 - lr: 0.0030\n",
      "Epoch 604/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 245.9654 - reconstruction_loss: 180.9057 - KL_loss: 41.8455 - lr: 0.0030\n",
      "Epoch 605/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.2293 - reconstruction_loss: 181.2274 - KL_loss: 41.7804 - lr: 0.0030\n",
      "Epoch 606/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.4973 - reconstruction_loss: 181.1966 - KL_loss: 41.9783 - lr: 0.0030\n",
      "Epoch 607/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.9104 - reconstruction_loss: 179.8630 - KL_loss: 41.9982 - lr: 0.0030\n",
      "Epoch 608/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.5018 - reconstruction_loss: 181.9233 - KL_loss: 42.1041 - lr: 0.0030\n",
      "Epoch 609/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 245.7655 - reconstruction_loss: 180.6559 - KL_loss: 41.9143 - lr: 0.0030\n",
      "Epoch 610/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 248.3281 - reconstruction_loss: 182.8257 - KL_loss: 42.0178 - lr: 0.0030\n",
      "Epoch 611/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.9847 - reconstruction_loss: 182.5105 - KL_loss: 42.0307 - lr: 0.0030\n",
      "Epoch 612/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 247.6949 - reconstruction_loss: 182.3518 - KL_loss: 41.9274 - lr: 0.0030\n",
      "Epoch 613/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 246.9381 - reconstruction_loss: 181.4211 - KL_loss: 42.1510 - lr: 0.0030\n",
      "Epoch 614/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 247.2914 - reconstruction_loss: 181.5740 - KL_loss: 42.3956 - lr: 0.0030\n",
      "Epoch 615/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 247.1632 - reconstruction_loss: 181.7179 - KL_loss: 42.0906 - lr: 0.0030\n",
      "Epoch 616/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.6588 - reconstruction_loss: 181.8514 - KL_loss: 41.6038 - lr: 0.0030\n",
      "Epoch 617/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 246.5943 - reconstruction_loss: 181.4713 - KL_loss: 41.7742 - lr: 0.0030\n",
      "Epoch 618/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.2275 - reconstruction_loss: 180.8768 - KL_loss: 42.1934 - lr: 0.0030\n",
      "Epoch 619/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 246.3603 - reconstruction_loss: 180.9621 - KL_loss: 42.1253 - lr: 0.0030\n",
      "Epoch 620/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 245.5798 - reconstruction_loss: 180.6823 - KL_loss: 41.7709 - lr: 0.0030\n",
      "Epoch 621/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 245.8235 - reconstruction_loss: 181.1147 - KL_loss: 41.5510 - lr: 0.0030\n",
      "Epoch 622/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 245.7744 - reconstruction_loss: 180.9128 - KL_loss: 41.7883 - lr: 0.0030\n",
      "Epoch 623/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 245.4231 - reconstruction_loss: 180.1538 - KL_loss: 42.1738 - lr: 0.0030\n",
      "Epoch 624/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 245.7544 - reconstruction_loss: 180.4937 - KL_loss: 42.1591 - lr: 0.0030\n",
      "Epoch 625/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 244.8579 - reconstruction_loss: 180.0398 - KL_loss: 41.8756 - lr: 0.0030\n",
      "Epoch 626/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 245.1411 - reconstruction_loss: 180.5228 - KL_loss: 41.6146 - lr: 0.0030\n",
      "Epoch 627/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 245.0295 - reconstruction_loss: 180.0321 - KL_loss: 41.9859 - lr: 0.0030\n",
      "Epoch 628/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 245.2848 - reconstruction_loss: 180.2578 - KL_loss: 42.0501 - lr: 0.0030\n",
      "Epoch 629/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 244.6692 - reconstruction_loss: 179.6628 - KL_loss: 42.1024 - lr: 0.0030\n",
      "Epoch 630/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 244.8196 - reconstruction_loss: 179.7408 - KL_loss: 42.0964 - lr: 0.0030\n",
      "Epoch 631/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 244.7547 - reconstruction_loss: 179.7642 - KL_loss: 42.0017 - lr: 0.0030\n",
      "Epoch 632/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 244.6598 - reconstruction_loss: 179.8888 - KL_loss: 41.8721 - lr: 0.0030\n",
      "Epoch 633/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.2461 - reconstruction_loss: 179.5069 - KL_loss: 41.8688 - lr: 0.0030\n",
      "Epoch 634/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.5869 - reconstruction_loss: 179.5633 - KL_loss: 42.0954 - lr: 0.0030\n",
      "Epoch 635/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.6165 - reconstruction_loss: 179.5076 - KL_loss: 42.1591 - lr: 0.0030\n",
      "Epoch 636/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.9639 - reconstruction_loss: 179.2349 - KL_loss: 41.9079 - lr: 0.0030\n",
      "Epoch 637/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.4564 - reconstruction_loss: 179.7127 - KL_loss: 41.8673 - lr: 0.0030\n",
      "Epoch 638/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.9238 - reconstruction_loss: 179.1081 - KL_loss: 41.9908 - lr: 0.0030\n",
      "Epoch 639/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 244.3983 - reconstruction_loss: 179.4862 - KL_loss: 42.0704 - lr: 0.0030\n",
      "Epoch 640/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.0000 - reconstruction_loss: 178.9247 - KL_loss: 42.2765 - lr: 0.0030\n",
      "Epoch 641/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.8754 - reconstruction_loss: 179.0901 - KL_loss: 41.9848 - lr: 0.0030\n",
      "Epoch 642/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 244.2543 - reconstruction_loss: 179.7897 - KL_loss: 41.6677 - lr: 0.0030\n",
      "Epoch 643/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.4297 - reconstruction_loss: 178.8263 - KL_loss: 41.9049 - lr: 0.0030\n",
      "Epoch 644/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.4018 - reconstruction_loss: 179.4900 - KL_loss: 42.1229 - lr: 0.0030\n",
      "Epoch 645/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.8557 - reconstruction_loss: 178.7308 - KL_loss: 42.3519 - lr: 0.0030\n",
      "Epoch 646/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.0186 - reconstruction_loss: 179.1278 - KL_loss: 42.1063 - lr: 0.0030\n",
      "Epoch 647/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.1504 - reconstruction_loss: 179.4268 - KL_loss: 41.8800 - lr: 0.0030\n",
      "Epoch 648/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.7302 - reconstruction_loss: 178.8816 - KL_loss: 42.1104 - lr: 0.0030\n",
      "Epoch 649/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 244.2168 - reconstruction_loss: 179.2368 - KL_loss: 42.1692 - lr: 0.0030\n",
      "Epoch 650/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.4275 - reconstruction_loss: 178.5823 - KL_loss: 42.1722 - lr: 0.0030\n",
      "Epoch 651/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.9954 - reconstruction_loss: 179.1544 - KL_loss: 42.0803 - lr: 0.0030\n",
      "Epoch 652/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.0770 - reconstruction_loss: 179.3949 - KL_loss: 41.9589 - lr: 0.0030\n",
      "Epoch 653/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.3878 - reconstruction_loss: 178.6695 - KL_loss: 42.0451 - lr: 0.0030\n",
      "Epoch 654/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.9700 - reconstruction_loss: 178.9683 - KL_loss: 42.2727 - lr: 0.0030\n",
      "Epoch 655/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.5637 - reconstruction_loss: 178.7552 - KL_loss: 42.1112 - lr: 0.0030\n",
      "Epoch 656/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.4034 - reconstruction_loss: 178.7257 - KL_loss: 42.0113 - lr: 0.0030\n",
      "Epoch 657/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.6291 - reconstruction_loss: 178.9419 - KL_loss: 41.9979 - lr: 0.0030\n",
      "Epoch 658/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.4860 - reconstruction_loss: 178.7348 - KL_loss: 42.1252 - lr: 0.0030\n",
      "Epoch 659/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.3271 - reconstruction_loss: 178.3871 - KL_loss: 42.3271 - lr: 0.0030\n",
      "Epoch 660/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.4275 - reconstruction_loss: 178.5865 - KL_loss: 42.1909 - lr: 0.0030\n",
      "Epoch 661/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.2930 - reconstruction_loss: 178.5692 - KL_loss: 42.1048 - lr: 0.0030\n",
      "Epoch 662/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.2801 - reconstruction_loss: 178.8100 - KL_loss: 41.8354 - lr: 0.0030\n",
      "Epoch 663/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.0999 - reconstruction_loss: 178.6979 - KL_loss: 41.8631 - lr: 0.0030\n",
      "Epoch 664/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.5642 - reconstruction_loss: 178.6813 - KL_loss: 42.2921 - lr: 0.0030\n",
      "Epoch 665/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.0228 - reconstruction_loss: 178.1691 - KL_loss: 42.2940 - lr: 0.0030\n",
      "Epoch 666/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.6383 - reconstruction_loss: 178.6840 - KL_loss: 42.3980 - lr: 0.0030\n",
      "Epoch 667/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.5898 - reconstruction_loss: 178.0780 - KL_loss: 42.0182 - lr: 0.0030\n",
      "Epoch 668/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.6431 - reconstruction_loss: 180.2069 - KL_loss: 41.7436 - lr: 0.0030\n",
      "Epoch 669/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.7057 - reconstruction_loss: 178.2078 - KL_loss: 42.0136 - lr: 0.0030\n",
      "Epoch 670/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 245.0994 - reconstruction_loss: 179.7204 - KL_loss: 42.6587 - lr: 0.0030\n",
      "Epoch 671/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.9467 - reconstruction_loss: 178.8552 - KL_loss: 42.4961 - lr: 0.0030\n",
      "Epoch 672/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 245.3453 - reconstruction_loss: 180.4339 - KL_loss: 42.0436 - lr: 0.0030\n",
      "Epoch 673/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 245.4550 - reconstruction_loss: 180.7018 - KL_loss: 41.8939 - lr: 0.0030\n",
      "Epoch 674/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.1220 - reconstruction_loss: 179.2953 - KL_loss: 42.1132 - lr: 0.0030\n",
      "Epoch 675/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.8298 - reconstruction_loss: 179.4325 - KL_loss: 42.6374 - lr: 0.0030\n",
      "Epoch 676/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.0726 - reconstruction_loss: 178.8850 - KL_loss: 42.5356 - lr: 0.0030\n",
      "Epoch 677/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.0568 - reconstruction_loss: 179.5565 - KL_loss: 41.8406 - lr: 0.0030\n",
      "Epoch 678/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.0940 - reconstruction_loss: 179.6137 - KL_loss: 41.8081 - lr: 0.0030\n",
      "Epoch 679/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.6012 - reconstruction_loss: 178.7657 - KL_loss: 42.2146 - lr: 0.0030\n",
      "Epoch 680/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.5737 - reconstruction_loss: 178.6326 - KL_loss: 42.3597 - lr: 0.0030\n",
      "Epoch 681/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 244.1422 - reconstruction_loss: 179.1898 - KL_loss: 42.3467 - lr: 0.0030\n",
      "Epoch 682/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.9335 - reconstruction_loss: 178.4934 - KL_loss: 41.9391 - lr: 0.0030\n",
      "Epoch 683/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.2702 - reconstruction_loss: 179.7748 - KL_loss: 41.8915 - lr: 0.0030\n",
      "Epoch 684/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.9620 - reconstruction_loss: 178.9963 - KL_loss: 42.2775 - lr: 0.0030\n",
      "Epoch 685/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.2723 - reconstruction_loss: 178.4656 - KL_loss: 42.2992 - lr: 0.0030\n",
      "Epoch 686/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.8486 - reconstruction_loss: 179.0445 - KL_loss: 42.1828 - lr: 0.0030\n",
      "Epoch 687/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.3014 - reconstruction_loss: 178.8995 - KL_loss: 41.9045 - lr: 0.0030\n",
      "Epoch 688/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.9612 - reconstruction_loss: 178.5369 - KL_loss: 41.9813 - lr: 0.0030\n",
      "Epoch 689/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.8919 - reconstruction_loss: 178.8318 - KL_loss: 42.4445 - lr: 0.0030\n",
      "Epoch 690/1000\n",
      "12416/12416 [==============================] - 1s 61us/sample - loss: 242.4555 - reconstruction_loss: 177.6324 - KL_loss: 42.4086 - lr: 0.0030\n",
      "Epoch 691/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.6653 - reconstruction_loss: 179.1566 - KL_loss: 41.9508 - lr: 0.0030\n",
      "Epoch 692/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.9222 - reconstruction_loss: 179.3029 - KL_loss: 42.0377 - lr: 0.0030\n",
      "Epoch 693/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.1903 - reconstruction_loss: 177.6426 - KL_loss: 42.1881 - lr: 0.0030\n",
      "Epoch 694/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 245.0866 - reconstruction_loss: 179.9219 - KL_loss: 42.4058 - lr: 0.0030\n",
      "Epoch 695/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 244.0852 - reconstruction_loss: 178.8485 - KL_loss: 42.6640 - lr: 0.0030\n",
      "Epoch 696/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 244.7461 - reconstruction_loss: 179.7649 - KL_loss: 42.3406 - lr: 0.0030\n",
      "Epoch 697/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.0208 - reconstruction_loss: 179.5919 - KL_loss: 41.8434 - lr: 0.0030\n",
      "Epoch 698/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 245.3160 - reconstruction_loss: 180.7395 - KL_loss: 41.9294 - lr: 0.0030\n",
      "Epoch 699/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.9512 - reconstruction_loss: 179.0629 - KL_loss: 42.3254 - lr: 0.0030\n",
      "Epoch 700/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 244.4916 - reconstruction_loss: 179.3208 - KL_loss: 42.5947 - lr: 0.0030\n",
      "Epoch 701/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 244.4889 - reconstruction_loss: 179.5087 - KL_loss: 42.3618 - lr: 0.0030\n",
      "Epoch 702/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.4578 - reconstruction_loss: 179.3057 - KL_loss: 41.6595 - lr: 0.0030\n",
      "Epoch 703/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.7087 - reconstruction_loss: 179.6156 - KL_loss: 41.6074 - lr: 0.0030\n",
      "Epoch 704/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.4432 - reconstruction_loss: 178.9682 - KL_loss: 41.9926 - lr: 0.0030\n",
      "Epoch 705/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.9148 - reconstruction_loss: 178.1466 - KL_loss: 42.3598 - lr: 0.0030\n",
      "Epoch 706/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.1866 - reconstruction_loss: 178.2991 - KL_loss: 42.4242 - lr: 0.0030\n",
      "Epoch 707/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.9191 - reconstruction_loss: 178.3474 - KL_loss: 42.1228 - lr: 0.0030\n",
      "Epoch 708/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.9189 - reconstruction_loss: 178.9412 - KL_loss: 41.5883 - lr: 0.0030\n",
      "Epoch 709/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.4335 - reconstruction_loss: 178.3636 - KL_loss: 41.6826 - lr: 0.0030\n",
      "Epoch 710/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.1367 - reconstruction_loss: 178.3967 - KL_loss: 42.2883 - lr: 0.0030\n",
      "Epoch 711/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.9427 - reconstruction_loss: 178.1018 - KL_loss: 42.4431 - lr: 0.0030\n",
      "Epoch 712/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.3390 - reconstruction_loss: 177.7379 - KL_loss: 42.2451 - lr: 0.0030\n",
      "Epoch 713/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.1904 - reconstruction_loss: 178.6902 - KL_loss: 42.0785 - lr: 0.0030\n",
      "Epoch 714/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.0133 - reconstruction_loss: 177.7936 - KL_loss: 41.9357 - lr: 0.0030\n",
      "Epoch 715/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.6925 - reconstruction_loss: 178.3212 - KL_loss: 42.0079 - lr: 0.0030\n",
      "Epoch 716/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.3153 - reconstruction_loss: 177.7610 - KL_loss: 42.2269 - lr: 0.0030\n",
      "Epoch 717/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.6906 - reconstruction_loss: 178.1343 - KL_loss: 42.2539 - lr: 0.0030\n",
      "Epoch 718/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.2473 - reconstruction_loss: 177.7238 - KL_loss: 42.2029 - lr: 0.0030\n",
      "Epoch 719/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.2874 - reconstruction_loss: 177.8381 - KL_loss: 42.1560 - lr: 0.0030\n",
      "Epoch 720/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.9401 - reconstruction_loss: 178.2999 - KL_loss: 42.2804 - lr: 0.0030\n",
      "Epoch 721/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.5504 - reconstruction_loss: 177.2789 - KL_loss: 42.0687 - lr: 0.0030\n",
      "Epoch 722/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.2637 - reconstruction_loss: 179.6315 - KL_loss: 42.1716 - lr: 0.0030\n",
      "Epoch 723/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.7609 - reconstruction_loss: 177.8767 - KL_loss: 42.4907 - lr: 0.0030\n",
      "Epoch 724/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.6627 - reconstruction_loss: 178.8397 - KL_loss: 42.3958 - lr: 0.0030\n",
      "Epoch 725/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.9913 - reconstruction_loss: 179.2307 - KL_loss: 42.1960 - lr: 0.0030\n",
      "Epoch 726/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.8934 - reconstruction_loss: 178.5093 - KL_loss: 41.9646 - lr: 0.0030\n",
      "Epoch 727/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.5886 - reconstruction_loss: 178.1725 - KL_loss: 42.1125 - lr: 0.0030\n",
      "Epoch 728/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.7719 - reconstruction_loss: 178.6114 - KL_loss: 42.6154 - lr: 0.0030\n",
      "Epoch 729/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.2089 - reconstruction_loss: 177.5744 - KL_loss: 42.3765 - lr: 0.0030\n",
      "Epoch 730/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.3246 - reconstruction_loss: 178.8104 - KL_loss: 41.9639 - lr: 0.0030\n",
      "Epoch 731/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.9889 - reconstruction_loss: 178.7590 - KL_loss: 41.8752 - lr: 0.0030\n",
      "Epoch 732/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.8365 - reconstruction_loss: 178.3645 - KL_loss: 42.0887 - lr: 0.0030\n",
      "Epoch 733/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.5254 - reconstruction_loss: 177.7205 - KL_loss: 42.5216 - lr: 0.0030\n",
      "Epoch 734/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.0803 - reconstruction_loss: 177.5047 - KL_loss: 42.3174 - lr: 0.0030\n",
      "Epoch 735/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.6890 - reconstruction_loss: 178.4945 - KL_loss: 41.8868 - lr: 0.0030\n",
      "Epoch 736/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.8171 - reconstruction_loss: 177.7460 - KL_loss: 41.8532 - lr: 0.0030\n",
      "Epoch 737/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.3061 - reconstruction_loss: 177.8382 - KL_loss: 42.2274 - lr: 0.0030\n",
      "Epoch 738/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.6375 - reconstruction_loss: 176.9987 - KL_loss: 42.4793 - lr: 0.0030\n",
      "Epoch 739/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.1652 - reconstruction_loss: 177.5522 - KL_loss: 42.4154 - lr: 0.0030\n",
      "Epoch 740/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.1453 - reconstruction_loss: 177.9689 - KL_loss: 41.9906 - lr: 0.0030\n",
      "Epoch 741/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.2902 - reconstruction_loss: 177.3424 - KL_loss: 41.8441 - lr: 0.0030\n",
      "Epoch 742/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.9025 - reconstruction_loss: 178.1876 - KL_loss: 42.3713 - lr: 0.0030\n",
      "Epoch 743/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.7745 - reconstruction_loss: 176.9652 - KL_loss: 42.6125 - lr: 0.0030\n",
      "Epoch 744/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.7412 - reconstruction_loss: 177.2041 - KL_loss: 42.3392 - lr: 0.0030\n",
      "Epoch 745/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.9971 - reconstruction_loss: 178.7584 - KL_loss: 41.8964 - lr: 0.0030\n",
      "Epoch 746/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.2608 - reconstruction_loss: 177.2152 - KL_loss: 41.9645 - lr: 0.0030\n",
      "Epoch 747/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.4391 - reconstruction_loss: 178.5952 - KL_loss: 42.5257 - lr: 0.0030\n",
      "Epoch 748/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.7693 - reconstruction_loss: 177.8270 - KL_loss: 42.5998 - lr: 0.0030\n",
      "Epoch 749/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.0128 - reconstruction_loss: 178.4609 - KL_loss: 42.1722 - lr: 0.0030\n",
      "Epoch 750/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.5912 - reconstruction_loss: 178.2101 - KL_loss: 42.1083 - lr: 0.0030\n",
      "Epoch 751/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 243.7893 - reconstruction_loss: 178.9220 - KL_loss: 42.4978 - lr: 0.0030\n",
      "Epoch 752/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.2092 - reconstruction_loss: 177.7596 - KL_loss: 42.2164 - lr: 0.0030\n",
      "Epoch 753/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.4574 - reconstruction_loss: 180.1293 - KL_loss: 41.8899 - lr: 0.0030\n",
      "Epoch 754/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 244.6422 - reconstruction_loss: 179.9402 - KL_loss: 42.2154 - lr: 0.0030\n",
      "Epoch 755/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.0114 - reconstruction_loss: 177.5039 - KL_loss: 42.3542 - lr: 0.0030\n",
      "Epoch 756/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.0217 - reconstruction_loss: 179.1036 - KL_loss: 42.4756 - lr: 0.0030\n",
      "Epoch 757/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.6102 - reconstruction_loss: 178.1435 - KL_loss: 42.2301 - lr: 0.0030\n",
      "Epoch 758/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.6852 - reconstruction_loss: 179.5625 - KL_loss: 41.7237 - lr: 0.0030\n",
      "Epoch 759/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.0221 - reconstruction_loss: 179.0083 - KL_loss: 41.6947 - lr: 0.0030\n",
      "Epoch 760/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.0148 - reconstruction_loss: 178.5271 - KL_loss: 42.2233 - lr: 0.0030\n",
      "Epoch 761/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.0397 - reconstruction_loss: 177.9594 - KL_loss: 42.8056 - lr: 0.0030\n",
      "Epoch 762/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.8633 - reconstruction_loss: 177.2906 - KL_loss: 42.4691 - lr: 0.0030\n",
      "Epoch 763/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.3586 - reconstruction_loss: 178.8838 - KL_loss: 42.1437 - lr: 0.0030\n",
      "Epoch 764/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.1893 - reconstruction_loss: 178.3534 - KL_loss: 41.6233 - lr: 0.0030\n",
      "Epoch 765/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.8383 - reconstruction_loss: 179.5716 - KL_loss: 41.9370 - lr: 0.0030\n",
      "Epoch 766/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 244.2919 - reconstruction_loss: 178.9860 - KL_loss: 42.8250 - lr: 0.0030\n",
      "Epoch 767/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.9068 - reconstruction_loss: 177.9334 - KL_loss: 42.7541 - lr: 0.0030\n",
      "Epoch 768/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.4337 - reconstruction_loss: 178.1988 - KL_loss: 41.9937 - lr: 0.0030\n",
      "Epoch 769/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.4875 - reconstruction_loss: 179.2527 - KL_loss: 41.9213 - lr: 0.0030\n",
      "Epoch 770/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.8047 - reconstruction_loss: 177.9392 - KL_loss: 41.7619 - lr: 0.0030\n",
      "Epoch 771/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.5059 - reconstruction_loss: 179.1011 - KL_loss: 42.0928 - lr: 0.0030\n",
      "Epoch 772/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.0435 - reconstruction_loss: 178.2538 - KL_loss: 42.5671 - lr: 0.0030\n",
      "Epoch 773/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 243.6124 - reconstruction_loss: 178.8549 - KL_loss: 42.3900 - lr: 0.0030\n",
      "Epoch 774/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.8474 - reconstruction_loss: 178.7707 - KL_loss: 41.8322 - lr: 0.0030\n",
      "Epoch 775/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.5162 - reconstruction_loss: 178.5776 - KL_loss: 41.7105 - lr: 0.0030\n",
      "Epoch 776/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.3796 - reconstruction_loss: 178.1138 - KL_loss: 42.0790 - lr: 0.0030\n",
      "Epoch 777/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.7855 - reconstruction_loss: 177.3522 - KL_loss: 42.3308 - lr: 0.0030\n",
      "Epoch 778/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.8538 - reconstruction_loss: 177.6774 - KL_loss: 42.0708 - lr: 0.0030\n",
      "Epoch 779/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.9976 - reconstruction_loss: 178.2962 - KL_loss: 41.6201 - lr: 0.0030\n",
      "Epoch 780/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.6826 - reconstruction_loss: 177.7461 - KL_loss: 41.9047 - lr: 0.0030\n",
      "Epoch 781/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.4811 - reconstruction_loss: 177.1337 - KL_loss: 42.3402 - lr: 0.0030\n",
      "Epoch 782/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.9854 - reconstruction_loss: 177.3599 - KL_loss: 42.5266 - lr: 0.0030\n",
      "Epoch 783/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.0824 - reconstruction_loss: 177.1333 - KL_loss: 41.9938 - lr: 0.0030\n",
      "Epoch 784/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.5090 - reconstruction_loss: 177.8171 - KL_loss: 41.6529 - lr: 0.0030\n",
      "Epoch 785/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.2874 - reconstruction_loss: 177.4588 - KL_loss: 41.8480 - lr: 0.0030\n",
      "Epoch 786/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.6622 - reconstruction_loss: 176.3291 - KL_loss: 42.4148 - lr: 0.0030\n",
      "Epoch 787/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.8918 - reconstruction_loss: 177.2744 - KL_loss: 42.5574 - lr: 0.0030\n",
      "Epoch 788/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.6012 - reconstruction_loss: 176.5295 - KL_loss: 42.1539 - lr: 0.0030\n",
      "Epoch 789/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.8489 - reconstruction_loss: 178.2218 - KL_loss: 41.5807 - lr: 0.0030\n",
      "Epoch 790/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.3697 - reconstruction_loss: 177.5504 - KL_loss: 41.7728 - lr: 0.0030\n",
      "Epoch 791/1000\n",
      "12416/12416 [==============================] - 0s 10us/sample - loss: 241.0634 - reconstruction_loss: 176.6602 - KL_loss: 42.4444 - lr: 0.0030\n",
      "Epoch 792/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.7881 - reconstruction_loss: 177.0331 - KL_loss: 42.7118 - lr: 0.0030\n",
      "Epoch 793/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.7104 - reconstruction_loss: 176.3978 - KL_loss: 42.3965 - lr: 0.0030\n",
      "Epoch 794/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.0777 - reconstruction_loss: 178.0259 - KL_loss: 41.9706 - lr: 0.0030\n",
      "Epoch 795/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.9249 - reconstruction_loss: 177.1988 - KL_loss: 41.7581 - lr: 0.0030\n",
      "Epoch 796/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 243.2120 - reconstruction_loss: 178.8088 - KL_loss: 42.1799 - lr: 0.0030\n",
      "Epoch 797/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.4157 - reconstruction_loss: 177.3930 - KL_loss: 42.8468 - lr: 0.0030\n",
      "Epoch 798/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.9832 - reconstruction_loss: 177.9698 - KL_loss: 42.8240 - lr: 0.0030\n",
      "Epoch 799/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.2970 - reconstruction_loss: 178.0513 - KL_loss: 42.1105 - lr: 0.0030\n",
      "Epoch 800/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.8231 - reconstruction_loss: 179.0365 - KL_loss: 41.6217 - lr: 0.0030\n",
      "Epoch 801/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.1704 - reconstruction_loss: 178.3631 - KL_loss: 41.6735 - lr: 0.0030\n",
      "Epoch 802/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.1617 - reconstruction_loss: 177.6207 - KL_loss: 42.4608 - lr: 0.0030\n",
      "Epoch 803/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.3324 - reconstruction_loss: 177.2442 - KL_loss: 42.9987 - lr: 0.0030\n",
      "Epoch 804/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.1385 - reconstruction_loss: 176.7000 - KL_loss: 42.5117 - lr: 0.0030\n",
      "Epoch 805/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.8592 - reconstruction_loss: 177.8875 - KL_loss: 41.8499 - lr: 0.0030\n",
      "Epoch 806/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.0932 - reconstruction_loss: 177.4565 - KL_loss: 41.7673 - lr: 0.0030\n",
      "Epoch 807/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.6844 - reconstruction_loss: 177.6082 - KL_loss: 42.0944 - lr: 0.0030\n",
      "Epoch 808/1000\n",
      "12416/12416 [==============================] - 0s 13us/sample - loss: 240.3768 - reconstruction_loss: 176.0511 - KL_loss: 42.4728 - lr: 0.0030\n",
      "Epoch 809/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.3092 - reconstruction_loss: 177.7046 - KL_loss: 42.5369 - lr: 0.0030\n",
      "Epoch 810/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.2121 - reconstruction_loss: 177.1959 - KL_loss: 42.0415 - lr: 0.0030\n",
      "Epoch 811/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.3879 - reconstruction_loss: 177.6920 - KL_loss: 41.7257 - lr: 0.0030\n",
      "Epoch 812/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.5108 - reconstruction_loss: 177.4077 - KL_loss: 42.1107 - lr: 0.0030\n",
      "Epoch 813/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.6834 - reconstruction_loss: 176.3720 - KL_loss: 42.5051 - lr: 0.0030\n",
      "Epoch 814/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.1990 - reconstruction_loss: 176.9229 - KL_loss: 42.4012 - lr: 0.0030\n",
      "Epoch 815/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.7020 - reconstruction_loss: 177.0181 - KL_loss: 41.7905 - lr: 0.0030\n",
      "Epoch 816/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.1315 - reconstruction_loss: 177.3225 - KL_loss: 41.8859 - lr: 0.0030\n",
      "Epoch 817/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.6684 - reconstruction_loss: 176.6782 - KL_loss: 42.1482 - lr: 0.0030\n",
      "Epoch 818/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.7935 - reconstruction_loss: 176.5734 - KL_loss: 42.3775 - lr: 0.0030\n",
      "Epoch 819/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.5338 - reconstruction_loss: 176.2874 - KL_loss: 42.4019 - lr: 0.0030\n",
      "Epoch 820/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.8944 - reconstruction_loss: 176.8478 - KL_loss: 42.1856 - lr: 0.0030\n",
      "Epoch 821/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.3508 - reconstruction_loss: 176.6481 - KL_loss: 41.9109 - lr: 0.0030\n",
      "Epoch 822/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.2160 - reconstruction_loss: 176.7691 - KL_loss: 41.6940 - lr: 0.0030\n",
      "Epoch 823/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.2796 - reconstruction_loss: 176.4290 - KL_loss: 42.0121 - lr: 0.0030\n",
      "Epoch 824/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.9915 - reconstruction_loss: 175.6443 - KL_loss: 42.6162 - lr: 0.0030\n",
      "Epoch 825/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.1822 - reconstruction_loss: 175.8231 - KL_loss: 42.5735 - lr: 0.0030\n",
      "Epoch 826/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.1642 - reconstruction_loss: 176.3467 - KL_loss: 42.0977 - lr: 0.0030\n",
      "Epoch 827/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.9193 - reconstruction_loss: 176.1608 - KL_loss: 42.0414 - lr: 0.0030\n",
      "Epoch 828/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.8822 - reconstruction_loss: 176.0142 - KL_loss: 42.1277 - lr: 0.0030\n",
      "Epoch 829/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.0218 - reconstruction_loss: 175.8488 - KL_loss: 42.4090 - lr: 0.0030\n",
      "Epoch 830/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.6479 - reconstruction_loss: 175.7184 - KL_loss: 42.2335 - lr: 0.0030\n",
      "Epoch 831/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.4167 - reconstruction_loss: 176.5534 - KL_loss: 42.1457 - lr: 0.0030\n",
      "Epoch 832/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.6304 - reconstruction_loss: 175.6507 - KL_loss: 42.2914 - lr: 0.0030\n",
      "Epoch 833/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.7680 - reconstruction_loss: 175.7631 - KL_loss: 42.3166 - lr: 0.0030\n",
      "Epoch 834/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.3323 - reconstruction_loss: 176.2346 - KL_loss: 42.3433 - lr: 0.0030\n",
      "Epoch 835/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.5397 - reconstruction_loss: 175.8136 - KL_loss: 42.0931 - lr: 0.0030\n",
      "Epoch 836/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.1741 - reconstruction_loss: 176.4436 - KL_loss: 42.0395 - lr: 0.0030\n",
      "Epoch 837/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.7026 - reconstruction_loss: 175.7545 - KL_loss: 42.2624 - lr: 0.0030\n",
      "Epoch 838/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.7175 - reconstruction_loss: 175.3187 - KL_loss: 42.7428 - lr: 0.0030\n",
      "Epoch 839/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.8428 - reconstruction_loss: 175.5986 - KL_loss: 42.5611 - lr: 0.0030\n",
      "Epoch 840/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.3971 - reconstruction_loss: 175.8924 - KL_loss: 41.9191 - lr: 0.0030\n",
      "Epoch 841/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.6435 - reconstruction_loss: 176.2641 - KL_loss: 41.7930 - lr: 0.0030\n",
      "Epoch 842/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.5607 - reconstruction_loss: 175.6409 - KL_loss: 42.2652 - lr: 0.0030\n",
      "Epoch 843/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.3877 - reconstruction_loss: 175.0753 - KL_loss: 42.7177 - lr: 0.0030\n",
      "Epoch 844/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.6030 - reconstruction_loss: 175.2178 - KL_loss: 42.7693 - lr: 0.0030\n",
      "Epoch 845/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.1265 - reconstruction_loss: 175.3495 - KL_loss: 42.1665 - lr: 0.0030\n",
      "Epoch 846/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.8966 - reconstruction_loss: 176.4230 - KL_loss: 41.7870 - lr: 0.0030\n",
      "Epoch 847/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.1873 - reconstruction_loss: 175.7133 - KL_loss: 41.9247 - lr: 0.0030\n",
      "Epoch 848/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.4802 - reconstruction_loss: 175.5860 - KL_loss: 42.3183 - lr: 0.0030\n",
      "Epoch 849/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.1605 - reconstruction_loss: 174.7940 - KL_loss: 42.7947 - lr: 0.0030\n",
      "Epoch 850/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.6934 - reconstruction_loss: 175.4276 - KL_loss: 42.6943 - lr: 0.0030\n",
      "Epoch 851/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.8342 - reconstruction_loss: 175.2227 - KL_loss: 42.0934 - lr: 0.0030\n",
      "Epoch 852/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.2678 - reconstruction_loss: 175.7225 - KL_loss: 41.9885 - lr: 0.0030\n",
      "Epoch 853/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.0694 - reconstruction_loss: 175.4492 - KL_loss: 42.0676 - lr: 0.0030\n",
      "Epoch 854/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.0580 - reconstruction_loss: 175.9703 - KL_loss: 42.4917 - lr: 0.0030\n",
      "Epoch 855/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.6529 - reconstruction_loss: 174.2670 - KL_loss: 42.9199 - lr: 0.0030\n",
      "Epoch 856/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.6382 - reconstruction_loss: 176.3841 - KL_loss: 42.5709 - lr: 0.0030\n",
      "Epoch 857/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.0185 - reconstruction_loss: 175.3386 - KL_loss: 42.1220 - lr: 0.0030\n",
      "Epoch 858/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.9985 - reconstruction_loss: 177.0999 - KL_loss: 42.1460 - lr: 0.0030\n",
      "Epoch 859/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.3513 - reconstruction_loss: 176.4399 - KL_loss: 42.1792 - lr: 0.0030\n",
      "Epoch 860/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.0498 - reconstruction_loss: 175.8257 - KL_loss: 42.5721 - lr: 0.0030\n",
      "Epoch 861/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.5178 - reconstruction_loss: 175.8720 - KL_loss: 42.8725 - lr: 0.0030\n",
      "Epoch 862/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.3577 - reconstruction_loss: 175.3749 - KL_loss: 42.3916 - lr: 0.0030\n",
      "Epoch 863/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.8078 - reconstruction_loss: 176.8967 - KL_loss: 42.0919 - lr: 0.0030\n",
      "Epoch 864/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.7243 - reconstruction_loss: 176.0392 - KL_loss: 42.0421 - lr: 0.0030\n",
      "Epoch 865/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.6008 - reconstruction_loss: 175.7849 - KL_loss: 42.2219 - lr: 0.0030\n",
      "Epoch 866/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.9845 - reconstruction_loss: 176.5115 - KL_loss: 42.7478 - lr: 0.0030\n",
      "Epoch 867/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.0943 - reconstruction_loss: 174.9288 - KL_loss: 42.6666 - lr: 0.0030\n",
      "Epoch 868/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.2641 - reconstruction_loss: 178.1769 - KL_loss: 42.1517 - lr: 0.0030\n",
      "Epoch 869/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.7337 - reconstruction_loss: 177.7558 - KL_loss: 42.1090 - lr: 0.0030\n",
      "Epoch 870/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.8248 - reconstruction_loss: 176.8037 - KL_loss: 42.2906 - lr: 0.0030\n",
      "Epoch 871/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.2982 - reconstruction_loss: 177.0435 - KL_loss: 42.5339 - lr: 0.0030\n",
      "Epoch 872/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.6927 - reconstruction_loss: 176.4981 - KL_loss: 42.5050 - lr: 0.0030\n",
      "Epoch 873/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.6380 - reconstruction_loss: 176.6519 - KL_loss: 42.2687 - lr: 0.0030\n",
      "Epoch 874/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.3595 - reconstruction_loss: 176.4975 - KL_loss: 42.2089 - lr: 0.0030\n",
      "Epoch 875/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.7236 - reconstruction_loss: 176.6980 - KL_loss: 42.3219 - lr: 0.0030\n",
      "Epoch 876/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.2748 - reconstruction_loss: 175.5681 - KL_loss: 42.1858 - lr: 0.0030\n",
      "Epoch 877/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.5653 - reconstruction_loss: 177.8755 - KL_loss: 41.9084 - lr: 0.0030\n",
      "Epoch 878/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.2245 - reconstruction_loss: 176.3011 - KL_loss: 42.2438 - lr: 0.0030\n",
      "Epoch 879/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 242.2686 - reconstruction_loss: 177.4700 - KL_loss: 42.9628 - lr: 0.0030\n",
      "Epoch 880/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 242.6924 - reconstruction_loss: 177.8163 - KL_loss: 42.8980 - lr: 0.0030\n",
      "Epoch 881/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.4690 - reconstruction_loss: 176.7951 - KL_loss: 42.0459 - lr: 0.0030\n",
      "Epoch 882/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.1587 - reconstruction_loss: 177.5229 - KL_loss: 41.9515 - lr: 0.0030\n",
      "Epoch 883/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.5484 - reconstruction_loss: 176.7857 - KL_loss: 42.1456 - lr: 0.0030\n",
      "Epoch 884/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.2586 - reconstruction_loss: 176.2470 - KL_loss: 42.3350 - lr: 0.0030\n",
      "Epoch 885/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.5175 - reconstruction_loss: 176.1416 - KL_loss: 42.7598 - lr: 0.0030\n",
      "Epoch 886/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.0546 - reconstruction_loss: 175.1306 - KL_loss: 42.4800 - lr: 0.0030\n",
      "Epoch 887/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 241.1396 - reconstruction_loss: 177.5117 - KL_loss: 41.9916 - lr: 0.0030\n",
      "Epoch 888/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.1703 - reconstruction_loss: 176.6882 - KL_loss: 41.8753 - lr: 0.0030\n",
      "Epoch 889/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.7456 - reconstruction_loss: 177.0219 - KL_loss: 42.1574 - lr: 0.0030\n",
      "Epoch 890/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.5434 - reconstruction_loss: 176.5233 - KL_loss: 42.3823 - lr: 0.0030\n",
      "Epoch 891/1000\n",
      "12416/12416 [==============================] - 2s 126us/sample - loss: 240.5619 - reconstruction_loss: 176.2625 - KL_loss: 42.6883 - lr: 0.0030\n",
      "Epoch 892/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.1650 - reconstruction_loss: 175.9670 - KL_loss: 42.6214 - lr: 0.0030\n",
      "Epoch 893/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.8841 - reconstruction_loss: 176.2347 - KL_loss: 42.1077 - lr: 0.0030\n",
      "Epoch 894/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.8169 - reconstruction_loss: 176.6773 - KL_loss: 41.6047 - lr: 0.0030\n",
      "Epoch 895/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.8848 - reconstruction_loss: 176.4638 - KL_loss: 41.8928 - lr: 0.0030\n",
      "Epoch 896/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.6583 - reconstruction_loss: 175.8408 - KL_loss: 42.3060 - lr: 0.0030\n",
      "Epoch 897/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.5771 - reconstruction_loss: 175.6014 - KL_loss: 42.4672 - lr: 0.0030\n",
      "Epoch 898/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.1017 - reconstruction_loss: 175.2709 - KL_loss: 42.3712 - lr: 0.0030\n",
      "Epoch 899/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.3939 - reconstruction_loss: 175.7064 - KL_loss: 42.2045 - lr: 0.0030\n",
      "Epoch 900/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.5571 - reconstruction_loss: 176.1375 - KL_loss: 41.9284 - lr: 0.0030\n",
      "Epoch 901/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.5847 - reconstruction_loss: 175.0671 - KL_loss: 42.1209 - lr: 0.0030\n",
      "Epoch 902/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.9466 - reconstruction_loss: 175.8309 - KL_loss: 42.5717 - lr: 0.0030\n",
      "Epoch 903/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.2445 - reconstruction_loss: 174.4447 - KL_loss: 42.4615 - lr: 0.0030\n",
      "Epoch 904/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.4265 - reconstruction_loss: 176.7314 - KL_loss: 42.0538 - lr: 0.0030\n",
      "Epoch 905/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.4529 - reconstruction_loss: 176.7239 - KL_loss: 42.0426 - lr: 0.0030\n",
      "Epoch 906/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.6341 - reconstruction_loss: 175.0623 - KL_loss: 42.1822 - lr: 0.0030\n",
      "Epoch 907/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.4172 - reconstruction_loss: 176.2933 - KL_loss: 42.5283 - lr: 0.0030\n",
      "Epoch 908/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.6519 - reconstruction_loss: 175.4202 - KL_loss: 42.7201 - lr: 0.0030\n",
      "Epoch 909/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.5069 - reconstruction_loss: 175.6472 - KL_loss: 42.3628 - lr: 0.0030\n",
      "Epoch 910/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.9193 - reconstruction_loss: 176.3964 - KL_loss: 41.9531 - lr: 0.0030\n",
      "Epoch 911/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.0906 - reconstruction_loss: 175.7528 - KL_loss: 41.8994 - lr: 0.0030\n",
      "Epoch 912/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.4058 - reconstruction_loss: 175.4663 - KL_loss: 42.4715 - lr: 0.0030\n",
      "Epoch 913/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.4121 - reconstruction_loss: 175.0716 - KL_loss: 42.8156 - lr: 0.0030\n",
      "Epoch 914/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.6330 - reconstruction_loss: 174.9662 - KL_loss: 42.3411 - lr: 0.0030\n",
      "Epoch 915/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.3820 - reconstruction_loss: 175.9310 - KL_loss: 42.0111 - lr: 0.0030\n",
      "Epoch 916/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.7517 - reconstruction_loss: 175.5711 - KL_loss: 41.7750 - lr: 0.0030\n",
      "Epoch 917/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.2513 - reconstruction_loss: 175.7876 - KL_loss: 42.0034 - lr: 0.0030\n",
      "Epoch 918/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.8673 - reconstruction_loss: 174.8702 - KL_loss: 42.6501 - lr: 0.0030\n",
      "Epoch 919/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.8439 - reconstruction_loss: 174.5479 - KL_loss: 42.9027 - lr: 0.0030\n",
      "Epoch 920/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.4599 - reconstruction_loss: 174.8785 - KL_loss: 42.2724 - lr: 0.0030\n",
      "Epoch 921/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.8090 - reconstruction_loss: 175.5172 - KL_loss: 41.9229 - lr: 0.0030\n",
      "Epoch 922/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.5257 - reconstruction_loss: 175.2006 - KL_loss: 41.9982 - lr: 0.0030\n",
      "Epoch 923/1000\n",
      "12416/12416 [==============================] - 2s 128us/sample - loss: 238.0963 - reconstruction_loss: 174.4656 - KL_loss: 42.3511 - lr: 0.0030\n",
      "Epoch 924/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.9839 - reconstruction_loss: 175.0765 - KL_loss: 42.5027 - lr: 0.0030\n",
      "Epoch 925/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.8854 - reconstruction_loss: 174.2547 - KL_loss: 42.3941 - lr: 0.0030\n",
      "Epoch 926/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.3752 - reconstruction_loss: 175.7177 - KL_loss: 42.3090 - lr: 0.0030\n",
      "Epoch 927/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.5233 - reconstruction_loss: 174.9907 - KL_loss: 42.1945 - lr: 0.0030\n",
      "Epoch 928/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.1573 - reconstruction_loss: 175.3530 - KL_loss: 42.3188 - lr: 0.0030\n",
      "Epoch 929/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.4937 - reconstruction_loss: 174.4793 - KL_loss: 42.7019 - lr: 0.0030\n",
      "Epoch 930/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.5008 - reconstruction_loss: 175.3202 - KL_loss: 42.7936 - lr: 0.0030\n",
      "Epoch 931/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.1203 - reconstruction_loss: 174.6560 - KL_loss: 42.1494 - lr: 0.0030\n",
      "Epoch 932/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 241.0193 - reconstruction_loss: 177.4570 - KL_loss: 41.9838 - lr: 0.0030\n",
      "Epoch 933/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.0059 - reconstruction_loss: 176.3573 - KL_loss: 42.1135 - lr: 0.0030\n",
      "Epoch 934/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.6669 - reconstruction_loss: 175.7262 - KL_loss: 42.5595 - lr: 0.0030\n",
      "Epoch 935/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 240.8405 - reconstruction_loss: 176.2571 - KL_loss: 42.9749 - lr: 0.0030\n",
      "Epoch 936/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.4797 - reconstruction_loss: 174.7416 - KL_loss: 42.4116 - lr: 0.0030\n",
      "Epoch 937/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 240.8154 - reconstruction_loss: 177.2290 - KL_loss: 41.9107 - lr: 0.0030\n",
      "Epoch 938/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.9501 - reconstruction_loss: 176.4595 - KL_loss: 41.9678 - lr: 0.0030\n",
      "Epoch 939/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.4199 - reconstruction_loss: 175.7090 - KL_loss: 42.2653 - lr: 0.0030\n",
      "Epoch 940/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.7931 - reconstruction_loss: 175.7573 - KL_loss: 42.5766 - lr: 0.0030\n",
      "Epoch 941/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.9751 - reconstruction_loss: 174.8282 - KL_loss: 42.7821 - lr: 0.0030\n",
      "Epoch 942/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.3020 - reconstruction_loss: 175.4750 - KL_loss: 42.4387 - lr: 0.0030\n",
      "Epoch 943/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.8685 - reconstruction_loss: 175.5707 - KL_loss: 41.9087 - lr: 0.0030\n",
      "Epoch 944/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.0258 - reconstruction_loss: 176.0492 - KL_loss: 41.6320 - lr: 0.0030\n",
      "Epoch 945/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.0219 - reconstruction_loss: 175.5243 - KL_loss: 42.1052 - lr: 0.0030\n",
      "Epoch 946/1000\n",
      "12416/12416 [==============================] - 0s 7us/sample - loss: 237.9903 - reconstruction_loss: 174.1610 - KL_loss: 42.6031 - lr: 0.0030\n",
      "Epoch 947/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.4256 - reconstruction_loss: 175.4250 - KL_loss: 42.6255 - lr: 0.0030\n",
      "Epoch 948/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.7073 - reconstruction_loss: 175.0862 - KL_loss: 42.2630 - lr: 0.0030\n",
      "Epoch 949/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.8058 - reconstruction_loss: 175.6319 - KL_loss: 41.8546 - lr: 0.0030\n",
      "Epoch 950/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.1583 - reconstruction_loss: 175.7554 - KL_loss: 41.9576 - lr: 0.0030\n",
      "Epoch 951/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.6814 - reconstruction_loss: 175.0064 - KL_loss: 42.3783 - lr: 0.0030\n",
      "Epoch 952/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.6539 - reconstruction_loss: 173.6522 - KL_loss: 42.8200 - lr: 0.0030\n",
      "Epoch 953/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.6882 - reconstruction_loss: 175.7830 - KL_loss: 42.5186 - lr: 0.0030\n",
      "Epoch 954/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.9653 - reconstruction_loss: 174.7704 - KL_loss: 41.9370 - lr: 0.0030\n",
      "Epoch 955/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.6627 - reconstruction_loss: 176.2305 - KL_loss: 42.0129 - lr: 0.0030\n",
      "Epoch 956/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.6945 - reconstruction_loss: 175.2304 - KL_loss: 42.1512 - lr: 0.0030\n",
      "Epoch 957/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.9468 - reconstruction_loss: 176.0148 - KL_loss: 42.4852 - lr: 0.0030\n",
      "Epoch 958/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.2133 - reconstruction_loss: 175.0115 - KL_loss: 42.8203 - lr: 0.0030\n",
      "Epoch 959/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 239.6664 - reconstruction_loss: 175.7173 - KL_loss: 42.5936 - lr: 0.0030\n",
      "Epoch 960/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.5437 - reconstruction_loss: 176.0716 - KL_loss: 42.0350 - lr: 0.0030\n",
      "Epoch 961/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.4375 - reconstruction_loss: 175.6458 - KL_loss: 41.5299 - lr: 0.0030\n",
      "Epoch 962/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.9845 - reconstruction_loss: 175.5580 - KL_loss: 42.1125 - lr: 0.0030\n",
      "Epoch 963/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.0182 - reconstruction_loss: 174.1100 - KL_loss: 42.7044 - lr: 0.0030\n",
      "Epoch 964/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.9264 - reconstruction_loss: 175.0276 - KL_loss: 42.6139 - lr: 0.0030\n",
      "Epoch 965/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.7055 - reconstruction_loss: 175.2758 - KL_loss: 42.1769 - lr: 0.0030\n",
      "Epoch 966/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.4056 - reconstruction_loss: 175.2351 - KL_loss: 41.9517 - lr: 0.0030\n",
      "Epoch 967/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.3148 - reconstruction_loss: 174.9629 - KL_loss: 42.0358 - lr: 0.0030\n",
      "Epoch 968/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 237.6211 - reconstruction_loss: 173.9734 - KL_loss: 42.4907 - lr: 0.0030\n",
      "Epoch 969/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.9002 - reconstruction_loss: 174.9889 - KL_loss: 42.6249 - lr: 0.0030\n",
      "Epoch 970/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.2998 - reconstruction_loss: 174.0199 - KL_loss: 42.1419 - lr: 0.0030\n",
      "Epoch 971/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 239.0441 - reconstruction_loss: 176.0265 - KL_loss: 41.7172 - lr: 0.0030\n",
      "Epoch 972/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.9443 - reconstruction_loss: 175.5907 - KL_loss: 42.0302 - lr: 0.0030\n",
      "Epoch 973/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.3977 - reconstruction_loss: 173.5581 - KL_loss: 42.7101 - lr: 0.0030\n",
      "Epoch 974/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 238.2509 - reconstruction_loss: 174.3679 - KL_loss: 42.6481 - lr: 0.0030\n",
      "Epoch 975/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.0364 - reconstruction_loss: 174.5337 - KL_loss: 42.3419 - lr: 0.0030\n",
      "Epoch 976/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 238.1572 - reconstruction_loss: 174.9077 - KL_loss: 42.0277 - lr: 0.0030\n",
      "Epoch 977/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 237.8284 - reconstruction_loss: 174.4803 - KL_loss: 42.1679 - lr: 0.0030\n",
      "Epoch 978/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 237.7730 - reconstruction_loss: 174.1277 - KL_loss: 42.5128 - lr: 0.0030\n",
      "Epoch 979/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 237.3666 - reconstruction_loss: 173.7552 - KL_loss: 42.4939 - lr: 0.0030\n",
      "Epoch 980/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.7160 - reconstruction_loss: 174.4899 - KL_loss: 42.1166 - lr: 0.0030\n",
      "Epoch 981/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 237.5532 - reconstruction_loss: 174.5791 - KL_loss: 41.8759 - lr: 0.0030\n",
      "Epoch 982/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 237.2927 - reconstruction_loss: 174.0292 - KL_loss: 42.1841 - lr: 0.0030\n",
      "Epoch 983/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.4460 - reconstruction_loss: 173.8600 - KL_loss: 42.4889 - lr: 0.0030\n",
      "Epoch 984/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.2988 - reconstruction_loss: 173.7055 - KL_loss: 42.5034 - lr: 0.0030\n",
      "Epoch 985/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.3704 - reconstruction_loss: 174.0046 - KL_loss: 42.2789 - lr: 0.0030\n",
      "Epoch 986/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.0594 - reconstruction_loss: 174.0857 - KL_loss: 41.9540 - lr: 0.0030\n",
      "Epoch 987/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.1367 - reconstruction_loss: 173.9671 - KL_loss: 42.1003 - lr: 0.0030\n",
      "Epoch 988/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.3539 - reconstruction_loss: 173.6096 - KL_loss: 42.6216 - lr: 0.0030\n",
      "Epoch 989/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 236.9410 - reconstruction_loss: 173.2973 - KL_loss: 42.6568 - lr: 0.0030\n",
      "Epoch 990/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 237.6857 - reconstruction_loss: 174.2786 - KL_loss: 42.3506 - lr: 0.0030\n",
      "Epoch 991/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 236.7439 - reconstruction_loss: 173.4837 - KL_loss: 42.2834 - lr: 0.0030\n",
      "Epoch 992/1000\n",
      "12416/12416 [==============================] - 0s 9us/sample - loss: 237.6101 - reconstruction_loss: 174.3442 - KL_loss: 42.2243 - lr: 0.0030\n",
      "Epoch 993/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.0114 - reconstruction_loss: 173.5497 - KL_loss: 42.4832 - lr: 0.0030\n",
      "Epoch 994/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.4192 - reconstruction_loss: 173.8596 - KL_loss: 42.5234 - lr: 0.0030\n",
      "Epoch 995/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.0932 - reconstruction_loss: 173.7192 - KL_loss: 42.3802 - lr: 0.0030\n",
      "Epoch 996/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.2587 - reconstruction_loss: 174.1647 - KL_loss: 42.0945 - lr: 0.0030\n",
      "Epoch 997/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 236.8839 - reconstruction_loss: 173.8703 - KL_loss: 42.0791 - lr: 0.0030\n",
      "Epoch 998/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.5862 - reconstruction_loss: 173.9860 - KL_loss: 42.5615 - lr: 0.0030\n",
      "Epoch 999/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 236.6212 - reconstruction_loss: 173.0363 - KL_loss: 42.6260 - lr: 0.0030\n",
      "Epoch 1000/1000\n",
      "12416/12416 [==============================] - 0s 8us/sample - loss: 237.4841 - reconstruction_loss: 174.0065 - KL_loss: 42.4158 - lr: 0.0030\n",
      "\n",
      "training finished in 1000 epochs (reach max pre-specified epoches), transform data to adjust the platform effect...\n",
      "\n",
      "WARNING: when transforming data, after reversed Min-Max Scaling, apply exp transformation then multiple the factor and round to integer\n",
      "\n",
      "re-run DE on CVAE transformed scRNA-seq data!\n",
      "filtering genes present in <10 cells: 3 genes removed\n",
      "\n",
      "Differential analysis across cell-types on scRNA-seq data...\n",
      "0%...8%...17%...25%...33%...42%...50%...58%...67%...75%...83%...92%...finally selected 527 cell-type marker genes\n",
      "\n",
      "\n",
      "platform effect adjustment by CVAE finished. Elapsed time: 14.67 minutes.\n",
      "\n",
      "\n",
      "use the marker genes derived from CVAE transformed scRNA-seq for downstream regression!\n",
      "\n",
      "gene filtering before modeling...\n",
      "11 genes with nUMIs<5 in all spatial spots and need to be excluded\n",
      "finally use 516 genes for modeling\n",
      "\n",
      "spot filtering before modeling...\n",
      "all spots passed filtering\n",
      "\n",
      "\n",
      "######### Start GLRM modeling... #########\n",
      "\n",
      "GLRM settings:\n",
      "use SciPy minimize method:  L-BFGS-B\n",
      "global optimization turned off, local minimum will be used in GLRM\n",
      "use hybrid version of GLRM\n",
      "Numba detected total 64 available CPU cores. Use 64 CPU cores\n",
      "use 2001 points to calculate the heavy-tail density\n",
      "use weight threshold for Adaptive Lasso:  0.001\n",
      "total 252 unique nUMIs, min: 0.0, max: 1009.0\n",
      "\n",
      "Build graph: \n",
      " Graph with 581 nodes and 1029 edges\n",
      "\n",
      "estimation of gene-specific platform effect gamma_g is skipped as already using CVAE to adjust platform effect\n",
      "\n",
      "\n",
      "Start GLRM fitting...\n",
      "\n",
      "first estimate MLE theta and corresponding e^alpha and sigma^2...\n",
      "\n",
      "GLRM model initialization...\n",
      "calculate MLE theta and sigma^2 iteratively...\n",
      "  iter | time_opt | time_sig | sigma2\n",
      "     0 |   18.556 |    3.244 |  0.506\n",
      "     1 |   11.246 |    2.886 |  0.394\n",
      "     2 |    9.647 |    2.985 |  0.367\n",
      "     3 |    7.990 |    2.457 |  0.360\n",
      "     4 |    6.371 |    2.878 |  0.358\n",
      "     5 |    4.074 |    2.028 |  0.358\n",
      "MLE theta and sigma^2 calculation finished. Elapsed time: 1.24 minutes.\n",
      "MLE theta estimation finished. Elapsed time: 1.24 minutes.\n",
      "\n",
      "calculate weights of Adaptive Lasso...\n",
      "\n",
      "Stage 1: variable selection using Adaptive Lasso starts with the MLE theta and e^alpha, using already estimated sigma^2 and gamma_g...\n",
      "specified hyper-parameter for Adaptive Lasso is: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "hyper-parameter for Adaptive Lasso: use cross-validation to find the optimal value from 8 candidates...\n",
      "\n",
      "Start cross-validation for hyper-parameter lambda_r...\n",
      "directly estimate theta by Adaptive Lasso loss function as NO Graph Laplacian constrain!\n",
      "0%...11%...22%...33%...44%...56%...67%...78%...early stop\n",
      "find optimal lambda_r 1.931 with average negative log-likelihood 43996.9234 by 5 fold cross-validation. Elapsed time: 7.17 minutes.\n",
      "\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |     13.750 |     13.750 |      0.132 |      0.132 |       1.00 |       1.00 |    2.717 |    0.000 |    0.004 |   0.229445 |   0.114723\n",
      "     1 |     13.721 |      0.087 |      0.132 |      0.146 |       1.00 |       2.00 |    6.402 |    0.000 |    0.003 |   0.228922 |   0.114461\n",
      "     2 |     13.622 |      0.382 |      0.132 |      0.173 |       2.00 |       4.00 |    6.484 |    0.000 |    0.003 |   0.227431 |   0.113715\n",
      "     3 |     10.277 |     18.428 |      0.137 |      0.213 |       4.00 |       8.00 |    7.510 |    0.000 |    0.003 |   0.172430 |   0.086215\n",
      "     4 |      6.694 |     39.014 |      0.157 |      0.261 |       8.00 |       8.00 |    8.224 |    0.000 |    0.003 |   0.101426 |   0.050713\n",
      "     5 |      4.762 |     36.219 |      0.154 |      0.283 |       8.00 |       8.00 |   10.829 |    0.000 |    0.003 |   0.071220 |   0.035610\n",
      "     6 |      4.351 |     23.051 |      0.141 |      0.293 |       8.00 |       8.00 |    7.666 |    0.000 |    0.003 |   0.069808 |   0.034904\n",
      "     7 |      3.748 |     14.046 |      0.132 |      0.301 |       8.00 |      16.00 |    7.561 |    0.000 |    0.003 |   0.059306 |   0.029653\n",
      "     8 |      2.753 |     20.939 |      0.139 |      0.323 |      16.00 |      16.00 |    7.700 |    0.000 |    0.003 |   0.041096 |   0.020548\n",
      "     9 |      2.199 |     20.602 |      0.139 |      0.346 |      16.00 |      16.00 |    6.496 |    0.000 |    0.003 |   0.030777 |   0.015389\n",
      "    10 |      2.018 |     14.048 |      0.132 |      0.369 |      16.00 |      32.00 |    6.277 |    0.000 |    0.003 |   0.029398 |   0.014699\n",
      "    11 |      1.732 |     16.987 |      0.135 |      0.409 |      32.00 |      32.00 |    6.728 |    0.000 |    0.003 |   0.024831 |   0.012416\n",
      "    12 |      1.472 |     20.266 |      0.138 |      0.444 |      32.00 |      32.00 |    6.756 |    0.000 |    0.003 |   0.020174 |   0.010087\n",
      "    13 |      1.302 |     17.881 |      0.136 |      0.472 |      32.00 |      64.00 |    6.483 |    0.000 |    0.003 |   0.017894 |   0.008947\n",
      "    14 |      1.113 |     22.190 |      0.140 |      0.524 |      64.00 |      64.00 |    7.064 |    0.000 |    0.003 |   0.015352 |   0.007676\n",
      "    15 |      0.943 |     24.732 |      0.143 |      0.567 |      64.00 |      64.00 |    6.761 |    0.000 |    0.003 |   0.012833 |   0.006417\n",
      "    16 |      0.844 |     21.852 |      0.140 |      0.602 |      64.00 |     128.00 |    6.403 |    0.000 |    0.003 |   0.011387 |   0.005694\n",
      "    17 |      0.735 |     28.050 |      0.146 |      0.669 |     128.00 |     128.00 |    6.913 |    0.000 |    0.003 |   0.009912 |   0.004956\n",
      "    18 |      0.629 |     32.938 |      0.151 |      0.726 |     128.00 |     128.00 |    6.407 |    0.000 |    0.003 |   0.008362 |   0.004181\n",
      "    19 |      0.560 |     29.378 |      0.147 |      0.775 |     128.00 |     256.00 |    6.288 |    0.000 |    0.003 |   0.007351 |   0.003676\n",
      "    20 |      0.488 |     37.176 |      0.155 |      0.864 |     256.00 |     256.00 |    6.638 |    0.000 |    0.003 |   0.006387 |   0.003193\n",
      "    21 |      0.416 |     44.256 |      0.162 |      0.940 |     256.00 |     256.00 |    6.436 |    0.000 |    0.003 |   0.005412 |   0.002706\n",
      "    22 |      0.370 |     42.012 |      0.160 |      1.003 |     256.00 |     512.00 |    6.427 |    0.000 |    0.003 |   0.004726 |   0.002363\n",
      "    23 |      0.317 |     52.374 |      0.170 |      1.118 |     512.00 |     512.00 |    6.831 |    0.000 |    0.003 |   0.004018 |   0.002009\n",
      "    24 |      0.274 |     59.680 |      0.178 |      1.216 |     512.00 |     512.00 |    6.484 |    0.000 |    0.003 |   0.003421 |   0.001710\n",
      "    25 |      0.241 |     53.271 |      0.171 |      1.297 |     512.00 |    1024.00 |    9.478 |    0.000 |    0.003 |   0.003002 |   0.001501\n",
      "    26 |      0.208 |     77.482 |      0.196 |      1.444 |    1024.00 |    1024.00 |    6.359 |    0.000 |    0.003 |   0.002555 |   0.001278\n",
      "    27 |      0.184 |     82.128 |      0.200 |      1.573 |    1024.00 |    1024.00 |    5.965 |    0.000 |    0.003 |   0.002191 |   0.001095\n",
      "    28 |      0.165 |     71.904 |      0.190 |      1.688 |    1024.00 |    2048.00 |    6.068 |    0.000 |    0.003 |   0.001960 |   0.000980\n",
      "    29 |      0.143 |     97.300 |      0.215 |      1.900 |    2048.00 |    2048.00 |    6.208 |    0.000 |    0.003 |   0.001677 |   0.000839\n",
      "    30 |      0.125 |    105.049 |      0.223 |      2.085 |    2048.00 |    2048.00 |    6.165 |    0.000 |    0.003 |   0.001430 |   0.000715\n",
      "    31 |      0.113 |     93.512 |      0.212 |      2.243 |    2048.00 |    4096.00 |    5.435 |    0.000 |    0.003 |   0.001276 |   0.000638\n",
      "    32 |      0.098 |    120.409 |      0.238 |      2.533 |    4096.00 |    4096.00 |    5.842 |    0.000 |    0.003 |   0.001100 |   0.000550\n",
      "    33 |      0.083 |    147.833 |      0.266 |      2.780 |    4096.00 |    4096.00 |    5.746 |    0.000 |    0.003 |   0.000917 |   0.000459\n",
      "    34 |      0.074 |    138.937 |      0.257 |      2.981 |    4096.00 |    8192.00 |    5.266 |    0.000 |    0.003 |   0.000800 |   0.000400\n",
      "    35 |      0.063 |    173.504 |      0.292 |      3.337 |    8192.00 |    8192.00 |    5.362 |    0.000 |    0.003 |   0.000674 |   0.000337\n",
      "    36 |      0.053 |    195.413 |      0.313 |      3.637 |    8192.00 |    8192.00 |    4.818 |    0.000 |    0.003 |   0.000561 |   0.000281\n",
      "    37 |      0.048 |    180.228 |      0.298 |      3.889 |    8192.00 |   16384.00 |    4.894 |    0.000 |    0.003 |   0.000494 |   0.000247\n",
      "    38 |      0.041 |    211.655 |      0.330 |      4.352 |   16384.00 |   16384.00 |    4.780 |    0.000 |    0.003 |   0.000423 |   0.000211\n",
      "    39 |      0.035 |    234.331 |      0.352 |      4.752 |   16384.00 |   16384.00 |    4.641 |    0.000 |    0.003 |   0.000348 |   0.000174\n",
      "    40 |      0.030 |    227.351 |      0.345 |      5.078 |   16384.00 |   32768.00 |    4.361 |    0.000 |    0.003 |   0.000295 |   0.000147\n",
      "    41 |      0.025 |    286.516 |      0.405 |      5.634 |   32768.00 |   32768.00 |    4.568 |    0.000 |    0.003 |   0.000243 |   0.000121\n",
      "    42 |      0.021 |    324.327 |      0.442 |      6.090 |   32768.00 |   32768.00 |    4.537 |    0.000 |    0.003 |   0.000199 |   0.000100\n",
      "    43 |      0.019 |    301.966 |      0.420 |      6.470 |   32768.00 |   65536.00 |    4.029 |    0.000 |    0.003 |   0.000174 |   0.000087\n",
      "    44 |      0.016 |    365.616 |      0.484 |      7.157 |   65536.00 |   65536.00 |    4.064 |    0.000 |    0.003 |   0.000143 |   0.000071\n",
      "    45 |      0.013 |    416.569 |      0.535 |      7.718 |   65536.00 |   65536.00 |    3.976 |    0.000 |    0.003 |   0.000112 |   0.000056\n",
      "    46 |      0.011 |    378.373 |      0.496 |      8.170 |   65536.00 |  131072.00 |    3.719 |    0.000 |    0.003 |   0.000095 |   0.000048\n",
      "    47 |      0.010 |    428.605 |      0.547 |      8.994 |  131072.00 |  131072.00 |    3.792 |    0.000 |    0.003 |   0.000080 |   0.000040\n",
      "    48 |      0.008 |    450.123 |      0.568 |      9.726 |  131072.00 |          / |    3.482 |    0.000 |    0.003 |   0.000065 |   0.000032\n",
      "early stop!\n",
      "Terminated (optimal) in 49 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 4.92 minutes.\n",
      "\n",
      "Stage 1 variable selection finished. Elapsed time: 12.08 minutes.\n",
      "\n",
      "Stage 2: final theta estimation with Graph Laplacian Constrain using already estimated sigma^2 and gamma_g\n",
      "specified hyper-parameter for Graph Laplacian Constrain is: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "hyper-parameter for Graph Laplacian Constrain: use cross-validation to find the optimal value from 8 candidates...\n",
      "\n",
      "Start cross-validation for hyper-parameter lambda_g...\n",
      "still use ADMM even NO Graph Laplacian constrain (lambda_g=0)\n",
      "0%...11%...22%...33%...44%...56%...67%...78%...89%...early stop\n",
      "find optimal lambda_g 5.179 with average negative log-likelihood 46995.3970 by 5 fold cross-validation. Elapsed time: 59.05 minutes.\n",
      "\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |     15.574 |     14.739 |      0.134 |      0.134 |       1.00 |       1.00 |    9.538 |    0.000 |    0.007 |   0.135348 |   0.145475\n",
      "     1 |     12.535 |      9.141 |      0.131 |      0.143 |       1.00 |       1.00 |    3.804 |    0.000 |    0.007 |   0.173163 |   0.126456\n",
      "     2 |     10.383 |     10.707 |      0.129 |      0.152 |       1.00 |       1.00 |    3.980 |    0.000 |    0.007 |   0.125524 |   0.108012\n",
      "     3 |      9.002 |     12.022 |      0.130 |      0.160 |       1.00 |       2.00 |    3.712 |    0.000 |    0.007 |   0.097762 |   0.094289\n",
      "     4 |      8.328 |     18.667 |      0.137 |      0.176 |       2.00 |       2.00 |    4.176 |    0.000 |    0.007 |   0.088704 |   0.087751\n",
      "     5 |      7.221 |     20.261 |      0.138 |      0.190 |       2.00 |       2.00 |    4.230 |    0.000 |    0.007 |   0.082851 |   0.075816\n",
      "     6 |      6.148 |     22.019 |      0.140 |      0.202 |       2.00 |       4.00 |    4.146 |    0.000 |    0.007 |   0.064786 |   0.064996\n",
      "     7 |      5.566 |     32.387 |      0.150 |      0.224 |       4.00 |       4.00 |    4.634 |    0.000 |    0.007 |   0.059290 |   0.058186\n",
      "     8 |      4.553 |     34.313 |      0.152 |      0.241 |       4.00 |       4.00 |    4.338 |    0.000 |    0.007 |   0.053363 |   0.047083\n",
      "     9 |      3.539 |     36.496 |      0.155 |      0.255 |       4.00 |       8.00 |    4.242 |    0.000 |    0.007 |   0.036481 |   0.037015\n",
      "    10 |      3.022 |     50.126 |      0.168 |      0.278 |       8.00 |       8.00 |    4.609 |    0.000 |    0.007 |   0.031613 |   0.031078\n",
      "    11 |      2.224 |     52.184 |      0.170 |      0.295 |       8.00 |       8.00 |    4.252 |    0.000 |    0.007 |   0.026829 |   0.022483\n",
      "    12 |      1.468 |     54.216 |      0.172 |      0.306 |       8.00 |      16.00 |    3.810 |    0.000 |    0.007 |   0.014377 |   0.015154\n",
      "    13 |      1.194 |     68.538 |      0.187 |      0.323 |      16.00 |      16.00 |    4.342 |    0.000 |    0.006 |   0.011338 |   0.012008\n",
      "    14 |      0.773 |     69.974 |      0.188 |      0.333 |      16.00 |      16.00 |    3.935 |    0.000 |    0.006 |   0.009942 |   0.007491\n",
      "    15 |      0.383 |     71.262 |      0.189 |      0.338 |      16.00 |      32.00 |    3.305 |    0.000 |    0.006 |   0.003460 |   0.003814\n",
      "    16 |      0.344 |     83.191 |      0.201 |      0.346 |      32.00 |      32.00 |    3.417 |    0.000 |    0.005 |   0.002041 |   0.003193\n",
      "    17 |      0.211 |     83.787 |      0.202 |      0.350 |      32.00 |      32.00 |    3.312 |    0.000 |    0.005 |   0.002929 |   0.001806\n",
      "    18 |      0.076 |     84.387 |      0.202 |      0.351 |      32.00 |      64.00 |    2.555 |    0.000 |    0.005 |   0.000838 |   0.000633\n",
      "    19 |      0.101 |     92.800 |      0.211 |      0.353 |      64.00 |      64.00 |    2.471 |    0.000 |    0.005 |   0.000288 |   0.000753\n",
      "    20 |      0.060 |     92.954 |      0.211 |      0.355 |      64.00 |      64.00 |    3.015 |    0.000 |    0.005 |   0.000865 |   0.000460\n",
      "    21 |      0.018 |     93.161 |      0.211 |      0.355 |      64.00 |     128.00 |    2.252 |    0.000 |    0.005 |   0.000246 |   0.000154\n",
      "    22 |      0.030 |     98.375 |      0.216 |      0.357 |     128.00 |     128.00 |    2.016 |    0.000 |    0.004 |   0.000065 |   0.000210\n",
      "    23 |      0.018 |     98.404 |      0.216 |      0.358 |     128.00 |     128.00 |    2.749 |    0.000 |    0.004 |   0.000254 |   0.000130\n",
      "    24 |      0.006 |     98.459 |      0.217 |      0.358 |     128.00 |     256.00 |    2.219 |    0.000 |    0.004 |   0.000079 |   0.000043\n",
      "    25 |      0.008 |    101.398 |      0.219 |      0.359 |     256.00 |          / |    2.061 |    0.000 |    0.004 |   0.000023 |   0.000058\n",
      "early stop!\n",
      "Terminated (optimal) in 26 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 1.63 minutes.\n",
      "\n",
      "\n",
      "stage 2 finished. Elapsed time: 60.68 minutes.\n",
      "\n",
      "GLRM fitting finished. Elapsed time: 74.00 minutes.\n",
      "\n",
      "\n",
      "Post-processing estimated cell-type proportion theta...\n",
      "hard thresholding small theta values with threshold 0\n",
      "\n",
      "\n",
      "cell type deconvolution finished. Estimate results saved in /home/exouser/Spatial/celltype_proportions.csv. Elapsed time: 1.48 hours.\n",
      "\n",
      "\n",
      "######### No imputation #########\n",
      "\n",
      "\n",
      "whole pipeline finished. Total elapsed time: 1.48 hours.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='runDeconvolution -q sim_seq_based_spatial_spot_nUMI.csv                           -r scRNA_data_full.csv                           -c ref_scRNA_cell_celltype.csv                           -a sim_spatial_spot_adjacency_matrix.csv                           --n_marker_per_cmp 20                           -n 64                           --cvae_init_lr 0.003                           --num_hidden_layer 1                           --use_batch_norm false                           --cvae_train_epoch 1000                           --n_pseudo_spot 0\\n', returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = '''runDeconvolution -q sim_seq_based_spatial_spot_nUMI.csv \\\n",
    "                          -r scRNA_data_full.csv \\\n",
    "                          -c ref_scRNA_cell_celltype.csv \\\n",
    "                          -a sim_spatial_spot_adjacency_matrix.csv \\\n",
    "                          --n_marker_per_cmp 20 \\\n",
    "                          -n 64 \\\n",
    "                          --cvae_init_lr 0.003 \\\n",
    "                          --num_hidden_layer 1 \\\n",
    "                          --use_batch_norm false \\\n",
    "                          --cvae_train_epoch 1000 \\\n",
    "                          --n_pseudo_spot 0\n",
    "'''\n",
    "\n",
    "subprocess.run(cmd, check=True, text=True, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
