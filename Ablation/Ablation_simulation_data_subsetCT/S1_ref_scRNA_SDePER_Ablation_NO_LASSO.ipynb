{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db496b92",
   "metadata": {},
   "source": [
    "# Run Ablation Test on *SDePER* on simulated data with subset of cell types: Scenario 1 + scRNA-seq data as reference + WITHOUT sparsity penalty\n",
    "\n",
    "In this Notebook we run **ablation test** on SDePER on simulated data. For generating simulated data **with subset of cell types** via coarse-graining procedure please refer [generate_simulated_spatial_data_subsetCT.nb.html](https://rawcdn.githack.com/az7jh2/SDePER_Analysis/ab7b78abe53a4c625b71ce9eb5ab96bf2b829c5c/Simulation/Generate_simulation_data_subsetCT/generate_simulated_spatial_data_subsetCT.nb.html).\n",
    "\n",
    "**Scenario 1** means the reference data for deconvolution includes all single cells with the **matched 5 cell types**.\n",
    "\n",
    "**scRNA-seq data as reference** means the reference data is scRNA-seq data ([GSE115746](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115746)) from the same tissue with simulated spatial data, therefore **platform effect exists**.\n",
    "\n",
    "**WITHOUT sparsity penalty** means we fit the graph Laplacian regularized model without the sparsity penalty, specifically omitting Adaptive LASSO regularization (by setting the command option `--lambda_r` to 0).\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "So here we use the **4 input files** as shown below:\n",
    "\n",
    "1. raw nUMI counts of simulated spatial transcriptomic data **with subset of cell types** (spots × genes): [sim_subsetCT_spatial_spot_nUMI.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Generate_simulation_data_subsetCT/sim_subsetCT_spatial_spot_nUMI.csv)\n",
    "2. raw nUMI counts of reference scRNA-seq data (cells × genes): `scRNA_data_full.csv`. Since the file size of csv file of raw nUMI matrix of all 23,178 cells and 45,768 genes is up to 2.3 GB, we do not provide this file in our repository. It's just a **matrix transpose** of [GSE115746_cells_exon_counts.csv.gz](https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE115746&format=file&file=GSE115746%5Fcells%5Fexon%5Fcounts%2Ecsv%2Egz) in [GSE115746](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115746) to satisty the file format requirement that rows as cells and columns as genes\n",
    "3. cell type annotations for cells of **selected 5 cell types** in reference scRNA-seq data (cells × 1): [ref_scRNA_subsetCT_cell_celltype.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Run_SDePER_on_simulation_data_subsetCT/Scenario_1/ref_scRNA_seq/ref_scRNA_subsetCT_cell_celltype.csv)\n",
    "4. adjacency matrix of spots in simulated spatial transcriptomic data **with subset of cell types** (spots × spots): [sim_subsetCT_spatial_spot_adjacency_matrix.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Generate_simulation_data_subsetCT/sim_subsetCT_spatial_spot_adjacency_matrix.csv)\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "SDePER settings are the same as baseline run [S1_ref_scRNA_SDePER_WITH_CVAE.ipynb](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Run_SDePER_on_simulation_data_subsetCT/Scenario_1/ref_scRNA_seq/S1_ref_spatial_SDePER_WITH_CVAE.ipynb):\n",
    "\n",
    "* number of included highly variable genes `n_hv_gene`: 500\n",
    "* number of selected TOP marker genes for each comparison in Differential `n_marker_per_cmp`: 50\n",
    "* seed for random values `seed`: 2\n",
    "* number of used CPU cores `n_core`: 64\n",
    "\n",
    "ALL other options are left as default.\n",
    "\n",
    "**For ablation test, set hyper-parameter for Adaptive Lasso `lambda_r` as 0**.\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "the `bash` command to start cell type deconvolution is\n",
    "\n",
    "`runDeconvolution -q sim_subsetCT_spatial_spot_nUMI.csv -r scRNA_data_full.csv -c ref_scRNA_subsetCT_cell_celltype.csv -a sim_subsetCT_spatial_spot_adjacency_matrix.csv --n_hv_gene 500 --n_marker_per_cmp 50 --seed 2 -n 64 --lambda_r 0`\n",
    "\n",
    "Note this Notebook uses **SDePER v1.0.0**. Cell type deconvolution result is renamed as [S1_ref_scRNA_SDePER_Ablation_NO_LASSO_celltype_proportions.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Ablation/Ablation_simulation_data_subsetCT/S1_ref_scRNA_SDePER_Ablation_NO_LASSO_celltype_proportions.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9e3939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SDePER (Spatial Deconvolution method with Platform Effect Removal) v1.0.0\n",
      "\n",
      "\n",
      "running options:\n",
      "spatial_file: /home/exouser/Spatial/sim_subsetCT_spatial_spot_nUMI.csv\n",
      "ref_file: /home/exouser/Spatial/scRNA_data_full.csv\n",
      "ref_celltype_file: /home/exouser/Spatial/ref_scRNA_subsetCT_cell_celltype.csv\n",
      "marker_file: None\n",
      "loc_file: None\n",
      "A_file: /home/exouser/Spatial/sim_subsetCT_spatial_spot_adjacency_matrix.csv\n",
      "n_cores: 64\n",
      "lambda_r: 0.0\n",
      "lambda_g: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "use_cvae: True\n",
      "threshold: 0\n",
      "n_hv_gene: 500\n",
      "n_marker_per_cmp: 50\n",
      "pseudo_spot_min_cell: 2\n",
      "pseudo_spot_max_cell: 8\n",
      "seq_depth_scaler: 10000\n",
      "cvae_input_scaler: 10\n",
      "cvae_init_lr: 0.003\n",
      "redo_de: True\n",
      "seed: 2\n",
      "diagnosis: False\n",
      "verbose: True\n",
      "use_imputation: False\n",
      "diameter: 200\n",
      "impute_diameter: [160, 114, 80]\n",
      "\n",
      "\n",
      "######### Preprocessing... #########\n",
      "\n",
      "######### First build CVAE... #########\n",
      "\n",
      "read spatial data from file /home/exouser/Spatial/sim_subsetCT_spatial_spot_nUMI.csv\n",
      "total 502 spots; 1020 genes\n",
      "\n",
      "read scRNA-seq data from file /home/exouser/Spatial/scRNA_data_full.csv\n",
      "total 23178 cells; 45768 genes\n",
      "read scRNA-seq cell-type annotation from file /home/exouser/Spatial/ref_scRNA_subsetCT_cell_celltype.csv\n",
      "total 5 cell-types\n",
      "subset cells with cell-type annotation, finally keep 6831 cells; 45768 genes\n",
      "\n",
      "total 995 overlapped genes\n",
      "\n",
      "identify 500 highly variable genes from scRNA-seq data...\n",
      "\n",
      "identify cell-type marker genes...\n",
      "no marker gene profile provided. Perform DE to get cell-type marker genes on scRNA-seq data...\n",
      "\n",
      "Differential analysis across cell-types on scRNA-seq data...\n",
      "finally selected 326 cell-type marker genes\n",
      "\n",
      "\n",
      "use union of highly variable gene list and cell-type marker gene list derived from scRNA-seq data, finally get 587 genes for downstream analysis\n",
      "\n",
      "start CVAE building...\n",
      "\n",
      "generate pseudo-spots containing 2 to 8 cells from scRNA-seq cells...\n",
      "generate 200800 pseudo-spots for training and 50200 pseudo-spots for validation\n",
      "scaling inputs to range 0 to 10\n",
      "in training -- spatial spots : pseudo-spots = 502 : 207631\n",
      "\n",
      "Start training...\n",
      "\n",
      "Train on 208133 samples, validate on 50200 samples\n",
      "Epoch 1/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 581.2446 - val_loss: 366.0523 - lr: 0.0030\n",
      "Epoch 2/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 546.8146 - val_loss: 343.6681 - lr: 0.0030\n",
      "Epoch 3/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 514.5396 - val_loss: 306.3656 - lr: 0.0030\n",
      "Epoch 4/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 472.9413 - val_loss: 275.2516 - lr: 0.0030\n",
      "Epoch 5/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 438.9370 - val_loss: 249.4489 - lr: 0.0030\n",
      "Epoch 6/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 408.8046 - val_loss: 223.7682 - lr: 0.0030\n",
      "Epoch 7/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 378.0183 - val_loss: 204.6430 - lr: 0.0030\n",
      "Epoch 8/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 355.2249 - val_loss: 191.1544 - lr: 0.0030\n",
      "Epoch 9/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 339.5063 - val_loss: 179.3178 - lr: 0.0030\n",
      "Epoch 10/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 325.5657 - val_loss: 169.2994 - lr: 0.0030\n",
      "Epoch 11/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 313.0959 - val_loss: 161.5672 - lr: 0.0030\n",
      "Epoch 12/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 302.8929 - val_loss: 154.2728 - lr: 0.0030\n",
      "Epoch 13/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 293.4313 - val_loss: 147.9110 - lr: 0.0030\n",
      "Epoch 14/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 284.9255 - val_loss: 143.0920 - lr: 0.0030\n",
      "Epoch 15/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 278.7922 - val_loss: 137.9804 - lr: 0.0030\n",
      "Epoch 16/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 272.1497 - val_loss: 136.4719 - lr: 0.0030\n",
      "Epoch 17/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 269.1786 - val_loss: 131.8595 - lr: 0.0030\n",
      "Epoch 18/1000\n",
      "208133/208133 [==============================] - 3s 16us/sample - loss: 263.9747 - val_loss: 130.5828 - lr: 0.0030\n",
      "Epoch 19/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 262.0625 - val_loss: 129.4494 - lr: 0.0030\n",
      "Epoch 20/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 260.3896 - val_loss: 124.9529 - lr: 0.0030\n",
      "Epoch 21/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 254.9629 - val_loss: 125.2468 - lr: 0.0030\n",
      "Epoch 22/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 254.5140 - val_loss: 123.0422 - lr: 0.0030\n",
      "Epoch 23/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 251.6982 - val_loss: 121.7056 - lr: 0.0030\n",
      "Epoch 24/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 249.9397 - val_loss: 121.0548 - lr: 0.0030\n",
      "Epoch 25/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 248.7177 - val_loss: 118.6449 - lr: 0.0030\n",
      "Epoch 26/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 245.7587 - val_loss: 118.0604 - lr: 0.0030\n",
      "Epoch 27/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 244.7257 - val_loss: 116.2785 - lr: 0.0030\n",
      "Epoch 28/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 242.4097 - val_loss: 115.3829 - lr: 0.0030\n",
      "Epoch 29/1000\n",
      "208133/208133 [==============================] - 3s 16us/sample - loss: 241.1435 - val_loss: 114.4630 - lr: 0.0030\n",
      "Epoch 30/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 239.8465 - val_loss: 113.1848 - lr: 0.0030\n",
      "Epoch 31/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 238.0456 - val_loss: 112.4854 - lr: 0.0030\n",
      "Epoch 32/1000\n",
      "208133/208133 [==============================] - 2s 10us/sample - loss: 237.0516 - val_loss: 110.9997 - lr: 0.0030\n",
      "Epoch 33/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 235.2443 - val_loss: 111.0387 - lr: 0.0030\n",
      "Epoch 34/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 235.0703 - val_loss: 109.8004 - lr: 0.0030\n",
      "Epoch 35/1000\n",
      "208133/208133 [==============================] - 2s 11us/sample - loss: 233.3153 - val_loss: 109.7791 - lr: 0.0030\n",
      "Epoch 36/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 233.0686 - val_loss: 109.4218 - lr: 0.0030\n",
      "Epoch 37/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 232.6406 - val_loss: 108.1883 - lr: 0.0030\n",
      "Epoch 38/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 231.0780 - val_loss: 108.0281 - lr: 0.0030\n",
      "Epoch 39/1000\n",
      "208133/208133 [==============================] - 2s 10us/sample - loss: 230.7503 - val_loss: 107.4053 - lr: 0.0030\n",
      "Epoch 40/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 229.9214 - val_loss: 106.7732 - lr: 0.0030\n",
      "Epoch 41/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 229.0718 - val_loss: 106.0665 - lr: 0.0030\n",
      "Epoch 42/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 228.1978 - val_loss: 106.0826 - lr: 0.0030\n",
      "Epoch 43/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 228.0030 - val_loss: 105.5841 - lr: 0.0030\n",
      "Epoch 44/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 227.1557 - val_loss: 104.6985 - lr: 0.0030\n",
      "Epoch 45/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 226.1142 - val_loss: 104.9115 - lr: 0.0030\n",
      "Epoch 46/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 226.1133 - val_loss: 103.8923 - lr: 0.0030\n",
      "Epoch 47/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 224.8104 - val_loss: 104.3604 - lr: 0.0030\n",
      "Epoch 48/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 225.1640 - val_loss: 104.3482 - lr: 0.0030\n",
      "Epoch 49/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208133/208133 [==============================] - 4s 21us/sample - loss: 224.9200 - val_loss: 102.9591 - lr: 0.0030\n",
      "Epoch 50/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 223.1726 - val_loss: 102.8039 - lr: 0.0030\n",
      "Epoch 51/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 222.8904 - val_loss: 102.6342 - lr: 0.0030\n",
      "Epoch 52/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 222.6080 - val_loss: 101.4610 - lr: 0.0030\n",
      "Epoch 53/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 221.2181 - val_loss: 102.7518 - lr: 0.0030\n",
      "Epoch 54/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 222.5054 - val_loss: 102.5404 - lr: 0.0030\n",
      "Epoch 55/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 222.1001 - val_loss: 100.6587 - lr: 0.0030\n",
      "Epoch 56/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 219.9105 - val_loss: 100.9915 - lr: 0.0030\n",
      "Epoch 57/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 220.2343 - val_loss: 100.6119 - lr: 0.0030\n",
      "Epoch 58/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 219.5530 - val_loss: 99.6743 - lr: 0.0030\n",
      "Epoch 59/1000\n",
      "208133/208133 [==============================] - 2s 12us/sample - loss: 218.4049 - val_loss: 99.8197 - lr: 0.0030\n",
      "Epoch 60/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 218.4310 - val_loss: 99.2281 - lr: 0.0030\n",
      "Epoch 61/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 217.6709 - val_loss: 98.7481 - lr: 0.0030\n",
      "Epoch 62/1000\n",
      "208133/208133 [==============================] - 3s 13us/sample - loss: 216.9816 - val_loss: 98.5141 - lr: 0.0030\n",
      "Epoch 63/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 216.7679 - val_loss: 97.7280 - lr: 0.0030\n",
      "Epoch 64/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 215.7905 - val_loss: 97.7843 - lr: 0.0030\n",
      "Epoch 65/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 215.7287 - val_loss: 97.1124 - lr: 0.0030\n",
      "Epoch 66/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 214.9125 - val_loss: 97.0503 - lr: 0.0030\n",
      "Epoch 67/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 214.7503 - val_loss: 96.5832 - lr: 0.0030\n",
      "Epoch 68/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 214.3064 - val_loss: 96.0950 - lr: 0.0030\n",
      "Epoch 69/1000\n",
      "208133/208133 [==============================] - 5s 23us/sample - loss: 213.6009 - val_loss: 95.8864 - lr: 0.0030\n",
      "Epoch 70/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 213.3127 - val_loss: 95.3405 - lr: 0.0030\n",
      "Epoch 71/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 212.5650 - val_loss: 95.3981 - lr: 0.0030\n",
      "Epoch 72/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 212.5645 - val_loss: 94.8059 - lr: 0.0030\n",
      "Epoch 73/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 211.7415 - val_loss: 94.9519 - lr: 0.0030\n",
      "Epoch 74/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 211.8149 - val_loss: 94.5850 - lr: 0.0030\n",
      "Epoch 75/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 211.3276 - val_loss: 94.0421 - lr: 0.0030\n",
      "Epoch 76/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 210.5095 - val_loss: 93.9204 - lr: 0.0030\n",
      "Epoch 77/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 210.2424 - val_loss: 93.6301 - lr: 0.0030\n",
      "Epoch 78/1000\n",
      "208133/208133 [==============================] - 3s 12us/sample - loss: 209.8378 - val_loss: 93.1825 - lr: 0.0030\n",
      "Epoch 79/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 209.3301 - val_loss: 92.9775 - lr: 0.0030\n",
      "Epoch 80/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 208.9092 - val_loss: 93.0033 - lr: 0.0030\n",
      "Epoch 81/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 208.8912 - val_loss: 92.5401 - lr: 0.0030\n",
      "Epoch 82/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 208.2623 - val_loss: 92.8332 - lr: 0.0030\n",
      "Epoch 83/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 208.5151 - val_loss: 92.5792 - lr: 0.0030\n",
      "Epoch 84/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 208.2004 - val_loss: 92.0327 - lr: 0.0030\n",
      "Epoch 85/1000\n",
      "208133/208133 [==============================] - 4s 19us/sample - loss: 207.4524 - val_loss: 92.2042 - lr: 0.0030\n",
      "Epoch 86/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 207.5493 - val_loss: 91.7221 - lr: 0.0030\n",
      "Epoch 87/1000\n",
      "208133/208133 [==============================] - 3s 16us/sample - loss: 206.8374 - val_loss: 92.1353 - lr: 0.0030\n",
      "Epoch 88/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 207.2153 - val_loss: 92.0627 - lr: 0.0030\n",
      "Epoch 89/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 207.1606 - val_loss: 91.3995 - lr: 0.0030\n",
      "Epoch 90/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 206.1350 - val_loss: 91.4451 - lr: 0.0030\n",
      "Epoch 91/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 206.1793 - val_loss: 91.4765 - lr: 0.0030\n",
      "Epoch 92/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 206.1169 - val_loss: 90.7900 - lr: 0.0030\n",
      "Epoch 93/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 205.1416 - val_loss: 91.7125 - lr: 0.0030\n",
      "Epoch 94/1000\n",
      "208133/208133 [==============================] - 2s 10us/sample - loss: 206.0804 - val_loss: 91.2498 - lr: 0.0030\n",
      "Epoch 95/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 205.4726 - val_loss: 91.3580 - lr: 0.0030\n",
      "Epoch 96/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 205.5139 - val_loss: 91.1049 - lr: 0.0030\n",
      "Epoch 97/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 205.0279 - val_loss: 91.2188 - lr: 0.0030\n",
      "Epoch 98/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 205.3045 - val_loss: 90.8274 - lr: 0.0030\n",
      "Epoch 99/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 204.6416 - val_loss: 90.9561 - lr: 0.0030\n",
      "Epoch 100/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 204.7964 - val_loss: 90.4872 - lr: 0.0030\n",
      "Epoch 101/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 204.1974 - val_loss: 91.4081 - lr: 0.0030\n",
      "Epoch 102/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 205.0512 - val_loss: 90.9788 - lr: 0.0030\n",
      "Epoch 103/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 204.6783 - val_loss: 90.0909 - lr: 0.0030\n",
      "Epoch 104/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 203.3795 - val_loss: 90.2574 - lr: 0.0030\n",
      "Epoch 105/1000\n",
      "208133/208133 [==============================] - 13s 61us/sample - loss: 203.4117 - val_loss: 89.8770 - lr: 0.0030\n",
      "Epoch 106/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 202.9395 - val_loss: 89.5724 - lr: 0.0030\n",
      "Epoch 107/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 202.4964 - val_loss: 90.3917 - lr: 0.0030\n",
      "Epoch 108/1000\n",
      "208133/208133 [==============================] - 1s 6us/sample - loss: 203.4520 - val_loss: 89.9362 - lr: 0.0030\n",
      "Epoch 109/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 202.6914 - val_loss: 89.9541 - lr: 0.0030\n",
      "Epoch 110/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 202.9644 - val_loss: 89.9345 - lr: 0.0030\n",
      "Epoch 111/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 202.7961 - val_loss: 89.5552 - lr: 0.0030\n",
      "Epoch 112/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 201.9595 - val_loss: 89.4350 - lr: 0.0030\n",
      "Epoch 113/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 201.9188 - val_loss: 89.6736 - lr: 0.0030\n",
      "Epoch 114/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208133/208133 [==============================] - 2s 9us/sample - loss: 201.9153 - val_loss: 89.2573 - lr: 0.0030\n",
      "Epoch 115/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 201.4188 - val_loss: 89.7136 - lr: 0.0030\n",
      "Epoch 116/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 201.9841 - val_loss: 89.5695 - lr: 0.0030\n",
      "Epoch 117/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 201.6106 - val_loss: 89.1123 - lr: 0.0030\n",
      "Epoch 118/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 201.2051 - val_loss: 88.9555 - lr: 0.0030\n",
      "Epoch 119/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 200.8956 - val_loss: 89.2981 - lr: 0.0030\n",
      "Epoch 120/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 201.0734 - val_loss: 88.9737 - lr: 0.0030\n",
      "Epoch 121/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 200.6777 - val_loss: 89.1263 - lr: 0.0030\n",
      "Epoch 122/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 200.6508 - val_loss: 89.0371 - lr: 0.0030\n",
      "Epoch 123/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 200.5228 - val_loss: 88.7586 - lr: 0.0030\n",
      "Epoch 124/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 200.0697 - val_loss: 88.6915 - lr: 0.0030\n",
      "Epoch 125/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 199.8795 - val_loss: 88.6287 - lr: 0.0030\n",
      "Epoch 126/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 199.8658 - val_loss: 88.4720 - lr: 0.0030\n",
      "Epoch 127/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 199.5019 - val_loss: 88.7778 - lr: 0.0030\n",
      "Epoch 128/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 199.8567 - val_loss: 88.6133 - lr: 0.0030\n",
      "Epoch 129/1000\n",
      "208133/208133 [==============================] - 2s 10us/sample - loss: 199.5742 - val_loss: 88.5181 - lr: 0.0030\n",
      "Epoch 130/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 199.3103 - val_loss: 88.5040 - lr: 0.0030\n",
      "Epoch 131/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 199.1481 - val_loss: 88.2691 - lr: 0.0030\n",
      "Epoch 132/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 198.8760 - val_loss: 88.1429 - lr: 0.0030\n",
      "Epoch 133/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 198.6322 - val_loss: 88.1992 - lr: 0.0030\n",
      "Epoch 134/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 198.6923 - val_loss: 88.0259 - lr: 0.0030\n",
      "Epoch 135/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 198.3381 - val_loss: 88.2543 - lr: 0.0030\n",
      "Epoch 136/1000\n",
      "208133/208133 [==============================] - 6s 27us/sample - loss: 198.5003 - val_loss: 88.0906 - lr: 0.0030\n",
      "Epoch 137/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 198.2486 - val_loss: 87.9063 - lr: 0.0030\n",
      "Epoch 138/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 197.8598 - val_loss: 87.8265 - lr: 0.0030\n",
      "Epoch 139/1000\n",
      "208133/208133 [==============================] - 5s 25us/sample - loss: 197.6116 - val_loss: 87.8411 - lr: 0.0030\n",
      "Epoch 140/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 197.6245 - val_loss: 87.6812 - lr: 0.0030\n",
      "Epoch 141/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 197.3322 - val_loss: 87.7404 - lr: 0.0030\n",
      "Epoch 142/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 197.3792 - val_loss: 87.5923 - lr: 0.0030\n",
      "Epoch 143/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 196.9830 - val_loss: 87.5001 - lr: 0.0030\n",
      "Epoch 144/1000\n",
      "208133/208133 [==============================] - 4s 19us/sample - loss: 196.9397 - val_loss: 87.4117 - lr: 0.0030\n",
      "Epoch 145/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 196.7028 - val_loss: 87.4253 - lr: 0.0030\n",
      "Epoch 146/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 196.6099 - val_loss: 87.3012 - lr: 0.0030\n",
      "Epoch 147/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 196.3132 - val_loss: 87.3592 - lr: 0.0030\n",
      "Epoch 148/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 196.2763 - val_loss: 87.1880 - lr: 0.0030\n",
      "Epoch 149/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 196.0404 - val_loss: 87.1890 - lr: 0.0030\n",
      "Epoch 150/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 195.9969 - val_loss: 87.0480 - lr: 0.0030\n",
      "Epoch 151/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 195.5875 - val_loss: 87.0450 - lr: 0.0030\n",
      "Epoch 152/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 195.5952 - val_loss: 86.9073 - lr: 0.0030\n",
      "Epoch 153/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 195.3175 - val_loss: 86.9909 - lr: 0.0030\n",
      "Epoch 154/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 195.2922 - val_loss: 86.8315 - lr: 0.0030\n",
      "Epoch 155/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 195.0172 - val_loss: 86.8276 - lr: 0.0030\n",
      "Epoch 156/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 194.8743 - val_loss: 86.7062 - lr: 0.0030\n",
      "Epoch 157/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 194.6703 - val_loss: 86.6311 - lr: 0.0030\n",
      "Epoch 158/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 194.4222 - val_loss: 86.5232 - lr: 0.0030\n",
      "Epoch 159/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 194.1827 - val_loss: 86.5982 - lr: 0.0030\n",
      "Epoch 160/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 194.2570 - val_loss: 86.4865 - lr: 0.0030\n",
      "Epoch 161/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 193.9099 - val_loss: 86.5017 - lr: 0.0030\n",
      "Epoch 162/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 193.8532 - val_loss: 86.3988 - lr: 0.0030\n",
      "Epoch 163/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 193.6931 - val_loss: 86.2377 - lr: 0.0030\n",
      "Epoch 164/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 193.3071 - val_loss: 86.1508 - lr: 0.0030\n",
      "Epoch 165/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 193.1193 - val_loss: 86.3530 - lr: 0.0030\n",
      "Epoch 166/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 193.3011 - val_loss: 86.2979 - lr: 0.0030\n",
      "Epoch 167/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 193.0566 - val_loss: 85.9015 - lr: 0.0030\n",
      "Epoch 168/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 192.5145 - val_loss: 85.8466 - lr: 0.0030\n",
      "Epoch 169/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 192.3421 - val_loss: 86.1600 - lr: 0.0030\n",
      "Epoch 170/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 192.6160 - val_loss: 86.0470 - lr: 0.0030\n",
      "Epoch 171/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 192.4176 - val_loss: 85.7114 - lr: 0.0030\n",
      "Epoch 172/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 191.7695 - val_loss: 85.6341 - lr: 0.0030\n",
      "Epoch 173/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 191.6420 - val_loss: 85.8828 - lr: 0.0030\n",
      "Epoch 174/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 191.8298 - val_loss: 85.7700 - lr: 0.0030\n",
      "Epoch 175/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 191.5941 - val_loss: 85.6293 - lr: 0.0030\n",
      "Epoch 176/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 191.2392 - val_loss: 85.6765 - lr: 0.0030\n",
      "Epoch 177/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 191.3298 - val_loss: 85.3527 - lr: 0.0030\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208133/208133 [==============================] - 2s 9us/sample - loss: 190.6953 - val_loss: 85.7229 - lr: 0.0030\n",
      "Epoch 179/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 191.0672 - val_loss: 85.4506 - lr: 0.0030\n",
      "Epoch 180/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 190.6185 - val_loss: 85.4162 - lr: 0.0030\n",
      "Epoch 181/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 190.4425 - val_loss: 85.3994 - lr: 0.0030\n",
      "Epoch 182/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 190.2994 - val_loss: 85.2252 - lr: 0.0030\n",
      "Epoch 183/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 189.9550 - val_loss: 85.4325 - lr: 0.0030\n",
      "Epoch 184/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 190.1222 - val_loss: 85.0478 - lr: 0.0030\n",
      "Epoch 185/1000\n",
      "208133/208133 [==============================] - 4s 19us/sample - loss: 189.4854 - val_loss: 85.3888 - lr: 0.0030\n",
      "Epoch 186/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 189.7264 - val_loss: 85.0672 - lr: 0.0030\n",
      "Epoch 187/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 189.3220 - val_loss: 85.4277 - lr: 0.0030\n",
      "Epoch 188/1000\n",
      "208133/208133 [==============================] - 4s 17us/sample - loss: 189.6815 - val_loss: 85.2417 - lr: 0.0030\n",
      "Epoch 189/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 189.3784 - val_loss: 85.1773 - lr: 0.0030\n",
      "Epoch 190/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 189.0936 - val_loss: 85.1903 - lr: 0.0030\n",
      "Epoch 191/1000\n",
      "208133/208133 [==============================] - 4s 17us/sample - loss: 189.0435 - val_loss: 84.9912 - lr: 0.0030\n",
      "Epoch 192/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 188.5689 - val_loss: 85.1373 - lr: 0.0030\n",
      "Epoch 193/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 188.7436 - val_loss: 84.9379 - lr: 0.0030\n",
      "Epoch 194/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 188.4008 - val_loss: 84.9568 - lr: 0.0030\n",
      "Epoch 195/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 188.2773 - val_loss: 84.8272 - lr: 0.0030\n",
      "Epoch 196/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 187.9762 - val_loss: 84.9981 - lr: 0.0030\n",
      "Epoch 197/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 188.1490 - val_loss: 84.9342 - lr: 0.0030\n",
      "Epoch 198/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 187.8419 - val_loss: 84.6987 - lr: 0.0030\n",
      "Epoch 199/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 187.5342 - val_loss: 84.7766 - lr: 0.0030\n",
      "Epoch 200/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 187.6167 - val_loss: 84.5479 - lr: 0.0030\n",
      "Epoch 201/1000\n",
      "208133/208133 [==============================] - 3s 14us/sample - loss: 187.1013 - val_loss: 85.1467 - lr: 0.0030\n",
      "Epoch 202/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 187.7812 - val_loss: 84.9104 - lr: 0.0030\n",
      "Epoch 203/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 187.5183 - val_loss: 84.8681 - lr: 0.0030\n",
      "Epoch 204/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 187.1007 - val_loss: 84.9572 - lr: 0.0030\n",
      "Epoch 205/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 187.1851 - val_loss: 84.7918 - lr: 0.0030\n",
      "Epoch 206/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 186.8908 - val_loss: 84.5272 - lr: 0.0030\n",
      "Epoch 207/1000\n",
      "208133/208133 [==============================] - 2s 10us/sample - loss: 186.4371 - val_loss: 85.0910 - lr: 0.0030\n",
      "Epoch 208/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 187.1351 - val_loss: 84.6270 - lr: 0.0030\n",
      "Epoch 209/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 186.4123 - val_loss: 84.9440 - lr: 0.0030\n",
      "Epoch 210/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 186.7113 - val_loss: 84.9022 - lr: 0.0030\n",
      "Epoch 211/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 186.7190 - val_loss: 84.8209 - lr: 0.0030\n",
      "Epoch 212/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 186.4535 - val_loss: 84.6367 - lr: 0.0030\n",
      "Epoch 213/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 186.0624 - val_loss: 84.7035 - lr: 0.0030\n",
      "Epoch 214/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 186.0902 - val_loss: 84.5779 - lr: 0.0030\n",
      "Epoch 215/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 185.8651 - val_loss: 84.2971 - lr: 0.0030\n",
      "Epoch 216/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 185.3239 - val_loss: 84.9291 - lr: 0.0030\n",
      "Epoch 217/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 186.1383 - val_loss: 84.2573 - lr: 0.0030\n",
      "Epoch 218/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 185.1010 - val_loss: 85.6297 - lr: 0.0030\n",
      "Epoch 219/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 186.9182 - val_loss: 85.5735 - lr: 0.0030\n",
      "Epoch 220/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 186.8621 - val_loss: 84.5601 - lr: 0.0030\n",
      "Epoch 221/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 185.3206 - val_loss: 84.6470 - lr: 0.0030\n",
      "Epoch 222/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 185.3623 - val_loss: 84.5472 - lr: 0.0030\n",
      "Epoch 223/1000\n",
      "208133/208133 [==============================] - 4s 19us/sample - loss: 185.1759 - val_loss: 84.4347 - lr: 0.0030\n",
      "Epoch 224/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 184.9240 - val_loss: 84.4068 - lr: 0.0030\n",
      "Epoch 225/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 184.7799 - val_loss: 84.2817 - lr: 0.0030\n",
      "Epoch 226/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 184.5050 - val_loss: 84.4161 - lr: 0.0030\n",
      "Epoch 227/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 184.7302 - val_loss: 84.1858 - lr: 0.0030\n",
      "Epoch 228/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 184.2410 - val_loss: 84.7542 - lr: 0.0030\n",
      "Epoch 229/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 184.9661 - val_loss: 84.4613 - lr: 0.0030\n",
      "Epoch 230/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 184.4469 - val_loss: 84.6215 - lr: 0.0030\n",
      "Epoch 231/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 184.7233 - val_loss: 84.5578 - lr: 0.0030\n",
      "Epoch 232/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 184.6656 - val_loss: 84.3977 - lr: 0.0030\n",
      "Epoch 233/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 184.1326 - val_loss: 84.3378 - lr: 0.0030\n",
      "Epoch 234/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 184.0465 - val_loss: 84.4322 - lr: 0.0030\n",
      "Epoch 235/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 184.0707 - val_loss: 84.0158 - lr: 0.0030\n",
      "Epoch 236/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 183.4714 - val_loss: 84.6548 - lr: 0.0030\n",
      "Epoch 237/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 184.2480 - val_loss: 84.5697 - lr: 0.0030\n",
      "Epoch 238/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 184.0816 - val_loss: 83.9985 - lr: 0.0030\n",
      "Epoch 239/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 183.1163 - val_loss: 84.1770 - lr: 0.0030\n",
      "Epoch 240/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 183.4105 - val_loss: 84.1021 - lr: 0.0030\n",
      "Epoch 241/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 183.1528 - val_loss: 84.0965 - lr: 0.0030\n",
      "Epoch 242/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208133/208133 [==============================] - 2s 9us/sample - loss: 182.9571 - val_loss: 84.0603 - lr: 0.0030\n",
      "Epoch 243/1000\n",
      "208133/208133 [==============================] - 3s 14us/sample - loss: 182.9142 - val_loss: 83.8429 - lr: 0.0030\n",
      "Epoch 244/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 182.5152 - val_loss: 84.1322 - lr: 0.0030\n",
      "Epoch 245/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 182.6873 - val_loss: 83.9140 - lr: 0.0030\n",
      "Epoch 246/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 182.2937 - val_loss: 84.1921 - lr: 0.0030\n",
      "Epoch 247/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 182.4494 - val_loss: 84.0420 - lr: 0.0030\n",
      "Epoch 248/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 182.2444 - val_loss: 84.0604 - lr: 0.0030\n",
      "Epoch 249/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 182.0903 - val_loss: 83.9719 - lr: 0.0030\n",
      "Epoch 250/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 181.8850 - val_loss: 84.1614 - lr: 0.0030\n",
      "Epoch 251/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 182.1588 - val_loss: 83.8491 - lr: 0.0030\n",
      "Epoch 252/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 181.5929 - val_loss: 84.0602 - lr: 0.0030\n",
      "Epoch 253/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 181.9020 - val_loss: 83.9293 - lr: 0.0030\n",
      "Epoch 254/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 181.4631 - val_loss: 84.0864 - lr: 0.0030\n",
      "Epoch 255/1000\n",
      "208133/208133 [==============================] - 1s 6us/sample - loss: 181.5677 - val_loss: 83.9225 - lr: 0.0030\n",
      "Epoch 256/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 181.3921 - val_loss: 83.9500 - lr: 0.0030\n",
      "Epoch 257/1000\n",
      "208133/208133 [==============================] - 3s 14us/sample - loss: 181.2968 - val_loss: 83.9967 - lr: 0.0030\n",
      "Epoch 258/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 181.2268 - val_loss: 83.7449 - lr: 0.0030\n",
      "Epoch 259/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 180.7739 - val_loss: 83.7096 - lr: 0.0030\n",
      "Epoch 260/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 180.6561 - val_loss: 83.7612 - lr: 0.0030\n",
      "Epoch 261/1000\n",
      "208133/208133 [==============================] - 2s 10us/sample - loss: 180.7389 - val_loss: 83.6380 - lr: 0.0030\n",
      "Epoch 262/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 180.4395 - val_loss: 83.8318 - lr: 0.0030\n",
      "Epoch 263/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 180.5738 - val_loss: 83.7041 - lr: 0.0030\n",
      "Epoch 264/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 180.3208 - val_loss: 83.7585 - lr: 0.0030\n",
      "Epoch 265/1000\n",
      "208133/208133 [==============================] - 3s 14us/sample - loss: 180.2663 - val_loss: 83.5362 - lr: 0.0030\n",
      "Epoch 266/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 179.9624 - val_loss: 84.3068 - lr: 0.0030\n",
      "Epoch 267/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 180.8787 - val_loss: 84.1660 - lr: 0.0030\n",
      "Epoch 268/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 180.6108 - val_loss: 83.6555 - lr: 0.0030\n",
      "Epoch 269/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 179.8395 - val_loss: 83.8801 - lr: 0.0030\n",
      "Epoch 270/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 180.1116 - val_loss: 83.6157 - lr: 0.0030\n",
      "Epoch 271/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 179.7051 - val_loss: 83.6408 - lr: 0.0030\n",
      "Epoch 272/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 179.5298 - val_loss: 83.6149 - lr: 0.0030\n",
      "Epoch 273/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 179.5682 - val_loss: 83.6024 - lr: 0.0030\n",
      "Epoch 274/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 179.3644 - val_loss: 83.5434 - lr: 0.0030\n",
      "Epoch 275/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 179.2231 - val_loss: 83.6754 - lr: 0.0030\n",
      "Epoch 276/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 179.3115 - val_loss: 83.3740 - lr: 0.0030\n",
      "Epoch 277/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 178.7980 - val_loss: 83.9047 - lr: 0.0030\n",
      "Epoch 278/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 179.3750 - val_loss: 83.5687 - lr: 0.0030\n",
      "Epoch 279/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 178.9746 - val_loss: 83.5610 - lr: 0.0030\n",
      "Epoch 280/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 178.9099 - val_loss: 83.7294 - lr: 0.0030\n",
      "Epoch 281/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 179.1036 - val_loss: 83.5691 - lr: 0.0030\n",
      "Epoch 282/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 178.7346 - val_loss: 83.5962 - lr: 0.0030\n",
      "Epoch 283/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 178.6165 - val_loss: 83.5494 - lr: 0.0030\n",
      "Epoch 284/1000\n",
      "208133/208133 [==============================] - 2s 11us/sample - loss: 178.6870 - val_loss: 83.2939 - lr: 0.0030\n",
      "Epoch 285/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 178.0610 - val_loss: 84.1863 - lr: 0.0030\n",
      "Epoch 286/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 179.2106 - val_loss: 83.8945 - lr: 0.0030\n",
      "Epoch 287/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 179.0519 - val_loss: 83.8979 - lr: 0.0030\n",
      "Epoch 288/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 178.6213 - val_loss: 83.8203 - lr: 0.0030\n",
      "Epoch 289/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 178.6056 - val_loss: 83.9440 - lr: 0.0030\n",
      "Epoch 290/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 178.9050 - val_loss: 83.7505 - lr: 0.0030\n",
      "Epoch 291/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 178.3486 - val_loss: 83.6571 - lr: 0.0030\n",
      "Epoch 292/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 178.1427 - val_loss: 83.6910 - lr: 0.0030\n",
      "Epoch 293/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 178.1653 - val_loss: 83.7288 - lr: 0.0030\n",
      "Epoch 294/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 178.0717 - val_loss: 83.3937 - lr: 0.0030\n",
      "Epoch 295/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 177.5513 - val_loss: 83.9690 - lr: 0.0030\n",
      "Epoch 296/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 178.2329 - val_loss: 83.7502 - lr: 0.0030\n",
      "Epoch 297/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 177.8925 - val_loss: 83.5865 - lr: 0.0030\n",
      "Epoch 298/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 177.5616 - val_loss: 83.5249 - lr: 0.0030\n",
      "Epoch 299/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 177.6259 - val_loss: 83.5982 - lr: 0.0030\n",
      "Epoch 300/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 177.4089 - val_loss: 83.3804 - lr: 0.0022\n",
      "Epoch 301/1000\n",
      "208133/208133 [==============================] - 4s 22us/sample - loss: 177.1792 - val_loss: 83.4126 - lr: 0.0022\n",
      "Epoch 302/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 177.1489 - val_loss: 83.4569 - lr: 0.0022\n",
      "Epoch 303/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 177.0755 - val_loss: 83.2563 - lr: 0.0022\n",
      "Epoch 304/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 176.8424 - val_loss: 83.2692 - lr: 0.0022\n",
      "Epoch 305/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 176.8465 - val_loss: 83.1915 - lr: 0.0022\n",
      "Epoch 306/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208133/208133 [==============================] - 2s 9us/sample - loss: 176.6025 - val_loss: 83.2981 - lr: 0.0022\n",
      "Epoch 307/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 176.8586 - val_loss: 83.1673 - lr: 0.0022\n",
      "Epoch 308/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 176.5679 - val_loss: 83.3292 - lr: 0.0022\n",
      "Epoch 309/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 176.8633 - val_loss: 83.3327 - lr: 0.0022\n",
      "Epoch 310/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 176.7637 - val_loss: 83.2073 - lr: 0.0022\n",
      "Epoch 311/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 176.4439 - val_loss: 83.2296 - lr: 0.0022\n",
      "Epoch 312/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 176.5106 - val_loss: 83.2071 - lr: 0.0022\n",
      "Epoch 313/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 176.2070 - val_loss: 83.1692 - lr: 0.0022\n",
      "Epoch 314/1000\n",
      "208133/208133 [==============================] - 2s 11us/sample - loss: 176.2333 - val_loss: 83.0996 - lr: 0.0022\n",
      "Epoch 315/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 176.0355 - val_loss: 83.2556 - lr: 0.0022\n",
      "Epoch 316/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 176.1715 - val_loss: 83.0514 - lr: 0.0022\n",
      "Epoch 317/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 175.9198 - val_loss: 83.4879 - lr: 0.0022\n",
      "Epoch 318/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 176.3308 - val_loss: 83.4615 - lr: 0.0022\n",
      "Epoch 319/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 176.3146 - val_loss: 83.1559 - lr: 0.0022\n",
      "Epoch 320/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 175.7792 - val_loss: 83.1977 - lr: 0.0022\n",
      "Epoch 321/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 175.8634 - val_loss: 83.1531 - lr: 0.0022\n",
      "Epoch 322/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 175.7235 - val_loss: 83.1041 - lr: 0.0022\n",
      "Epoch 323/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 175.5557 - val_loss: 83.1316 - lr: 0.0022\n",
      "Epoch 324/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 175.5428 - val_loss: 83.1150 - lr: 0.0022\n",
      "Epoch 325/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 175.5154 - val_loss: 83.0856 - lr: 0.0022\n",
      "Epoch 326/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 175.2791 - val_loss: 83.1470 - lr: 0.0022\n",
      "Epoch 327/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 175.4567 - val_loss: 82.9924 - lr: 0.0022\n",
      "Epoch 328/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 175.1286 - val_loss: 83.2068 - lr: 0.0022\n",
      "Epoch 329/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 175.2590 - val_loss: 83.0757 - lr: 0.0022\n",
      "Epoch 330/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 175.1704 - val_loss: 83.0566 - lr: 0.0022\n",
      "Epoch 331/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 175.0873 - val_loss: 83.0734 - lr: 0.0022\n",
      "Epoch 332/1000\n",
      "208133/208133 [==============================] - 2s 10us/sample - loss: 175.0079 - val_loss: 83.0659 - lr: 0.0022\n",
      "Epoch 333/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 174.8744 - val_loss: 83.0794 - lr: 0.0022\n",
      "Epoch 334/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 174.8820 - val_loss: 82.9689 - lr: 0.0022\n",
      "Epoch 335/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 174.6415 - val_loss: 83.1742 - lr: 0.0022\n",
      "Epoch 336/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 174.8557 - val_loss: 82.9578 - lr: 0.0022\n",
      "Epoch 337/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 174.5987 - val_loss: 83.0851 - lr: 0.0022\n",
      "Epoch 338/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 174.6957 - val_loss: 83.0327 - lr: 0.0022\n",
      "Epoch 339/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 174.5769 - val_loss: 83.1137 - lr: 0.0022\n",
      "Epoch 340/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 174.5479 - val_loss: 83.1547 - lr: 0.0022\n",
      "Epoch 341/1000\n",
      "208133/208133 [==============================] - 3s 13us/sample - loss: 174.6385 - val_loss: 83.0297 - lr: 0.0022\n",
      "Epoch 342/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 174.3181 - val_loss: 83.0475 - lr: 0.0022\n",
      "Epoch 343/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 174.3740 - val_loss: 83.0007 - lr: 0.0022\n",
      "Epoch 344/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 174.1861 - val_loss: 82.9764 - lr: 0.0022\n",
      "Epoch 345/1000\n",
      "208133/208133 [==============================] - 2s 12us/sample - loss: 174.1588 - val_loss: 82.9646 - lr: 0.0022\n",
      "Epoch 346/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 174.0251 - val_loss: 83.0465 - lr: 0.0022\n",
      "Epoch 347/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 174.0540 - val_loss: 82.9140 - lr: 0.0022\n",
      "Epoch 348/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 173.9467 - val_loss: 82.9818 - lr: 0.0022\n",
      "Epoch 349/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 173.8103 - val_loss: 83.0007 - lr: 0.0022\n",
      "Epoch 350/1000\n",
      "208133/208133 [==============================] - 4s 21us/sample - loss: 173.8652 - val_loss: 82.8372 - lr: 0.0022\n",
      "Epoch 351/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 173.6617 - val_loss: 82.9970 - lr: 0.0022\n",
      "Epoch 352/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 173.7970 - val_loss: 82.9072 - lr: 0.0022\n",
      "Epoch 353/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 173.6188 - val_loss: 83.0175 - lr: 0.0022\n",
      "Epoch 354/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 173.6679 - val_loss: 82.9322 - lr: 0.0022\n",
      "Epoch 355/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 173.5753 - val_loss: 82.9352 - lr: 0.0022\n",
      "Epoch 356/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 173.4278 - val_loss: 82.9841 - lr: 0.0022\n",
      "Epoch 357/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 173.4914 - val_loss: 82.8706 - lr: 0.0022\n",
      "Epoch 358/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 173.2151 - val_loss: 83.0561 - lr: 0.0022\n",
      "Epoch 359/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 173.4368 - val_loss: 82.9334 - lr: 0.0022\n",
      "Epoch 360/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 173.2075 - val_loss: 82.9332 - lr: 0.0022\n",
      "Epoch 361/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 173.2263 - val_loss: 82.8431 - lr: 0.0022\n",
      "Epoch 362/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 172.9967 - val_loss: 83.0324 - lr: 0.0022\n",
      "Epoch 363/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 173.2472 - val_loss: 83.0768 - lr: 0.0022\n",
      "Epoch 364/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 173.3100 - val_loss: 82.9046 - lr: 0.0022\n",
      "Epoch 365/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.8577 - val_loss: 83.0468 - lr: 0.0022\n",
      "Epoch 366/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 173.1114 - val_loss: 82.8956 - lr: 0.0017\n",
      "Epoch 367/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.8806 - val_loss: 82.8158 - lr: 0.0017\n",
      "Epoch 368/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.6051 - val_loss: 82.9665 - lr: 0.0017\n",
      "Epoch 369/1000\n",
      "208133/208133 [==============================] - 3s 16us/sample - loss: 172.8171 - val_loss: 82.8212 - lr: 0.0017\n",
      "Epoch 370/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.5561 - val_loss: 82.7861 - lr: 0.0017\n",
      "Epoch 371/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.4957 - val_loss: 82.7642 - lr: 0.0017\n",
      "Epoch 372/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.3558 - val_loss: 82.8891 - lr: 0.0017\n",
      "Epoch 373/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.5109 - val_loss: 82.9129 - lr: 0.0017\n",
      "Epoch 374/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.5533 - val_loss: 82.7391 - lr: 0.0017\n",
      "Epoch 375/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 172.1760 - val_loss: 83.0493 - lr: 0.0017\n",
      "Epoch 376/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.7227 - val_loss: 83.1065 - lr: 0.0017\n",
      "Epoch 377/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.8630 - val_loss: 82.8681 - lr: 0.0017\n",
      "Epoch 378/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.2690 - val_loss: 82.9052 - lr: 0.0017\n",
      "Epoch 379/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.2824 - val_loss: 82.9950 - lr: 0.0017\n",
      "Epoch 380/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.4609 - val_loss: 82.9383 - lr: 0.0017\n",
      "Epoch 381/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.2009 - val_loss: 82.7617 - lr: 0.0017\n",
      "Epoch 382/1000\n",
      "208133/208133 [==============================] - 4s 17us/sample - loss: 171.9778 - val_loss: 82.8730 - lr: 0.0017\n",
      "Epoch 383/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.0934 - val_loss: 82.6769 - lr: 0.0017\n",
      "Epoch 384/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.7505 - val_loss: 83.1663 - lr: 0.0017\n",
      "Epoch 385/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.4079 - val_loss: 83.1435 - lr: 0.0017\n",
      "Epoch 386/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.4405 - val_loss: 82.9012 - lr: 0.0017\n",
      "Epoch 387/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.9111 - val_loss: 82.9506 - lr: 0.0017\n",
      "Epoch 388/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.9679 - val_loss: 82.8811 - lr: 0.0017\n",
      "Epoch 389/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.8846 - val_loss: 82.9328 - lr: 0.0017\n",
      "Epoch 390/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 171.8537 - val_loss: 82.7515 - lr: 0.0017\n",
      "Epoch 391/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 171.6237 - val_loss: 83.1879 - lr: 0.0017\n",
      "Epoch 392/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.0526 - val_loss: 83.2005 - lr: 0.0017\n",
      "Epoch 393/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 172.1585 - val_loss: 82.8799 - lr: 0.0017\n",
      "Epoch 394/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.6141 - val_loss: 82.9041 - lr: 0.0017\n",
      "Epoch 395/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.5978 - val_loss: 82.9560 - lr: 0.0017\n",
      "Epoch 396/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 171.7361 - val_loss: 82.7491 - lr: 0.0017\n",
      "Epoch 397/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.3248 - val_loss: 82.8018 - lr: 0.0017\n",
      "Epoch 398/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.3549 - val_loss: 82.8218 - lr: 0.0017\n",
      "Epoch 399/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.3736 - val_loss: 82.7263 - lr: 0.0013\n",
      "Epoch 400/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.1589 - val_loss: 82.9101 - lr: 0.0013\n",
      "Epoch 401/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.3596 - val_loss: 82.8989 - lr: 0.0013\n",
      "Epoch 402/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.3963 - val_loss: 82.8926 - lr: 0.0013\n",
      "Epoch 403/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 171.3335 - val_loss: 82.7821 - lr: 0.0013\n",
      "Epoch 404/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 171.1105 - val_loss: 82.9435 - lr: 0.0013\n",
      "Epoch 405/1000\n",
      "208133/208133 [==============================] - 3s 15us/sample - loss: 171.2681 - val_loss: 82.8855 - lr: 0.0013\n",
      "Epoch 406/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.3244 - val_loss: 82.8860 - lr: 0.0013\n",
      "Epoch 407/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.2335 - val_loss: 82.8029 - lr: 0.0013\n",
      "Epoch 408/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.9802 - val_loss: 82.7785 - lr: 0.0013\n",
      "Epoch 409/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 170.9964 - val_loss: 82.7853 - lr: 0.0013\n",
      "Epoch 410/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.9388 - val_loss: 82.7769 - lr: 0.0013\n",
      "Epoch 411/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.9522 - val_loss: 82.7003 - lr: 0.0013\n",
      "Epoch 412/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.7973 - val_loss: 82.9533 - lr: 0.0013\n",
      "Epoch 413/1000\n",
      "208133/208133 [==============================] - 2s 10us/sample - loss: 171.0614 - val_loss: 82.9329 - lr: 0.0013\n",
      "Epoch 414/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 171.0405 - val_loss: 82.7728 - lr: 0.0013\n",
      "Epoch 415/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.8240 - val_loss: 82.7715 - lr: 0.0013\n",
      "Epoch 416/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.7382 - val_loss: 82.8291 - lr: 0.0013\n",
      "Epoch 417/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.7935 - val_loss: 82.7335 - lr: 0.0013\n",
      "Epoch 418/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.7236 - val_loss: 82.6824 - lr: 9.4922e-04\n",
      "Epoch 419/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.6035 - val_loss: 82.6839 - lr: 9.4922e-04\n",
      "Epoch 420/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.5642 - val_loss: 82.7564 - lr: 9.4922e-04\n",
      "Epoch 421/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.5983 - val_loss: 82.6891 - lr: 9.4922e-04\n",
      "Epoch 422/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.5646 - val_loss: 82.7441 - lr: 9.4922e-04\n",
      "Epoch 423/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.5411 - val_loss: 82.7329 - lr: 9.4922e-04\n",
      "Epoch 424/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 170.4954 - val_loss: 82.7172 - lr: 9.4922e-04\n",
      "Epoch 425/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.4585 - val_loss: 82.7071 - lr: 9.4922e-04\n",
      "Epoch 426/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.4238 - val_loss: 82.7012 - lr: 9.4922e-04\n",
      "Epoch 427/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.4343 - val_loss: 82.6539 - lr: 9.4922e-04\n",
      "Epoch 428/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.3977 - val_loss: 82.7311 - lr: 9.4922e-04\n",
      "Epoch 429/1000\n",
      "208133/208133 [==============================] - 2s 12us/sample - loss: 170.4107 - val_loss: 82.7063 - lr: 9.4922e-04\n",
      "Epoch 430/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.3250 - val_loss: 82.7143 - lr: 9.4922e-04\n",
      "Epoch 431/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.2992 - val_loss: 82.7058 - lr: 9.4922e-04\n",
      "Epoch 432/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.2916 - val_loss: 82.7047 - lr: 9.4922e-04\n",
      "Epoch 433/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 170.2748 - val_loss: 82.7108 - lr: 9.4922e-04\n",
      "Epoch 434/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.2308 - val_loss: 82.7165 - lr: 9.4922e-04\n",
      "Epoch 435/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 170.1554 - val_loss: 82.6851 - lr: 9.4922e-04\n",
      "Epoch 436/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.1614 - val_loss: 82.7076 - lr: 9.4922e-04\n",
      "Epoch 437/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.1435 - val_loss: 82.7007 - lr: 9.4922e-04\n",
      "Epoch 438/1000\n",
      "208133/208133 [==============================] - 3s 15us/sample - loss: 170.0966 - val_loss: 82.6839 - lr: 9.4922e-04\n",
      "Epoch 439/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.0538 - val_loss: 82.6919 - lr: 9.4922e-04\n",
      "Epoch 440/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.0036 - val_loss: 82.7033 - lr: 9.4922e-04\n",
      "Epoch 441/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 170.0255 - val_loss: 82.6950 - lr: 9.4922e-04\n",
      "Epoch 442/1000\n",
      "208133/208133 [==============================] - 5s 25us/sample - loss: 169.9836 - val_loss: 82.7066 - lr: 9.4922e-04\n",
      "Epoch 443/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.9606 - val_loss: 82.6743 - lr: 7.1191e-04\n",
      "Epoch 444/1000\n",
      "208133/208133 [==============================] - 4s 20us/sample - loss: 169.9153 - val_loss: 82.6353 - lr: 7.1191e-04\n",
      "Epoch 445/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.8161 - val_loss: 82.6427 - lr: 7.1191e-04\n",
      "Epoch 446/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.8018 - val_loss: 82.6829 - lr: 7.1191e-04\n",
      "Epoch 447/1000\n",
      "208133/208133 [==============================] - 3s 14us/sample - loss: 169.8362 - val_loss: 82.6788 - lr: 7.1191e-04\n",
      "Epoch 448/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.7845 - val_loss: 82.6623 - lr: 7.1191e-04\n",
      "Epoch 449/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.7706 - val_loss: 82.6522 - lr: 7.1191e-04\n",
      "Epoch 450/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.7184 - val_loss: 82.6319 - lr: 7.1191e-04\n",
      "Epoch 451/1000\n",
      "208133/208133 [==============================] - 2s 11us/sample - loss: 169.6544 - val_loss: 82.6369 - lr: 7.1191e-04\n",
      "Epoch 452/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.6319 - val_loss: 82.6956 - lr: 7.1191e-04\n",
      "Epoch 453/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.6618 - val_loss: 82.6532 - lr: 7.1191e-04\n",
      "Epoch 454/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.6595 - val_loss: 82.6211 - lr: 7.1191e-04\n",
      "Epoch 455/1000\n",
      "208133/208133 [==============================] - 4s 18us/sample - loss: 169.5862 - val_loss: 82.6466 - lr: 7.1191e-04\n",
      "Epoch 456/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.5611 - val_loss: 82.6622 - lr: 7.1191e-04\n",
      "Epoch 457/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.5675 - val_loss: 82.6346 - lr: 7.1191e-04\n",
      "Epoch 458/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.5245 - val_loss: 82.6670 - lr: 7.1191e-04\n",
      "Epoch 459/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.4984 - val_loss: 82.6514 - lr: 7.1191e-04\n",
      "Epoch 460/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.5060 - val_loss: 82.6613 - lr: 7.1191e-04\n",
      "Epoch 461/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.4820 - val_loss: 82.6517 - lr: 7.1191e-04\n",
      "Epoch 462/1000\n",
      "208133/208133 [==============================] - 4s 21us/sample - loss: 169.4366 - val_loss: 82.6660 - lr: 7.1191e-04\n",
      "Epoch 463/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.3837 - val_loss: 82.6404 - lr: 7.1191e-04\n",
      "Epoch 464/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.3401 - val_loss: 82.7043 - lr: 7.1191e-04\n",
      "Epoch 465/1000\n",
      "208133/208133 [==============================] - 3s 13us/sample - loss: 169.3768 - val_loss: 82.6570 - lr: 7.1191e-04\n",
      "Epoch 466/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.3457 - val_loss: 82.6397 - lr: 7.1191e-04\n",
      "Epoch 467/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 169.2900 - val_loss: 82.6369 - lr: 7.1191e-04\n",
      "Epoch 468/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.2312 - val_loss: 82.6681 - lr: 7.1191e-04\n",
      "Epoch 469/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 169.2589 - val_loss: 82.6778 - lr: 7.1191e-04\n",
      "Epoch 470/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.2100 - val_loss: 82.6083 - lr: 5.3394e-04\n",
      "Epoch 471/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.1299 - val_loss: 82.6119 - lr: 5.3394e-04\n",
      "Epoch 472/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.1135 - val_loss: 82.6545 - lr: 5.3394e-04\n",
      "Epoch 473/1000\n",
      "208133/208133 [==============================] - 2s 11us/sample - loss: 169.1136 - val_loss: 82.6259 - lr: 5.3394e-04\n",
      "Epoch 474/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.0950 - val_loss: 82.6359 - lr: 5.3394e-04\n",
      "Epoch 475/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.1000 - val_loss: 82.6524 - lr: 5.3394e-04\n",
      "Epoch 476/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 169.0438 - val_loss: 82.6243 - lr: 5.3394e-04\n",
      "Epoch 477/1000\n",
      "208133/208133 [==============================] - 2s 10us/sample - loss: 169.0231 - val_loss: 82.5854 - lr: 5.3394e-04\n",
      "Epoch 478/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.9760 - val_loss: 82.6419 - lr: 5.3394e-04\n",
      "Epoch 479/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.9862 - val_loss: 82.6367 - lr: 5.3394e-04\n",
      "Epoch 480/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 168.9915 - val_loss: 82.6501 - lr: 5.3394e-04\n",
      "Epoch 481/1000\n",
      "208133/208133 [==============================] - 3s 16us/sample - loss: 168.9728 - val_loss: 82.6141 - lr: 5.3394e-04\n",
      "Epoch 482/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.9002 - val_loss: 82.6305 - lr: 5.3394e-04\n",
      "Epoch 483/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.9015 - val_loss: 82.5896 - lr: 5.3394e-04\n",
      "Epoch 484/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.8465 - val_loss: 82.6272 - lr: 5.3394e-04\n",
      "Epoch 485/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 168.8389 - val_loss: 82.6278 - lr: 5.3394e-04\n",
      "Epoch 486/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.8275 - val_loss: 82.6428 - lr: 5.3394e-04\n",
      "Epoch 487/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.8053 - val_loss: 82.6304 - lr: 5.3394e-04\n",
      "Epoch 488/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 168.7900 - val_loss: 82.6282 - lr: 5.3394e-04\n",
      "Epoch 489/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.7784 - val_loss: 82.6296 - lr: 5.3394e-04\n",
      "Epoch 490/1000\n",
      "208133/208133 [==============================] - 2s 12us/sample - loss: 168.7321 - val_loss: 82.6002 - lr: 5.3394e-04\n",
      "Epoch 491/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.7460 - val_loss: 82.6397 - lr: 5.3394e-04\n",
      "Epoch 492/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.6929 - val_loss: 82.6249 - lr: 5.3394e-04\n",
      "Epoch 493/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.6546 - val_loss: 82.6022 - lr: 4.0045e-04\n",
      "Epoch 494/1000\n",
      "208133/208133 [==============================] - 3s 13us/sample - loss: 168.6517 - val_loss: 82.6191 - lr: 4.0045e-04\n",
      "Epoch 495/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.5867 - val_loss: 82.5890 - lr: 4.0045e-04\n",
      "Epoch 496/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.5862 - val_loss: 82.6128 - lr: 4.0045e-04\n",
      "Epoch 497/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.5985 - val_loss: 82.6193 - lr: 4.0045e-04\n",
      "Epoch 498/1000\n",
      "208133/208133 [==============================] - 3s 14us/sample - loss: 168.5690 - val_loss: 82.6027 - lr: 4.0045e-04\n",
      "Epoch 499/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 168.5888 - val_loss: 82.6154 - lr: 4.0045e-04\n",
      "Epoch 500/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.5459 - val_loss: 82.6127 - lr: 4.0045e-04\n",
      "Epoch 501/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.5221 - val_loss: 82.6328 - lr: 4.0045e-04\n",
      "Epoch 502/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 168.4842 - val_loss: 82.5784 - lr: 4.0045e-04\n",
      "Epoch 503/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.5196 - val_loss: 82.6257 - lr: 4.0045e-04\n",
      "Epoch 504/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.4740 - val_loss: 82.6113 - lr: 4.0045e-04\n",
      "Epoch 505/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.4282 - val_loss: 82.6252 - lr: 4.0045e-04\n",
      "Epoch 506/1000\n",
      "208133/208133 [==============================] - 3s 14us/sample - loss: 168.4353 - val_loss: 82.5974 - lr: 4.0045e-04\n",
      "Epoch 507/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.4305 - val_loss: 82.5853 - lr: 4.0045e-04\n",
      "Epoch 508/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.3811 - val_loss: 82.6292 - lr: 4.0045e-04\n",
      "Epoch 509/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.3497 - val_loss: 82.6143 - lr: 4.0045e-04\n",
      "Epoch 510/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 168.3131 - val_loss: 82.5965 - lr: 4.0045e-04\n",
      "Epoch 511/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.2789 - val_loss: 82.6142 - lr: 4.0045e-04\n",
      "Epoch 512/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.3035 - val_loss: 82.6016 - lr: 4.0045e-04\n",
      "Epoch 513/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.2482 - val_loss: 82.6067 - lr: 4.0045e-04\n",
      "Epoch 514/1000\n",
      "208133/208133 [==============================] - 6s 28us/sample - loss: 168.2589 - val_loss: 82.6128 - lr: 4.0045e-04\n",
      "Epoch 515/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 168.2771 - val_loss: 82.5891 - lr: 4.0045e-04\n",
      "Epoch 516/1000\n",
      "208133/208133 [==============================] - 2s 12us/sample - loss: 168.1848 - val_loss: 82.6146 - lr: 4.0045e-04\n",
      "Epoch 517/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.2005 - val_loss: 82.6431 - lr: 4.0045e-04\n",
      "Epoch 518/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.1619 - val_loss: 82.5917 - lr: 3.0034e-04\n",
      "Epoch 519/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.1377 - val_loss: 82.5612 - lr: 3.0034e-04\n",
      "Epoch 520/1000\n",
      "208133/208133 [==============================] - 4s 18us/sample - loss: 168.1453 - val_loss: 82.6190 - lr: 3.0034e-04\n",
      "Epoch 521/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 168.1486 - val_loss: 82.6058 - lr: 3.0034e-04\n",
      "Epoch 522/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.1277 - val_loss: 82.5674 - lr: 3.0034e-04\n",
      "Epoch 523/1000\n",
      "208133/208133 [==============================] - 3s 12us/sample - loss: 168.0481 - val_loss: 82.5941 - lr: 3.0034e-04\n",
      "Epoch 524/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.0835 - val_loss: 82.6043 - lr: 3.0034e-04\n",
      "Epoch 525/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.0544 - val_loss: 82.6123 - lr: 3.0034e-04\n",
      "Epoch 526/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.0186 - val_loss: 82.5757 - lr: 3.0034e-04\n",
      "Epoch 527/1000\n",
      "208133/208133 [==============================] - 7s 33us/sample - loss: 168.0047 - val_loss: 82.5761 - lr: 3.0034e-04\n",
      "Epoch 528/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 168.0350 - val_loss: 82.5831 - lr: 3.0034e-04\n",
      "Epoch 529/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.9722 - val_loss: 82.5914 - lr: 3.0034e-04\n",
      "Epoch 530/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.9647 - val_loss: 82.5754 - lr: 3.0034e-04\n",
      "Epoch 531/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.9234 - val_loss: 82.5927 - lr: 3.0034e-04\n",
      "Epoch 532/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.9908 - val_loss: 82.5767 - lr: 3.0034e-04\n",
      "Epoch 533/1000\n",
      "208133/208133 [==============================] - 5s 22us/sample - loss: 167.9567 - val_loss: 82.5752 - lr: 3.0034e-04\n",
      "Epoch 534/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.9225 - val_loss: 82.5868 - lr: 3.0034e-04\n",
      "Epoch 535/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.8926 - val_loss: 82.5637 - lr: 3.0034e-04\n",
      "Epoch 536/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 167.8575 - val_loss: 82.5811 - lr: 3.0034e-04\n",
      "Epoch 537/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.8447 - val_loss: 82.5810 - lr: 2.2525e-04\n",
      "Epoch 538/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.8592 - val_loss: 82.5541 - lr: 2.2525e-04\n",
      "Epoch 539/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.8381 - val_loss: 82.6116 - lr: 2.2525e-04\n",
      "Epoch 540/1000\n",
      "208133/208133 [==============================] - 2s 10us/sample - loss: 167.7993 - val_loss: 82.5665 - lr: 2.2525e-04\n",
      "Epoch 541/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.7763 - val_loss: 82.5919 - lr: 2.2525e-04\n",
      "Epoch 542/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.8102 - val_loss: 82.5731 - lr: 2.2525e-04\n",
      "Epoch 543/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.7435 - val_loss: 82.5701 - lr: 2.2525e-04\n",
      "Epoch 544/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.7764 - val_loss: 82.5919 - lr: 2.2525e-04\n",
      "Epoch 545/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.7512 - val_loss: 82.5915 - lr: 2.2525e-04\n",
      "Epoch 546/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.7465 - val_loss: 82.5877 - lr: 2.2525e-04\n",
      "Epoch 547/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.7248 - val_loss: 82.5675 - lr: 2.2525e-04\n",
      "Epoch 548/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.7512 - val_loss: 82.5771 - lr: 2.2525e-04\n",
      "Epoch 549/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.6845 - val_loss: 82.5755 - lr: 2.2525e-04\n",
      "Epoch 550/1000\n",
      "208133/208133 [==============================] - 4s 18us/sample - loss: 167.7490 - val_loss: 82.5746 - lr: 2.2525e-04\n",
      "Epoch 551/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.6830 - val_loss: 82.5826 - lr: 2.2525e-04\n",
      "Epoch 552/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.6646 - val_loss: 82.5749 - lr: 2.2525e-04\n",
      "Epoch 553/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.6555 - val_loss: 82.5956 - lr: 2.2525e-04\n",
      "Epoch 554/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.6226 - val_loss: 82.5822 - lr: 2.2525e-04\n",
      "Epoch 555/1000\n",
      "208133/208133 [==============================] - 3s 15us/sample - loss: 167.6614 - val_loss: 82.5887 - lr: 2.2525e-04\n",
      "Epoch 556/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.6390 - val_loss: 82.5761 - lr: 1.6894e-04\n",
      "Epoch 557/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.6098 - val_loss: 82.5641 - lr: 1.6894e-04\n",
      "Epoch 558/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.5939 - val_loss: 82.5801 - lr: 1.6894e-04\n",
      "Epoch 559/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.6072 - val_loss: 82.5377 - lr: 1.6894e-04\n",
      "Epoch 560/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.5596 - val_loss: 82.5786 - lr: 1.6894e-04\n",
      "Epoch 561/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 167.5280 - val_loss: 82.5466 - lr: 1.6894e-04\n",
      "Epoch 562/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.5417 - val_loss: 82.5675 - lr: 1.6894e-04\n",
      "Epoch 563/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.5730 - val_loss: 82.5396 - lr: 1.6894e-04\n",
      "Epoch 564/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.5075 - val_loss: 82.5822 - lr: 1.6894e-04\n",
      "Epoch 565/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.5096 - val_loss: 82.5636 - lr: 1.6894e-04\n",
      "Epoch 566/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.5568 - val_loss: 82.5623 - lr: 1.6894e-04\n",
      "Epoch 567/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.5014 - val_loss: 82.5803 - lr: 1.6894e-04\n",
      "Epoch 568/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.5110 - val_loss: 82.5387 - lr: 1.6894e-04\n",
      "Epoch 569/1000\n",
      "208133/208133 [==============================] - 4s 17us/sample - loss: 167.4870 - val_loss: 82.5493 - lr: 1.6894e-04\n",
      "Epoch 570/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.4991 - val_loss: 82.5567 - lr: 1.6894e-04\n",
      "Epoch 571/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.4367 - val_loss: 82.5693 - lr: 1.6894e-04\n",
      "Epoch 572/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.4331 - val_loss: 82.5765 - lr: 1.6894e-04\n",
      "Epoch 573/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.4644 - val_loss: 82.5568 - lr: 1.6894e-04\n",
      "Epoch 574/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.4608 - val_loss: 82.5786 - lr: 1.6894e-04\n",
      "Epoch 575/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.3977 - val_loss: 82.5434 - lr: 1.2671e-04\n",
      "Epoch 576/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.4478 - val_loss: 82.5689 - lr: 1.2671e-04\n",
      "Epoch 577/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.3871 - val_loss: 82.5396 - lr: 1.2671e-04\n",
      "Epoch 578/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.3657 - val_loss: 82.5557 - lr: 1.2671e-04\n",
      "Epoch 579/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.3785 - val_loss: 82.5568 - lr: 1.2671e-04\n",
      "Epoch 580/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.3652 - val_loss: 82.5520 - lr: 1.2671e-04\n",
      "Epoch 581/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.3723 - val_loss: 82.5399 - lr: 1.2671e-04\n",
      "Epoch 582/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.3576 - val_loss: 82.5399 - lr: 1.2671e-04\n",
      "Epoch 583/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.3648 - val_loss: 82.5350 - lr: 1.2671e-04\n",
      "Epoch 584/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.3558 - val_loss: 82.5448 - lr: 1.2671e-04\n",
      "Epoch 585/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.3418 - val_loss: 82.5799 - lr: 1.2671e-04\n",
      "Epoch 586/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 167.2992 - val_loss: 82.5500 - lr: 1.2671e-04\n",
      "Epoch 587/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.3383 - val_loss: 82.5281 - lr: 1.2671e-04\n",
      "Epoch 588/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.3105 - val_loss: 82.5263 - lr: 1.2671e-04\n",
      "Epoch 589/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.3156 - val_loss: 82.5659 - lr: 1.2671e-04\n",
      "Epoch 590/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.3103 - val_loss: 82.5243 - lr: 1.2671e-04\n",
      "Epoch 591/1000\n",
      "208133/208133 [==============================] - 7s 32us/sample - loss: 167.3020 - val_loss: 82.5464 - lr: 1.2671e-04\n",
      "Epoch 592/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.2738 - val_loss: 82.5486 - lr: 1.2671e-04\n",
      "Epoch 593/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.2767 - val_loss: 82.5465 - lr: 1.2671e-04\n",
      "Epoch 594/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.2793 - val_loss: 82.5392 - lr: 1.2671e-04\n",
      "Epoch 595/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.2223 - val_loss: 82.5430 - lr: 1.2671e-04\n",
      "Epoch 596/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.2350 - val_loss: 82.5368 - lr: 1.2671e-04\n",
      "Epoch 597/1000\n",
      "208133/208133 [==============================] - 2s 12us/sample - loss: 167.2401 - val_loss: 82.5481 - lr: 1.2671e-04\n",
      "Epoch 598/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.2509 - val_loss: 82.5535 - lr: 1.2671e-04\n",
      "Epoch 599/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.1873 - val_loss: 82.5332 - lr: 1.2671e-04\n",
      "Epoch 600/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.1901 - val_loss: 82.5558 - lr: 1.2671e-04\n",
      "Epoch 601/1000\n",
      "208133/208133 [==============================] - 5s 26us/sample - loss: 167.1961 - val_loss: 82.5341 - lr: 1.2671e-04\n",
      "Epoch 602/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.2382 - val_loss: 82.5438 - lr: 1.2671e-04\n",
      "Epoch 603/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.1687 - val_loss: 82.5453 - lr: 1.2671e-04\n",
      "Epoch 604/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.1842 - val_loss: 82.5271 - lr: 1.2671e-04\n",
      "Epoch 605/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.1789 - val_loss: 82.5378 - lr: 1.2671e-04\n",
      "Epoch 606/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.1856 - val_loss: 82.5510 - lr: 9.5029e-05\n",
      "Epoch 607/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.1281 - val_loss: 82.5346 - lr: 9.5029e-05\n",
      "Epoch 608/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.1694 - val_loss: 82.5601 - lr: 9.5029e-05\n",
      "Epoch 609/1000\n",
      "208133/208133 [==============================] - 5s 25us/sample - loss: 167.1488 - val_loss: 82.5228 - lr: 9.5029e-05\n",
      "Epoch 610/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.1409 - val_loss: 82.5212 - lr: 9.5029e-05\n",
      "Epoch 611/1000\n",
      "208133/208133 [==============================] - 5s 25us/sample - loss: 167.0993 - val_loss: 82.5416 - lr: 9.5029e-05\n",
      "Epoch 612/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.1145 - val_loss: 82.5164 - lr: 9.5029e-05\n",
      "Epoch 613/1000\n",
      "208133/208133 [==============================] - 2s 11us/sample - loss: 167.1344 - val_loss: 82.5249 - lr: 9.5029e-05\n",
      "Epoch 614/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.1319 - val_loss: 82.5362 - lr: 9.5029e-05\n",
      "Epoch 615/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.1301 - val_loss: 82.5318 - lr: 9.5029e-05\n",
      "Epoch 616/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0968 - val_loss: 82.5145 - lr: 9.5029e-05\n",
      "Epoch 617/1000\n",
      "208133/208133 [==============================] - 4s 19us/sample - loss: 167.1223 - val_loss: 82.5440 - lr: 9.5029e-05\n",
      "Epoch 618/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.1091 - val_loss: 82.5237 - lr: 9.5029e-05\n",
      "Epoch 619/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0800 - val_loss: 82.5407 - lr: 9.5029e-05\n",
      "Epoch 620/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0891 - val_loss: 82.5089 - lr: 9.5029e-05\n",
      "Epoch 621/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0917 - val_loss: 82.5291 - lr: 9.5029e-05\n",
      "Epoch 622/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0834 - val_loss: 82.5186 - lr: 9.5029e-05\n",
      "Epoch 623/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0424 - val_loss: 82.5365 - lr: 9.5029e-05\n",
      "Epoch 624/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 167.0471 - val_loss: 82.4917 - lr: 9.5029e-05\n",
      "Epoch 625/1000\n",
      "208133/208133 [==============================] - 2s 12us/sample - loss: 167.1166 - val_loss: 82.5225 - lr: 9.5029e-05\n",
      "Epoch 626/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0283 - val_loss: 82.5453 - lr: 9.5029e-05\n",
      "Epoch 627/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0423 - val_loss: 82.5074 - lr: 9.5029e-05\n",
      "Epoch 628/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 167.0351 - val_loss: 82.5302 - lr: 9.5029e-05\n",
      "Epoch 629/1000\n",
      "208133/208133 [==============================] - 5s 22us/sample - loss: 167.0291 - val_loss: 82.5108 - lr: 9.5029e-05\n",
      "Epoch 630/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0663 - val_loss: 82.4990 - lr: 9.5029e-05\n",
      "Epoch 631/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0267 - val_loss: 82.5513 - lr: 9.5029e-05\n",
      "Epoch 632/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0091 - val_loss: 82.5311 - lr: 9.5029e-05\n",
      "Epoch 633/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0349 - val_loss: 82.5053 - lr: 9.5029e-05\n",
      "Epoch 634/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.9561 - val_loss: 82.5111 - lr: 9.5029e-05\n",
      "Epoch 635/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0216 - val_loss: 82.5137 - lr: 9.5029e-05\n",
      "Epoch 636/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.9847 - val_loss: 82.5310 - lr: 9.5029e-05\n",
      "Epoch 637/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.9981 - val_loss: 82.5048 - lr: 9.5029e-05\n",
      "Epoch 638/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 167.0033 - val_loss: 82.5196 - lr: 9.5029e-05\n",
      "Epoch 639/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.9510 - val_loss: 82.5381 - lr: 9.5029e-05\n",
      "Epoch 640/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.9772 - val_loss: 82.5221 - lr: 7.1272e-05\n",
      "Epoch 641/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.9790 - val_loss: 82.5122 - lr: 7.1272e-05\n",
      "Epoch 642/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.9923 - val_loss: 82.4979 - lr: 7.1272e-05\n",
      "Epoch 643/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.9512 - val_loss: 82.4964 - lr: 7.1272e-05\n",
      "Epoch 644/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.9372 - val_loss: 82.5361 - lr: 7.1272e-05\n",
      "Epoch 645/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.9805 - val_loss: 82.4953 - lr: 7.1272e-05\n",
      "Epoch 646/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.9646 - val_loss: 82.5023 - lr: 7.1272e-05\n",
      "Epoch 647/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.9510 - val_loss: 82.5119 - lr: 7.1272e-05\n",
      "Epoch 648/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.9115 - val_loss: 82.4992 - lr: 7.1272e-05\n",
      "Epoch 649/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.9359 - val_loss: 82.4947 - lr: 7.1272e-05\n",
      "Epoch 650/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.9521 - val_loss: 82.5058 - lr: 7.1272e-05\n",
      "Epoch 651/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.9299 - val_loss: 82.4932 - lr: 7.1272e-05\n",
      "Epoch 652/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.9193 - val_loss: 82.5044 - lr: 7.1272e-05\n",
      "Epoch 653/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.9152 - val_loss: 82.5051 - lr: 7.1272e-05\n",
      "Epoch 654/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.9180 - val_loss: 82.5110 - lr: 7.1272e-05\n",
      "Epoch 655/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.9064 - val_loss: 82.4975 - lr: 7.1272e-05\n",
      "Epoch 656/1000\n",
      "208133/208133 [==============================] - 4s 18us/sample - loss: 166.8691 - val_loss: 82.5398 - lr: 7.1272e-05\n",
      "Epoch 657/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8979 - val_loss: 82.4838 - lr: 7.1272e-05\n",
      "Epoch 658/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8759 - val_loss: 82.5249 - lr: 7.1272e-05\n",
      "Epoch 659/1000\n",
      "208133/208133 [==============================] - 1s 6us/sample - loss: 166.8634 - val_loss: 82.4670 - lr: 7.1272e-05\n",
      "Epoch 660/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.8811 - val_loss: 82.5026 - lr: 7.1272e-05\n",
      "Epoch 661/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8915 - val_loss: 82.4815 - lr: 7.1272e-05\n",
      "Epoch 662/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8739 - val_loss: 82.4950 - lr: 7.1272e-05\n",
      "Epoch 663/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8442 - val_loss: 82.4814 - lr: 7.1272e-05\n",
      "Epoch 664/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8488 - val_loss: 82.5230 - lr: 7.1272e-05\n",
      "Epoch 665/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8582 - val_loss: 82.4976 - lr: 7.1272e-05\n",
      "Epoch 666/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8590 - val_loss: 82.5287 - lr: 7.1272e-05\n",
      "Epoch 667/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8209 - val_loss: 82.5159 - lr: 7.1272e-05\n",
      "Epoch 668/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8717 - val_loss: 82.5031 - lr: 7.1272e-05\n",
      "Epoch 669/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.8414 - val_loss: 82.5212 - lr: 7.1272e-05\n",
      "Epoch 670/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 166.8440 - val_loss: 82.5285 - lr: 7.1272e-05\n",
      "Epoch 671/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8568 - val_loss: 82.5174 - lr: 7.1272e-05\n",
      "Epoch 672/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8128 - val_loss: 82.4972 - lr: 7.1272e-05\n",
      "Epoch 673/1000\n",
      "208133/208133 [==============================] - 6s 27us/sample - loss: 166.8071 - val_loss: 82.4963 - lr: 7.1272e-05\n",
      "Epoch 674/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7856 - val_loss: 82.4966 - lr: 7.1272e-05\n",
      "Epoch 675/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8082 - val_loss: 82.5218 - lr: 5.3454e-05\n",
      "Epoch 676/1000\n",
      "208133/208133 [==============================] - 5s 25us/sample - loss: 166.7825 - val_loss: 82.5056 - lr: 5.3454e-05\n",
      "Epoch 677/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8073 - val_loss: 82.4835 - lr: 5.3454e-05\n",
      "Epoch 678/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 166.8192 - val_loss: 82.4822 - lr: 5.3454e-05\n",
      "Epoch 679/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.7791 - val_loss: 82.4964 - lr: 5.3454e-05\n",
      "Epoch 680/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7314 - val_loss: 82.5014 - lr: 5.3454e-05\n",
      "Epoch 681/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7955 - val_loss: 82.5213 - lr: 5.3454e-05\n",
      "Epoch 682/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7446 - val_loss: 82.4904 - lr: 5.3454e-05\n",
      "Epoch 683/1000\n",
      "208133/208133 [==============================] - 5s 25us/sample - loss: 166.8040 - val_loss: 82.4861 - lr: 5.3454e-05\n",
      "Epoch 684/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7700 - val_loss: 82.4799 - lr: 5.3454e-05\n",
      "Epoch 685/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.8165 - val_loss: 82.5134 - lr: 5.3454e-05\n",
      "Epoch 686/1000\n",
      "208133/208133 [==============================] - 2s 9us/sample - loss: 166.7733 - val_loss: 82.4736 - lr: 5.3454e-05\n",
      "Epoch 687/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.7446 - val_loss: 82.4724 - lr: 5.3454e-05\n",
      "Epoch 688/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.7171 - val_loss: 82.5338 - lr: 5.3454e-05\n",
      "Epoch 689/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7560 - val_loss: 82.4921 - lr: 5.3454e-05\n",
      "Epoch 690/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.7809 - val_loss: 82.5144 - lr: 5.3454e-05\n",
      "Epoch 691/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7857 - val_loss: 82.5111 - lr: 5.3454e-05\n",
      "Epoch 692/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.7675 - val_loss: 82.4832 - lr: 5.3454e-05\n",
      "Epoch 693/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7697 - val_loss: 82.4788 - lr: 5.3454e-05\n",
      "Epoch 694/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7259 - val_loss: 82.4988 - lr: 4.0090e-05\n",
      "Epoch 695/1000\n",
      "208133/208133 [==============================] - 2s 11us/sample - loss: 166.7564 - val_loss: 82.4978 - lr: 4.0090e-05\n",
      "Epoch 696/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7273 - val_loss: 82.5081 - lr: 4.0090e-05\n",
      "Epoch 697/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7295 - val_loss: 82.4892 - lr: 4.0090e-05\n",
      "Epoch 698/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7682 - val_loss: 82.4879 - lr: 4.0090e-05\n",
      "Epoch 699/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.7278 - val_loss: 82.4775 - lr: 4.0090e-05\n",
      "Epoch 700/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7148 - val_loss: 82.5144 - lr: 4.0090e-05\n",
      "Epoch 701/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7183 - val_loss: 82.5074 - lr: 4.0090e-05\n",
      "Epoch 702/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7435 - val_loss: 82.4760 - lr: 4.0090e-05\n",
      "Epoch 703/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7385 - val_loss: 82.5057 - lr: 4.0090e-05\n",
      "Epoch 704/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.6594 - val_loss: 82.4981 - lr: 4.0090e-05\n",
      "Epoch 705/1000\n",
      "208133/208133 [==============================] - 2s 8us/sample - loss: 166.7042 - val_loss: 82.4825 - lr: 4.0090e-05\n",
      "Epoch 706/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7198 - val_loss: 82.4702 - lr: 4.0090e-05\n",
      "Epoch 707/1000\n",
      "208133/208133 [==============================] - 2s 7us/sample - loss: 166.7314 - val_loss: 82.4871 - lr: 4.0090e-05\n",
      "Epoch 708/1000\n",
      "208133/208133 [==============================] - 1s 7us/sample - loss: 166.6631 - val_loss: 82.4900 - lr: 4.0090e-05\n",
      "Epoch 709/1000\n",
      "208133/208133 [==============================] - 3s 15us/sample - loss: 166.7200 - val_loss: 82.4857 - lr: 4.0090e-05\n",
      "\n",
      "training finished in 709 epochs (early stop), transform data to adjust the platform effect...\n",
      "\n",
      "\n",
      "re-run DE on CVAE transformed scRNA-seq data!\n",
      "Differential analysis across cell-types on scRNA-seq data...\n",
      "finally selected 310 cell-type marker genes\n",
      "\n",
      "\n",
      "platform effect adjustment by CVAE finished. Elapsed time: 30.36 minutes.\n",
      "\n",
      "\n",
      "use the marker genes derived from CVAE transformed scRNA-seq for downstream regression!\n",
      "\n",
      "gene filtering before modeling...\n",
      "57 genes with nUMIs<5 in all spatial spots and need to be excluded\n",
      "finally use 253 genes for modeling\n",
      "\n",
      "spot filtering before modeling...\n",
      "all spots passed filtering\n",
      "\n",
      "\n",
      "######### Start GLRM modeling... #########\n",
      "\n",
      "GLRM settings:\n",
      "use SciPy minimize method:  L-BFGS-B\n",
      "global optimization turned off, local minimum will be used in GLRM\n",
      "use hybrid version of GLRM\n",
      "Numba detected total 64 available CPU cores. Use 64 CPU cores\n",
      "use 2001 points to calculate the heavy-tail density\n",
      "use weight threshold for Adaptive Lasso:  0.001\n",
      "total 247 unique nUMIs, min: 0.0, max: 569.0\n",
      "\n",
      "Build graph: \n",
      " Graph with 502 nodes and 867 edges\n",
      "\n",
      "estimation of gene-specific platform effect gamma_g is skipped as already using CVAE to adjust platform effect\n",
      "\n",
      "\n",
      "Start GLRM fitting...\n",
      "\n",
      "first estimate MLE theta and corresponding e^alpha and sigma^2...\n",
      "\n",
      "GLRM model initialization...\n",
      "calculate MLE theta and sigma^2 iteratively...\n",
      "  iter | time_opt | time_sig | sigma2\n",
      "     0 |    5.812 |    1.678 |  0.549\n",
      "     1 |    4.433 |    1.629 |  0.412\n",
      "     2 |    3.992 |    1.465 |  0.368\n",
      "     3 |    3.618 |    1.257 |  0.354\n",
      "     4 |    3.089 |    1.253 |  0.350\n",
      "     5 |    2.538 |    1.051 |  0.348\n",
      "     6 |    2.056 |    1.044 |  0.348\n",
      "MLE theta and sigma^2 calculation finished. Elapsed time: 0.58 minutes.\n",
      "MLE theta estimation finished. Elapsed time: 0.58 minutes.\n",
      "\n",
      "calculate weights of Adaptive Lasso...\n",
      "\n",
      "Stage 1: variable selection using Adaptive Lasso starts with the MLE theta and e^alpha, using already estimated sigma^2 and gamma_g...\n",
      "specified hyper-parameter for Adaptive Lasso is: 0.0\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |      0.001 |      0.001 |      0.071 |      0.071 |       1.00 |          / |    1.601 |    0.000 |    0.003 |   0.000026 |   0.000013\n",
      "Terminated (optimal) in 1 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 0.03 minutes.\n",
      "\n",
      "Stage 1 variable selection finished. Elapsed time: 0.03 minutes.\n",
      "\n",
      "Stage 2: final theta estimation with Graph Laplacian Constrain using already estimated sigma^2 and gamma_g\n",
      "specified hyper-parameter for Graph Laplacian Constrain is: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "hyper-parameter for Graph Laplacian Constrain: use cross-validation to find the optimal value from 8 candidates...\n",
      "\n",
      "Start cross-validation for hyper-parameter lambda_g...\n",
      "still use ADMM even NO Graph Laplacian constrain (lambda_g=0)\n",
      "0%...11%...22%...33%...44%...56%...67%...78%...89%...100%\n",
      "find optimal lambda_g 13.895 with average negative log-likelihood 18868.8486 by 5 fold cross-validation. Elapsed time: 38.76 minutes.\n",
      "\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |     11.189 |     10.171 |      0.082 |      0.082 |       1.00 |       1.00 |    4.831 |    0.000 |    0.003 |   0.208077 |   0.180841\n",
      "     1 |      9.043 |      7.054 |      0.080 |      0.089 |       1.00 |       1.00 |    2.496 |    0.000 |    0.003 |   0.190134 |   0.148295\n",
      "     2 |      7.306 |      7.152 |      0.078 |      0.095 |       1.00 |       1.00 |    2.471 |    0.000 |    0.003 |   0.134341 |   0.127138\n",
      "     3 |      6.671 |      7.239 |      0.078 |      0.101 |       1.00 |       2.00 |    2.479 |    0.000 |    0.003 |   0.118768 |   0.116077\n",
      "     4 |      6.065 |     11.889 |      0.083 |      0.113 |       2.00 |       2.00 |    2.666 |    0.000 |    0.003 |   0.111171 |   0.103968\n",
      "     5 |      5.460 |     11.786 |      0.083 |      0.123 |       2.00 |       2.00 |    2.698 |    0.000 |    0.003 |   0.095176 |   0.093827\n",
      "     6 |      4.960 |     12.095 |      0.083 |      0.133 |       2.00 |       4.00 |    2.563 |    0.000 |    0.003 |   0.085262 |   0.085642\n",
      "     7 |      4.451 |     19.757 |      0.091 |      0.150 |       4.00 |       4.00 |    2.689 |    0.000 |    0.003 |   0.079447 |   0.075318\n",
      "     8 |      3.886 |     19.822 |      0.091 |      0.164 |       4.00 |       4.00 |    2.684 |    0.000 |    0.003 |   0.068384 |   0.065393\n",
      "     9 |      3.356 |     20.575 |      0.091 |      0.177 |       4.00 |       8.00 |    2.571 |    0.000 |    0.003 |   0.056412 |   0.056814\n",
      "    10 |      2.869 |     32.287 |      0.103 |      0.199 |       8.00 |       8.00 |    2.707 |    0.000 |    0.003 |   0.050204 |   0.047595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    11 |      2.344 |     32.720 |      0.104 |      0.216 |       8.00 |       8.00 |    2.652 |    0.000 |    0.003 |   0.041445 |   0.038484\n",
      "    12 |      1.877 |     33.864 |      0.105 |      0.230 |       8.00 |      16.00 |    2.564 |    0.000 |    0.003 |   0.030503 |   0.031015\n",
      "    13 |      1.528 |     49.655 |      0.121 |      0.251 |      16.00 |      16.00 |    2.756 |    0.000 |    0.003 |   0.025714 |   0.024495\n",
      "    14 |      1.141 |     50.224 |      0.121 |      0.267 |      16.00 |      16.00 |    2.580 |    0.000 |    0.003 |   0.020541 |   0.017932\n",
      "    15 |      0.811 |     51.434 |      0.122 |      0.278 |      16.00 |      32.00 |    2.449 |    0.000 |    0.004 |   0.012447 |   0.012852\n",
      "    16 |      0.641 |     69.101 |      0.140 |      0.294 |      32.00 |      32.00 |    2.594 |    0.000 |    0.003 |   0.009816 |   0.009851\n",
      "    17 |      0.426 |     69.516 |      0.140 |      0.304 |      32.00 |      32.00 |    2.460 |    0.000 |    0.003 |   0.008136 |   0.006315\n",
      "    18 |      0.241 |     70.399 |      0.141 |      0.310 |      32.00 |      64.00 |    2.458 |    0.000 |    0.003 |   0.003502 |   0.003644\n",
      "    19 |      0.202 |     86.282 |      0.157 |      0.318 |      64.00 |      64.00 |    2.292 |    0.000 |    0.002 |   0.002257 |   0.002939\n",
      "    20 |      0.123 |     86.425 |      0.157 |      0.322 |      64.00 |      64.00 |    2.424 |    0.000 |    0.003 |   0.002568 |   0.001619\n",
      "    21 |      0.050 |     86.871 |      0.158 |      0.323 |      64.00 |     128.00 |    2.065 |    0.000 |    0.002 |   0.000864 |   0.000661\n",
      "    22 |      0.061 |     98.647 |      0.169 |      0.325 |     128.00 |     128.00 |    1.960 |    0.000 |    0.002 |   0.000324 |   0.000753\n",
      "    23 |      0.036 |     98.652 |      0.170 |      0.326 |     128.00 |     128.00 |    2.211 |    0.000 |    0.002 |   0.000794 |   0.000416\n",
      "    24 |      0.013 |     98.800 |      0.170 |      0.326 |     128.00 |     256.00 |    1.952 |    0.000 |    0.002 |   0.000268 |   0.000159\n",
      "    25 |      0.019 |    106.340 |      0.177 |      0.327 |     256.00 |     256.00 |    1.665 |    0.000 |    0.002 |   0.000091 |   0.000216\n",
      "    26 |      0.011 |    106.292 |      0.177 |      0.327 |     256.00 |     256.00 |    2.057 |    0.000 |    0.002 |   0.000222 |   0.000113\n",
      "    27 |      0.004 |    106.284 |      0.177 |      0.328 |     256.00 |     512.00 |    1.737 |    0.000 |    0.002 |   0.000069 |   0.000036\n",
      "    28 |      0.006 |    110.611 |      0.181 |      0.328 |     512.00 |          / |    1.811 |    0.000 |    0.002 |   0.000024 |   0.000060\n",
      "early stop!\n",
      "Terminated (optimal) in 29 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 1.20 minutes.\n",
      "\n",
      "\n",
      "stage 2 finished. Elapsed time: 39.95 minutes.\n",
      "\n",
      "GLRM fitting finished. Elapsed time: 40.56 minutes.\n",
      "\n",
      "\n",
      "Post-processing estimated cell-type proportion theta...\n",
      "hard thresholding small theta values with threshold 0\n",
      "\n",
      "\n",
      "cell type deconvolution finished. Estimate results saved in /home/exouser/Spatial/celltype_proportions.csv. Elapsed time: 1.18 hours.\n",
      "\n",
      "\n",
      "######### No imputation #########\n",
      "\n",
      "\n",
      "whole pipeline finished. Total elapsed time: 1.18 hours.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='runDeconvolution -q sim_subsetCT_spatial_spot_nUMI.csv                           -r scRNA_data_full.csv                           -c ref_scRNA_subsetCT_cell_celltype.csv                           -a sim_subsetCT_spatial_spot_adjacency_matrix.csv                           --n_hv_gene 500                           --n_marker_per_cmp 50                           --seed 2                           -n 64                           --lambda_r 0\\n', returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = '''runDeconvolution -q sim_subsetCT_spatial_spot_nUMI.csv \\\n",
    "                          -r scRNA_data_full.csv \\\n",
    "                          -c ref_scRNA_subsetCT_cell_celltype.csv \\\n",
    "                          -a sim_subsetCT_spatial_spot_adjacency_matrix.csv \\\n",
    "                          --n_hv_gene 500 \\\n",
    "                          --n_marker_per_cmp 50 \\\n",
    "                          --seed 2 \\\n",
    "                          -n 64 \\\n",
    "                          --lambda_r 0\n",
    "'''\n",
    "\n",
    "subprocess.run(cmd, check=True, text=True, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
