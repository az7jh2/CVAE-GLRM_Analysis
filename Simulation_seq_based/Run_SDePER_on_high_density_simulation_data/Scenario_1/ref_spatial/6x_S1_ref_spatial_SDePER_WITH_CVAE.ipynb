{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db496b92",
   "metadata": {},
   "source": [
    "# Run *SDePER* on sequencing-based 6x simulated data: Scenario 1 + Spatial data as reference + WITH CVAE\n",
    "\n",
    "In this Notebook we run SDePER on simulated data. For generating **sequencing-based high density** simulated data via coarse-graining procedure please refer [generate_simulated_spatial_data.nb.html](https://rawcdn.githack.com/az7jh2/SDePER_Analysis/d22e0c9b4574530a8ecbdf620638f2527ec20c5e/Simulation_seq_based/Generate_high_density_simulation_data/generate_simulated_spatial_data.nb.html) in [Generate_high_density_simulation_data](https://github.com/az7jh2/SDePER_Analysis/tree/main/Simulation_seq_based/Generate_high_density_simulation_data) folder.\n",
    "\n",
    "**Scenario 1** means the reference data for deconvolution includes all single cells with the **matched 12 cell types**.\n",
    "\n",
    "**Spatial data as reference** means the reference data is actually the [GSE102827](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE102827) scRNA-seq data which is used to generate the simulated data, therefore it's **free of platform effect**.\n",
    "\n",
    "**WITH CVAE** means we still use CVAE to remove platform effect although it's actually free of platform effect here.\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "So here we use the **4 input files** as shown below:\n",
    "\n",
    "1. raw nUMI counts of **6x** simulated spatial transcriptomic data (spots × genes): [sim_seq_based_6x_spatial_spot_nUMI.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation_seq_based/Generate_high_density_simulation_data/sim_seq_based_6x_spatial_spot_nUMI.csv)\n",
    "2. raw nUMI counts of reference GSE102827 scRNA-seq data (cells × genes): `GSE102827_scRNA_cell_nUMI.csv`. Since the file size of csv file of raw nUMI matrix of all 65,539 cells and 25,187 genes is up to 3.1 GB, we do not provide this file in our repository. It's just a **matrix transpose** of [GSE102827_merged_all_raw.csv.gz](https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE102827&format=file&file=GSE102827%5Fmerged%5Fall%5Fraw%2Ecsv%2Egz) in [GSE102827](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE102827) to satisty the file format requirement that rows as cells and columns as genes\n",
    "3. cell type annotations for selected **11,825 cells** used for simulated data generation in [GSE102827](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE102827) scRNA-seq data (cells × 1): [GSE102827_6x_scRNA_cell_celltype.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation_seq_based/Run_SDePER_on_high_density_simulation_data/Scenario_1/ref_spatial/GSE102827_6x_scRNA_cell_celltype.csv)\n",
    "4. adjacency matrix of spots in simulated spatial transcriptomic data (spots × spots): [sim_spatial_spot_adjacency_matrix.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Generate_simulation_data/sim_spatial_spot_adjacency_matrix.csv)\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "SDePER settings are the same as those used for the 1x setting ([S1_ref_spatial_SDePER_WITH_CVAE.ipynb](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation_seq_based/Run_SDePER_on_simulation_data/Scenario_1/ref_spatial/S1_ref_spatial_SDePER_WITH_CVAE.ipynb)):\n",
    "\n",
    "* number of selected TOP marker genes for each comparison in Differential `n_marker_per_cmp`: 20\n",
    "* number of used CPU cores `n_core`: 64\n",
    "* initial learning rate for training CVAE `cvae_init_lr`: 0.003\n",
    "* number of hidden layers in encoder and decoder of CVAE `num_hidden_layer`: 1\n",
    "* whether to use Batch Normalization `use_batch_norm`: false\n",
    "* CVAE training epochs `cvae_train_epoch`: 1000\n",
    "* for diagnostic purposes set `diagnosis`: true\n",
    "\n",
    "ALL other options are left as default.\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "the `bash` command to start cell type deconvolution is\n",
    "\n",
    "`runDeconvolution -q sim_seq_based_6x_spatial_spot_nUMI.csv -r GSE102827_scRNA_cell_nUMI.csv -c GSE102827_6x_scRNA_cell_celltype.csv -a sim_spatial_spot_adjacency_matrix.csv --n_marker_per_cmp 20 -n 64 --cvae_init_lr 0.003 --num_hidden_layer 1 --use_batch_norm false --cvae_train_epoch 1000 --diagnosis true`\n",
    "\n",
    "Note this Notebook uses **SDePER v1.2.1**. Cell type deconvolution result is renamed as [6x_S1_ref_spatial_SDePER_WITH_CVAE_celltype_proportions.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation_seq_based/Run_SDePER_on_high_density_simulation_data/Scenario_1/ref_spatial/6x_S1_ref_spatial_SDePER_WITH_CVAE_celltype_proportions.csv). Folder of diagnostic plots is compressed and renamed as [6x_S1_ref_spatial_SDePER_WITH_CVAE_diagnosis.tar](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation_seq_based/Run_SDePER_on_high_density_simulation_data/Scenario_1/ref_spatial/6x_S1_ref_spatial_SDePER_WITH_CVAE_diagnosis.tar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9e3939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SDePER (Spatial Deconvolution method with Platform Effect Removal) v1.2.1\n",
      "\n",
      "\n",
      "running options:\n",
      "spatial_file: /home/exouser/Spatial/sim_seq_based_6x_spatial_spot_nUMI.csv\n",
      "ref_file: /home/exouser/Spatial/GSE102827_scRNA_cell_nUMI.csv\n",
      "ref_celltype_file: /home/exouser/Spatial/GSE102827_6x_scRNA_cell_celltype.csv\n",
      "marker_file: None\n",
      "loc_file: None\n",
      "A_file: /home/exouser/Spatial/sim_spatial_spot_adjacency_matrix.csv\n",
      "n_cores: 64\n",
      "threshold: 0\n",
      "use_cvae: True\n",
      "use_imputation: False\n",
      "diagnosis: True\n",
      "verbose: True\n",
      "use_fdr: True\n",
      "p_val_cutoff: 0.05\n",
      "fc_cutoff: 1.2\n",
      "pct1_cutoff: 0.3\n",
      "pct2_cutoff: 0.1\n",
      "sortby_fc: True\n",
      "n_marker_per_cmp: 20\n",
      "filter_cell: True\n",
      "filter_gene: True\n",
      "n_hv_gene: 200\n",
      "n_pseudo_spot: 500000\n",
      "pseudo_spot_min_cell: 2\n",
      "pseudo_spot_max_cell: 8\n",
      "seq_depth_scaler: 10000\n",
      "cvae_input_scaler: 10\n",
      "cvae_init_lr: 0.003\n",
      "num_hidden_layer: 1\n",
      "use_batch_norm: False\n",
      "cvae_train_epoch: 1000\n",
      "use_spatial_pseudo: False\n",
      "redo_de: True\n",
      "seed: 383\n",
      "lambda_r: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "lambda_g: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "diameter: 200\n",
      "impute_diameter: [160, 114, 80]\n",
      "\n",
      "\n",
      "######### Preprocessing... #########\n",
      "\n",
      "first build CVAE...\n",
      "\n",
      "read spatial data from file /home/exouser/Spatial/sim_seq_based_6x_spatial_spot_nUMI.csv\n",
      "total 581 spots; 25187 genes\n",
      "\n",
      "filtering genes present in <3 spots: 7204 genes removed\n",
      "\n",
      "read scRNA-seq data from file /home/exouser/Spatial/GSE102827_scRNA_cell_nUMI.csv\n",
      "total 65539 cells; 25187 genes\n",
      "read scRNA-seq cell-type annotation from file /home/exouser/Spatial/GSE102827_6x_scRNA_cell_celltype.csv\n",
      "total 12 cell-types\n",
      "subset cells with cell-type annotation, finally keep 11825 cells; 25187 genes\n",
      "\n",
      "filtering cells with <200 genes: No cells removed\n",
      "\n",
      "filtering genes present in <10 cells: 8851 genes removed\n",
      "\n",
      "total 16336 overlapped genes\n",
      "\n",
      "identify 200 highly variable genes from scRNA-seq data...\n",
      "\n",
      "identify cell-type marker genes...\n",
      "no marker gene profile provided. Perform DE to get cell-type marker genes on scRNA-seq data...\n",
      "\n",
      "Differential analysis across cell-types on scRNA-seq data...\n",
      "0%...8%...WARNING: only 0 genes passing filtering (<20) for Endo vs Smc\n",
      "17%...25%...33%...42%...50%...58%...67%...WARNING: only 18 genes passing filtering (<20) for eL2/3 vs eL4\n",
      "WARNING: only 18 genes passing filtering (<20) for eL2/3 vs eL5\n",
      "75%...WARNING: only 12 genes passing filtering (<20) for eL4 vs eL2/3\n",
      "WARNING: only 8 genes passing filtering (<20) for eL4 vs eL5\n",
      "83%...WARNING: only 19 genes passing filtering (<20) for eL5 vs eL2/3\n",
      "WARNING: only 18 genes passing filtering (<20) for eL5 vs eL4\n",
      "92%...WARNING: only 10 genes passing filtering (<20) for eL6 vs eL5\n",
      "finally selected 534 cell-type marker genes\n",
      "\n",
      "\n",
      "use union of highly variable gene list and cell-type marker gene list derived from scRNA-seq data, finally get 625 genes for downstream analysis\n",
      "\n",
      "start CVAE building...\n",
      "\n",
      "generate 500000 pseudo-spots containing 2 to 8 cells from scRNA-seq cells...\n",
      "10%...20%...30%...40%...50%...60%...70%...80%...90%...100%...\n",
      "\n",
      "generate 0 pseudo-spots containing 2 to 6 spots from spatial spots...\n",
      "\n",
      "WARNING: first apply log transformation on sequencing depth normalized gene expressions, followed by Min-Max scaling\n",
      "\n",
      "                         |  training | validation\n",
      "spatial spots            |       581 |         0\n",
      "spatial pseudo-spots     |         0 |         0\n",
      "scRNA-seq cells          |     11825 |         0\n",
      "scRNA-seq pseudo-spots   |    400000 |    100000\n",
      "\n",
      "scaling inputs to range 0 to 10\n",
      "\n",
      "CVAE structure:\n",
      "Encoder: 626 - 150 - 36\n",
      "Decoder: 37 - 150 - 625\n",
      "\n",
      "\n",
      "Start training...\n",
      "\n",
      "Train on 412406 samples, validate on 100000 samples\n",
      "Epoch 1/1000\n",
      "412406/412406 [==============================] - 13s 32us/sample - loss: 1631.1630 - reconstruction_loss: 175.3703 - KL_loss: 25.1457 - val_loss: 167.3873 - val_reconstruction_loss: 149.9013 - val_KL_loss: 17.4861 - lr: 0.0030\n",
      "Epoch 2/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 1341.5264 - reconstruction_loss: 166.3439 - KL_loss: 24.6913 - val_loss: 166.9445 - val_reconstruction_loss: 145.1712 - val_KL_loss: 21.7733 - lr: 0.0030\n",
      "Epoch 3/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 795.3204 - reconstruction_loss: 157.7126 - KL_loss: 25.0041 - val_loss: 161.0248 - val_reconstruction_loss: 139.0053 - val_KL_loss: 22.0195 - lr: 0.0030\n",
      "Epoch 4/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 567.9635 - reconstruction_loss: 150.2153 - KL_loss: 23.2020 - val_loss: 149.5027 - val_reconstruction_loss: 131.7746 - val_KL_loss: 17.7281 - lr: 0.0030\n",
      "Epoch 5/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 508.4566 - reconstruction_loss: 142.4686 - KL_loss: 18.6313 - val_loss: 141.6671 - val_reconstruction_loss: 127.4506 - val_KL_loss: 14.2165 - lr: 0.0030\n",
      "Epoch 6/1000\n",
      "412406/412406 [==============================] - 7s 18us/sample - loss: 454.4902 - reconstruction_loss: 137.5260 - KL_loss: 14.9526 - val_loss: 136.9182 - val_reconstruction_loss: 121.7149 - val_KL_loss: 15.2033 - lr: 0.0030\n",
      "Epoch 7/1000\n",
      "412406/412406 [==============================] - 7s 18us/sample - loss: 418.3956 - reconstruction_loss: 131.1911 - KL_loss: 15.9391 - val_loss: 132.6918 - val_reconstruction_loss: 117.6550 - val_KL_loss: 15.0368 - lr: 0.0030\n",
      "Epoch 8/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 388.7609 - reconstruction_loss: 126.7145 - KL_loss: 15.6864 - val_loss: 127.5536 - val_reconstruction_loss: 113.0951 - val_KL_loss: 14.4585 - lr: 0.0030\n",
      "Epoch 9/1000\n",
      "412406/412406 [==============================] - 8s 21us/sample - loss: 363.8381 - reconstruction_loss: 121.8965 - KL_loss: 14.9985 - val_loss: 126.9291 - val_reconstruction_loss: 111.4639 - val_KL_loss: 15.4652 - lr: 0.0030\n",
      "Epoch 10/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 350.6219 - reconstruction_loss: 120.1452 - KL_loss: 15.9884 - val_loss: 119.3374 - val_reconstruction_loss: 104.8924 - val_KL_loss: 14.4450 - lr: 0.0030\n",
      "Epoch 11/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 333.7193 - reconstruction_loss: 113.4456 - KL_loss: 14.8906 - val_loss: 115.3196 - val_reconstruction_loss: 101.6570 - val_KL_loss: 13.6625 - lr: 0.0030\n",
      "Epoch 12/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 321.6997 - reconstruction_loss: 109.8937 - KL_loss: 14.0633 - val_loss: 111.3761 - val_reconstruction_loss: 96.8487 - val_KL_loss: 14.5273 - lr: 0.0030\n",
      "Epoch 13/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 311.4248 - reconstruction_loss: 104.9490 - KL_loss: 14.9496 - val_loss: 108.6170 - val_reconstruction_loss: 93.6554 - val_KL_loss: 14.9616 - lr: 0.0030\n",
      "Epoch 14/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 300.9897 - reconstruction_loss: 101.5251 - KL_loss: 15.3706 - val_loss: 106.1023 - val_reconstruction_loss: 91.1696 - val_KL_loss: 14.9327 - lr: 0.0030\n",
      "Epoch 15/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 293.9108 - reconstruction_loss: 98.8806 - KL_loss: 15.3190 - val_loss: 104.0876 - val_reconstruction_loss: 88.7143 - val_KL_loss: 15.3734 - lr: 0.0030\n",
      "Epoch 16/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 284.2416 - reconstruction_loss: 96.2903 - KL_loss: 15.7706 - val_loss: 101.9755 - val_reconstruction_loss: 87.0835 - val_KL_loss: 14.8920 - lr: 0.0030\n",
      "Epoch 17/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 274.6160 - reconstruction_loss: 94.4517 - KL_loss: 15.3071 - val_loss: 100.2059 - val_reconstruction_loss: 85.9173 - val_KL_loss: 14.2886 - lr: 0.0030\n",
      "Epoch 18/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 265.7299 - reconstruction_loss: 93.0182 - KL_loss: 14.7013 - val_loss: 97.0542 - val_reconstruction_loss: 82.4543 - val_KL_loss: 14.5999 - lr: 0.0030\n",
      "Epoch 19/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 255.6934 - reconstruction_loss: 89.2697 - KL_loss: 15.0208 - val_loss: 95.7616 - val_reconstruction_loss: 80.9439 - val_KL_loss: 14.8177 - lr: 0.0030\n",
      "Epoch 20/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 247.6141 - reconstruction_loss: 87.6151 - KL_loss: 15.2524 - val_loss: 94.6980 - val_reconstruction_loss: 79.8602 - val_KL_loss: 14.8378 - lr: 0.0030\n",
      "Epoch 21/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 243.9086 - reconstruction_loss: 86.4019 - KL_loss: 15.2840 - val_loss: 93.4823 - val_reconstruction_loss: 77.8776 - val_KL_loss: 15.6047 - lr: 0.0030\n",
      "Epoch 22/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 236.6163 - reconstruction_loss: 84.2932 - KL_loss: 16.0722 - val_loss: 94.4171 - val_reconstruction_loss: 77.4534 - val_KL_loss: 16.9637 - lr: 0.0030\n",
      "Epoch 23/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 233.2621 - reconstruction_loss: 83.6923 - KL_loss: 17.4441 - val_loss: 92.5463 - val_reconstruction_loss: 74.8405 - val_KL_loss: 17.7058 - lr: 0.0030\n",
      "Epoch 24/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 227.1576 - reconstruction_loss: 81.0085 - KL_loss: 18.1822 - val_loss: 93.2598 - val_reconstruction_loss: 75.9834 - val_KL_loss: 17.2764 - lr: 0.0030\n",
      "Epoch 25/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 223.9158 - reconstruction_loss: 81.9356 - KL_loss: 17.7413 - val_loss: 91.9440 - val_reconstruction_loss: 75.7918 - val_KL_loss: 16.1522 - lr: 0.0030\n",
      "Epoch 26/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 218.3790 - reconstruction_loss: 81.6206 - KL_loss: 16.6181 - val_loss: 88.6996 - val_reconstruction_loss: 73.5153 - val_KL_loss: 15.1843 - lr: 0.0030\n",
      "Epoch 27/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 214.4627 - reconstruction_loss: 79.3649 - KL_loss: 15.6523 - val_loss: 88.6720 - val_reconstruction_loss: 73.9968 - val_KL_loss: 14.6752 - lr: 0.0030\n",
      "Epoch 28/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 210.6824 - reconstruction_loss: 79.6693 - KL_loss: 15.1242 - val_loss: 86.6655 - val_reconstruction_loss: 72.4448 - val_KL_loss: 14.2208 - lr: 0.0030\n",
      "Epoch 29/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 207.9468 - reconstruction_loss: 78.0384 - KL_loss: 14.6441 - val_loss: 85.9110 - val_reconstruction_loss: 72.3151 - val_KL_loss: 13.5959 - lr: 0.0030\n",
      "Epoch 30/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 205.0544 - reconstruction_loss: 77.8566 - KL_loss: 14.0066 - val_loss: 85.0300 - val_reconstruction_loss: 71.6752 - val_KL_loss: 13.3548 - lr: 0.0030\n",
      "Epoch 31/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 201.0351 - reconstruction_loss: 77.1914 - KL_loss: 13.7658 - val_loss: 84.1918 - val_reconstruction_loss: 70.1154 - val_KL_loss: 14.0764 - lr: 0.0030\n",
      "Epoch 32/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 197.6671 - reconstruction_loss: 75.5302 - KL_loss: 14.4825 - val_loss: 84.3333 - val_reconstruction_loss: 69.6777 - val_KL_loss: 14.6556 - lr: 0.0030\n",
      "Epoch 33/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 196.6094 - reconstruction_loss: 75.0764 - KL_loss: 15.0527 - val_loss: 83.0862 - val_reconstruction_loss: 68.5492 - val_KL_loss: 14.5370 - lr: 0.0030\n",
      "Epoch 34/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 192.4044 - reconstruction_loss: 73.8167 - KL_loss: 14.9357 - val_loss: 83.1288 - val_reconstruction_loss: 69.1356 - val_KL_loss: 13.9932 - lr: 0.0030\n",
      "Epoch 35/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 191.9691 - reconstruction_loss: 74.4065 - KL_loss: 14.4009 - val_loss: 82.7065 - val_reconstruction_loss: 69.1624 - val_KL_loss: 13.5442 - lr: 0.0030\n",
      "Epoch 36/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 190.0644 - reconstruction_loss: 74.4289 - KL_loss: 13.9442 - val_loss: 81.3326 - val_reconstruction_loss: 67.9802 - val_KL_loss: 13.3523 - lr: 0.0030\n",
      "Epoch 37/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 185.7700 - reconstruction_loss: 73.1816 - KL_loss: 13.7375 - val_loss: 80.4640 - val_reconstruction_loss: 66.8395 - val_KL_loss: 13.6245 - lr: 0.0030\n",
      "Epoch 38/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 184.4121 - reconstruction_loss: 72.0048 - KL_loss: 14.0208 - val_loss: 80.1204 - val_reconstruction_loss: 66.7328 - val_KL_loss: 13.3876 - lr: 0.0030\n",
      "Epoch 39/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 181.9662 - reconstruction_loss: 71.7892 - KL_loss: 13.7949 - val_loss: 79.8103 - val_reconstruction_loss: 67.3926 - val_KL_loss: 12.4178 - lr: 0.0030\n",
      "Epoch 40/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 181.7001 - reconstruction_loss: 72.4234 - KL_loss: 12.8190 - val_loss: 79.3085 - val_reconstruction_loss: 67.1119 - val_KL_loss: 12.1966 - lr: 0.0030\n",
      "Epoch 41/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 179.2421 - reconstruction_loss: 72.1420 - KL_loss: 12.5956 - val_loss: 78.7226 - val_reconstruction_loss: 65.9167 - val_KL_loss: 12.8059 - lr: 0.0030\n",
      "Epoch 42/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 177.3732 - reconstruction_loss: 70.8871 - KL_loss: 13.2075 - val_loss: 78.1573 - val_reconstruction_loss: 65.2847 - val_KL_loss: 12.8726 - lr: 0.0030\n",
      "Epoch 43/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 175.8469 - reconstruction_loss: 70.2375 - KL_loss: 13.2806 - val_loss: 77.6086 - val_reconstruction_loss: 65.3100 - val_KL_loss: 12.2986 - lr: 0.0030\n",
      "Epoch 44/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 173.9928 - reconstruction_loss: 70.2321 - KL_loss: 12.7109 - val_loss: 77.1940 - val_reconstruction_loss: 65.1462 - val_KL_loss: 12.0479 - lr: 0.0030\n",
      "Epoch 45/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 173.1180 - reconstruction_loss: 70.0569 - KL_loss: 12.4600 - val_loss: 76.5273 - val_reconstruction_loss: 64.3053 - val_KL_loss: 12.2220 - lr: 0.0030\n",
      "Epoch 46/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 171.1762 - reconstruction_loss: 69.2119 - KL_loss: 12.6304 - val_loss: 76.1748 - val_reconstruction_loss: 63.7421 - val_KL_loss: 12.4327 - lr: 0.0030\n",
      "Epoch 47/1000\n",
      "412406/412406 [==============================] - 9s 22us/sample - loss: 170.3752 - reconstruction_loss: 68.6367 - KL_loss: 12.8489 - val_loss: 75.6090 - val_reconstruction_loss: 63.4858 - val_KL_loss: 12.1232 - lr: 0.0030\n",
      "Epoch 48/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 169.0541 - reconstruction_loss: 68.3473 - KL_loss: 12.5435 - val_loss: 75.1004 - val_reconstruction_loss: 63.3634 - val_KL_loss: 11.7369 - lr: 0.0030\n",
      "Epoch 49/1000\n",
      "412406/412406 [==============================] - 16s 39us/sample - loss: 167.6813 - reconstruction_loss: 68.1881 - KL_loss: 12.1431 - val_loss: 74.6732 - val_reconstruction_loss: 62.6960 - val_KL_loss: 11.9771 - lr: 0.0030\n",
      "Epoch 50/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 166.7683 - reconstruction_loss: 67.5512 - KL_loss: 12.3809 - val_loss: 74.0838 - val_reconstruction_loss: 62.0808 - val_KL_loss: 12.0031 - lr: 0.0030\n",
      "Epoch 51/1000\n",
      "412406/412406 [==============================] - 9s 22us/sample - loss: 165.7232 - reconstruction_loss: 66.9444 - KL_loss: 12.4235 - val_loss: 73.7372 - val_reconstruction_loss: 62.0295 - val_KL_loss: 11.7077 - lr: 0.0030\n",
      "Epoch 52/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 164.6749 - reconstruction_loss: 66.8472 - KL_loss: 12.1263 - val_loss: 73.3078 - val_reconstruction_loss: 61.7846 - val_KL_loss: 11.5231 - lr: 0.0030\n",
      "Epoch 53/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 163.6471 - reconstruction_loss: 66.5791 - KL_loss: 11.9361 - val_loss: 72.8171 - val_reconstruction_loss: 61.0858 - val_KL_loss: 11.7313 - lr: 0.0030\n",
      "Epoch 54/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 162.6361 - reconstruction_loss: 65.8766 - KL_loss: 12.1381 - val_loss: 72.4304 - val_reconstruction_loss: 60.7295 - val_KL_loss: 11.7010 - lr: 0.0030\n",
      "Epoch 55/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 161.5718 - reconstruction_loss: 65.5010 - KL_loss: 12.1128 - val_loss: 71.9995 - val_reconstruction_loss: 60.6327 - val_KL_loss: 11.3668 - lr: 0.0030\n",
      "Epoch 56/1000\n",
      "412406/412406 [==============================] - 16s 39us/sample - loss: 160.7656 - reconstruction_loss: 65.4065 - KL_loss: 11.7775 - val_loss: 71.5645 - val_reconstruction_loss: 60.1300 - val_KL_loss: 11.4345 - lr: 0.0030\n",
      "Epoch 57/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 159.5930 - reconstruction_loss: 64.8550 - KL_loss: 11.8338 - val_loss: 71.2320 - val_reconstruction_loss: 59.6088 - val_KL_loss: 11.6232 - lr: 0.0030\n",
      "Epoch 58/1000\n",
      "412406/412406 [==============================] - 25s 60us/sample - loss: 158.7629 - reconstruction_loss: 64.3238 - KL_loss: 12.0295 - val_loss: 70.7977 - val_reconstruction_loss: 59.3237 - val_KL_loss: 11.4740 - lr: 0.0030\n",
      "Epoch 59/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 157.7306 - reconstruction_loss: 64.0665 - KL_loss: 11.8762 - val_loss: 70.6235 - val_reconstruction_loss: 59.1648 - val_KL_loss: 11.4587 - lr: 0.0030\n",
      "Epoch 60/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 157.3902 - reconstruction_loss: 63.8637 - KL_loss: 11.8581 - val_loss: 70.0845 - val_reconstruction_loss: 58.6212 - val_KL_loss: 11.4633 - lr: 0.0030\n",
      "Epoch 61/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 155.8374 - reconstruction_loss: 63.3314 - KL_loss: 11.8566 - val_loss: 70.3053 - val_reconstruction_loss: 58.5344 - val_KL_loss: 11.7709 - lr: 0.0030\n",
      "Epoch 62/1000\n",
      "412406/412406 [==============================] - 17s 41us/sample - loss: 156.3990 - reconstruction_loss: 63.2621 - KL_loss: 12.1618 - val_loss: 70.1981 - val_reconstruction_loss: 58.3323 - val_KL_loss: 11.8659 - lr: 0.0030\n",
      "Epoch 63/1000\n",
      "412406/412406 [==============================] - 11s 26us/sample - loss: 155.5746 - reconstruction_loss: 63.0071 - KL_loss: 12.2555 - val_loss: 69.4311 - val_reconstruction_loss: 58.0194 - val_KL_loss: 11.4116 - lr: 0.0030\n",
      "Epoch 64/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 153.5977 - reconstruction_loss: 62.6777 - KL_loss: 11.7973 - val_loss: 69.8031 - val_reconstruction_loss: 58.7783 - val_KL_loss: 11.0248 - lr: 0.0030\n",
      "Epoch 65/1000\n",
      "412406/412406 [==============================] - 10s 23us/sample - loss: 154.1321 - reconstruction_loss: 63.4394 - KL_loss: 11.4052 - val_loss: 69.4172 - val_reconstruction_loss: 58.2679 - val_KL_loss: 11.1493 - lr: 0.0030\n",
      "Epoch 66/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 153.1399 - reconstruction_loss: 62.9368 - KL_loss: 11.5281 - val_loss: 68.8435 - val_reconstruction_loss: 57.2113 - val_KL_loss: 11.6322 - lr: 0.0030\n",
      "Epoch 67/1000\n",
      "412406/412406 [==============================] - 8s 21us/sample - loss: 151.9786 - reconstruction_loss: 61.8602 - KL_loss: 12.0110 - val_loss: 69.1727 - val_reconstruction_loss: 57.3346 - val_KL_loss: 11.8380 - lr: 0.0030\n",
      "Epoch 68/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 152.1641 - reconstruction_loss: 61.9962 - KL_loss: 12.2148 - val_loss: 68.6248 - val_reconstruction_loss: 56.8696 - val_KL_loss: 11.7552 - lr: 0.0030\n",
      "Epoch 69/1000\n",
      "412406/412406 [==============================] - 9s 21us/sample - loss: 150.8388 - reconstruction_loss: 61.4847 - KL_loss: 12.1323 - val_loss: 68.4812 - val_reconstruction_loss: 57.0282 - val_KL_loss: 11.4529 - lr: 0.0030\n",
      "Epoch 70/1000\n",
      "412406/412406 [==============================] - 9s 21us/sample - loss: 150.3915 - reconstruction_loss: 61.6682 - KL_loss: 11.8284 - val_loss: 68.5010 - val_reconstruction_loss: 57.4394 - val_KL_loss: 11.0615 - lr: 0.0030\n",
      "Epoch 71/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 150.4950 - reconstruction_loss: 62.0499 - KL_loss: 11.4346 - val_loss: 68.0774 - val_reconstruction_loss: 57.1495 - val_KL_loss: 10.9279 - lr: 0.0030\n",
      "Epoch 72/1000\n",
      "412406/412406 [==============================] - 17s 42us/sample - loss: 149.1431 - reconstruction_loss: 61.7264 - KL_loss: 11.3010 - val_loss: 67.8771 - val_reconstruction_loss: 56.5997 - val_KL_loss: 11.2773 - lr: 0.0030\n",
      "Epoch 73/1000\n",
      "412406/412406 [==============================] - 9s 21us/sample - loss: 148.9565 - reconstruction_loss: 61.2177 - KL_loss: 11.6533 - val_loss: 67.9195 - val_reconstruction_loss: 56.5776 - val_KL_loss: 11.3420 - lr: 0.0030\n",
      "Epoch 74/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 148.8704 - reconstruction_loss: 61.1933 - KL_loss: 11.7177 - val_loss: 67.3470 - val_reconstruction_loss: 56.2265 - val_KL_loss: 11.1205 - lr: 0.0030\n",
      "Epoch 75/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 147.6291 - reconstruction_loss: 60.8242 - KL_loss: 11.4987 - val_loss: 67.5003 - val_reconstruction_loss: 56.3933 - val_KL_loss: 11.1070 - lr: 0.0030\n",
      "Epoch 76/1000\n",
      "412406/412406 [==============================] - 9s 23us/sample - loss: 147.9330 - reconstruction_loss: 61.0015 - KL_loss: 11.4862 - val_loss: 67.6557 - val_reconstruction_loss: 56.6543 - val_KL_loss: 11.0014 - lr: 0.0030\n",
      "Epoch 77/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 147.8391 - reconstruction_loss: 61.2006 - KL_loss: 11.3786 - val_loss: 66.9957 - val_reconstruction_loss: 56.1804 - val_KL_loss: 10.8153 - lr: 0.0030\n",
      "Epoch 78/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 146.7336 - reconstruction_loss: 60.7346 - KL_loss: 11.1904 - val_loss: 66.6227 - val_reconstruction_loss: 55.6439 - val_KL_loss: 10.9788 - lr: 0.0030\n",
      "Epoch 79/1000\n",
      "412406/412406 [==============================] - 18s 43us/sample - loss: 146.1354 - reconstruction_loss: 60.2176 - KL_loss: 11.3533 - val_loss: 66.6930 - val_reconstruction_loss: 55.6423 - val_KL_loss: 11.0507 - lr: 0.0030\n",
      "Epoch 80/1000\n",
      "412406/412406 [==============================] - 12s 28us/sample - loss: 146.1279 - reconstruction_loss: 60.1621 - KL_loss: 11.4289 - val_loss: 66.2922 - val_reconstruction_loss: 55.4318 - val_KL_loss: 10.8604 - lr: 0.0030\n",
      "Epoch 81/1000\n",
      "412406/412406 [==============================] - 10s 25us/sample - loss: 145.1859 - reconstruction_loss: 59.9804 - KL_loss: 11.2427 - val_loss: 66.3197 - val_reconstruction_loss: 55.5388 - val_KL_loss: 10.7809 - lr: 0.0030\n",
      "Epoch 82/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 145.3662 - reconstruction_loss: 60.0850 - KL_loss: 11.1576 - val_loss: 66.1572 - val_reconstruction_loss: 55.4635 - val_KL_loss: 10.6936 - lr: 0.0030\n",
      "Epoch 83/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 144.9253 - reconstruction_loss: 59.9912 - KL_loss: 11.0642 - val_loss: 66.0440 - val_reconstruction_loss: 55.2959 - val_KL_loss: 10.7480 - lr: 0.0030\n",
      "Epoch 84/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 144.6097 - reconstruction_loss: 59.8513 - KL_loss: 11.1196 - val_loss: 65.8371 - val_reconstruction_loss: 54.8943 - val_KL_loss: 10.9428 - lr: 0.0030\n",
      "Epoch 85/1000\n",
      "412406/412406 [==============================] - 9s 21us/sample - loss: 143.9743 - reconstruction_loss: 59.4348 - KL_loss: 11.3152 - val_loss: 65.7142 - val_reconstruction_loss: 54.7129 - val_KL_loss: 11.0014 - lr: 0.0030\n",
      "Epoch 86/1000\n",
      "412406/412406 [==============================] - 15s 35us/sample - loss: 144.0612 - reconstruction_loss: 59.2311 - KL_loss: 11.3762 - val_loss: 65.5586 - val_reconstruction_loss: 54.7151 - val_KL_loss: 10.8435 - lr: 0.0030\n",
      "Epoch 87/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 143.4947 - reconstruction_loss: 59.2109 - KL_loss: 11.2177 - val_loss: 65.2297 - val_reconstruction_loss: 54.6428 - val_KL_loss: 10.5869 - lr: 0.0030\n",
      "Epoch 88/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 143.0222 - reconstruction_loss: 59.1796 - KL_loss: 10.9580 - val_loss: 65.1991 - val_reconstruction_loss: 54.6543 - val_KL_loss: 10.5448 - lr: 0.0030\n",
      "Epoch 89/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 142.5605 - reconstruction_loss: 59.1663 - KL_loss: 10.9163 - val_loss: 65.0206 - val_reconstruction_loss: 54.4349 - val_KL_loss: 10.5857 - lr: 0.0030\n",
      "Epoch 90/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 142.1340 - reconstruction_loss: 58.9234 - KL_loss: 10.9567 - val_loss: 64.8774 - val_reconstruction_loss: 54.2664 - val_KL_loss: 10.6110 - lr: 0.0030\n",
      "Epoch 91/1000\n",
      "412406/412406 [==============================] - 9s 21us/sample - loss: 141.8946 - reconstruction_loss: 58.7605 - KL_loss: 10.9801 - val_loss: 64.8059 - val_reconstruction_loss: 54.2485 - val_KL_loss: 10.5575 - lr: 0.0030\n",
      "Epoch 92/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 141.5209 - reconstruction_loss: 58.7366 - KL_loss: 10.9279 - val_loss: 64.6468 - val_reconstruction_loss: 54.1402 - val_KL_loss: 10.5066 - lr: 0.0030\n",
      "Epoch 93/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 141.1837 - reconstruction_loss: 58.6198 - KL_loss: 10.8800 - val_loss: 64.5733 - val_reconstruction_loss: 54.0033 - val_KL_loss: 10.5700 - lr: 0.0030\n",
      "Epoch 94/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 140.8359 - reconstruction_loss: 58.4535 - KL_loss: 10.9404 - val_loss: 64.3797 - val_reconstruction_loss: 53.7560 - val_KL_loss: 10.6237 - lr: 0.0030\n",
      "Epoch 95/1000\n",
      "412406/412406 [==============================] - 8s 21us/sample - loss: 140.4526 - reconstruction_loss: 58.2145 - KL_loss: 10.9894 - val_loss: 64.3643 - val_reconstruction_loss: 53.6875 - val_KL_loss: 10.6767 - lr: 0.0030\n",
      "Epoch 96/1000\n",
      "412406/412406 [==============================] - 23s 55us/sample - loss: 140.2695 - reconstruction_loss: 58.1599 - KL_loss: 11.0406 - val_loss: 64.1651 - val_reconstruction_loss: 53.6928 - val_KL_loss: 10.4723 - lr: 0.0030\n",
      "Epoch 97/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 139.8184 - reconstruction_loss: 58.1663 - KL_loss: 10.8310 - val_loss: 64.0758 - val_reconstruction_loss: 53.8088 - val_KL_loss: 10.2669 - lr: 0.0030\n",
      "Epoch 98/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 139.5714 - reconstruction_loss: 58.2557 - KL_loss: 10.6301 - val_loss: 63.9582 - val_reconstruction_loss: 53.7612 - val_KL_loss: 10.1970 - lr: 0.0030\n",
      "Epoch 99/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 139.1863 - reconstruction_loss: 58.2113 - KL_loss: 10.5560 - val_loss: 63.8220 - val_reconstruction_loss: 53.4945 - val_KL_loss: 10.3275 - lr: 0.0030\n",
      "Epoch 100/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 138.7739 - reconstruction_loss: 57.9217 - KL_loss: 10.6929 - val_loss: 63.7497 - val_reconstruction_loss: 53.2611 - val_KL_loss: 10.4886 - lr: 0.0030\n",
      "Epoch 101/1000\n",
      "412406/412406 [==============================] - 9s 22us/sample - loss: 138.6125 - reconstruction_loss: 57.7193 - KL_loss: 10.8418 - val_loss: 63.6074 - val_reconstruction_loss: 53.1712 - val_KL_loss: 10.4361 - lr: 0.0030\n",
      "Epoch 102/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 138.0919 - reconstruction_loss: 57.5984 - KL_loss: 10.7919 - val_loss: 63.5462 - val_reconstruction_loss: 53.2907 - val_KL_loss: 10.2555 - lr: 0.0030\n",
      "Epoch 103/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 138.0955 - reconstruction_loss: 57.7204 - KL_loss: 10.6074 - val_loss: 63.5121 - val_reconstruction_loss: 53.3808 - val_KL_loss: 10.1313 - lr: 0.0030\n",
      "Epoch 104/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 137.6534 - reconstruction_loss: 57.7919 - KL_loss: 10.4899 - val_loss: 63.3080 - val_reconstruction_loss: 53.0911 - val_KL_loss: 10.2169 - lr: 0.0030\n",
      "Epoch 105/1000\n",
      "412406/412406 [==============================] - 13s 30us/sample - loss: 137.4606 - reconstruction_loss: 57.5123 - KL_loss: 10.5700 - val_loss: 63.3039 - val_reconstruction_loss: 53.0041 - val_KL_loss: 10.2998 - lr: 0.0030\n",
      "Epoch 106/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 137.2726 - reconstruction_loss: 57.4035 - KL_loss: 10.6577 - val_loss: 63.1663 - val_reconstruction_loss: 52.8393 - val_KL_loss: 10.3270 - lr: 0.0030\n",
      "Epoch 107/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 136.7296 - reconstruction_loss: 57.2545 - KL_loss: 10.6805 - val_loss: 63.2515 - val_reconstruction_loss: 53.0260 - val_KL_loss: 10.2255 - lr: 0.0030\n",
      "Epoch 108/1000\n",
      "412406/412406 [==============================] - 19s 46us/sample - loss: 136.6965 - reconstruction_loss: 57.4308 - KL_loss: 10.5732 - val_loss: 63.0482 - val_reconstruction_loss: 52.9731 - val_KL_loss: 10.0751 - lr: 0.0030\n",
      "Epoch 109/1000\n",
      "412406/412406 [==============================] - 9s 21us/sample - loss: 136.2218 - reconstruction_loss: 57.3616 - KL_loss: 10.4266 - val_loss: 62.9362 - val_reconstruction_loss: 52.8882 - val_KL_loss: 10.0479 - lr: 0.0030\n",
      "Epoch 110/1000\n",
      "412406/412406 [==============================] - 10s 24us/sample - loss: 135.8337 - reconstruction_loss: 57.2637 - KL_loss: 10.4021 - val_loss: 62.9914 - val_reconstruction_loss: 52.9425 - val_KL_loss: 10.0489 - lr: 0.0030\n",
      "Epoch 111/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 135.6619 - reconstruction_loss: 57.2938 - KL_loss: 10.4031 - val_loss: 62.8735 - val_reconstruction_loss: 52.7932 - val_KL_loss: 10.0804 - lr: 0.0030\n",
      "Epoch 112/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 135.4240 - reconstruction_loss: 57.1664 - KL_loss: 10.4334 - val_loss: 62.7053 - val_reconstruction_loss: 52.6427 - val_KL_loss: 10.0626 - lr: 0.0030\n",
      "Epoch 113/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 135.1615 - reconstruction_loss: 57.0081 - KL_loss: 10.4177 - val_loss: 62.5979 - val_reconstruction_loss: 52.4795 - val_KL_loss: 10.1183 - lr: 0.0030\n",
      "Epoch 114/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 134.8985 - reconstruction_loss: 56.8336 - KL_loss: 10.4778 - val_loss: 62.5507 - val_reconstruction_loss: 52.4280 - val_KL_loss: 10.1227 - lr: 0.0030\n",
      "Epoch 115/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 134.4965 - reconstruction_loss: 56.7715 - KL_loss: 10.4734 - val_loss: 62.4459 - val_reconstruction_loss: 52.4255 - val_KL_loss: 10.0205 - lr: 0.0030\n",
      "Epoch 116/1000\n",
      "412406/412406 [==============================] - 8s 18us/sample - loss: 134.1957 - reconstruction_loss: 56.7912 - KL_loss: 10.3752 - val_loss: 62.5888 - val_reconstruction_loss: 52.7329 - val_KL_loss: 9.8558 - lr: 0.0030\n",
      "Epoch 117/1000\n",
      "412406/412406 [==============================] - 14s 34us/sample - loss: 134.2511 - reconstruction_loss: 57.0785 - KL_loss: 10.2082 - val_loss: 62.3280 - val_reconstruction_loss: 52.4940 - val_KL_loss: 9.8340 - lr: 0.0030\n",
      "Epoch 118/1000\n",
      "412406/412406 [==============================] - 12s 30us/sample - loss: 133.5745 - reconstruction_loss: 56.8500 - KL_loss: 10.1878 - val_loss: 62.4602 - val_reconstruction_loss: 52.5097 - val_KL_loss: 9.9505 - lr: 0.0030\n",
      "Epoch 119/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 133.7393 - reconstruction_loss: 56.8555 - KL_loss: 10.2978 - val_loss: 62.3295 - val_reconstruction_loss: 52.3480 - val_KL_loss: 9.9815 - lr: 0.0030\n",
      "Epoch 120/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 133.4151 - reconstruction_loss: 56.6983 - KL_loss: 10.3311 - val_loss: 62.1271 - val_reconstruction_loss: 52.2588 - val_KL_loss: 9.8683 - lr: 0.0030\n",
      "Epoch 121/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 132.8060 - reconstruction_loss: 56.5881 - KL_loss: 10.2157 - val_loss: 62.1584 - val_reconstruction_loss: 52.3397 - val_KL_loss: 9.8187 - lr: 0.0030\n",
      "Epoch 122/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 132.9770 - reconstruction_loss: 56.6851 - KL_loss: 10.1654 - val_loss: 62.1161 - val_reconstruction_loss: 52.3668 - val_KL_loss: 9.7493 - lr: 0.0030\n",
      "Epoch 123/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 132.6115 - reconstruction_loss: 56.7010 - KL_loss: 10.0994 - val_loss: 62.1437 - val_reconstruction_loss: 52.1845 - val_KL_loss: 9.9592 - lr: 0.0030\n",
      "Epoch 124/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 132.5029 - reconstruction_loss: 56.4923 - KL_loss: 10.3052 - val_loss: 62.1012 - val_reconstruction_loss: 52.1626 - val_KL_loss: 9.9386 - lr: 0.0030\n",
      "Epoch 125/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 132.2302 - reconstruction_loss: 56.4592 - KL_loss: 10.2897 - val_loss: 62.0071 - val_reconstruction_loss: 52.1911 - val_KL_loss: 9.8160 - lr: 0.0030\n",
      "Epoch 126/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 132.2209 - reconstruction_loss: 56.5034 - KL_loss: 10.1649 - val_loss: 61.9659 - val_reconstruction_loss: 52.2637 - val_KL_loss: 9.7022 - lr: 0.0030\n",
      "Epoch 127/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 131.6114 - reconstruction_loss: 56.5662 - KL_loss: 10.0472 - val_loss: 61.9134 - val_reconstruction_loss: 52.2104 - val_KL_loss: 9.7029 - lr: 0.0030\n",
      "Epoch 128/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 131.8875 - reconstruction_loss: 56.5041 - KL_loss: 10.0528 - val_loss: 61.9229 - val_reconstruction_loss: 52.1799 - val_KL_loss: 9.7430 - lr: 0.0030\n",
      "Epoch 129/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 131.4814 - reconstruction_loss: 56.4726 - KL_loss: 10.0859 - val_loss: 61.8172 - val_reconstruction_loss: 52.0900 - val_KL_loss: 9.7273 - lr: 0.0030\n",
      "Epoch 130/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 131.2531 - reconstruction_loss: 56.3644 - KL_loss: 10.0732 - val_loss: 61.8208 - val_reconstruction_loss: 52.0723 - val_KL_loss: 9.7485 - lr: 0.0030\n",
      "Epoch 131/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 131.0996 - reconstruction_loss: 56.3570 - KL_loss: 10.1016 - val_loss: 61.6783 - val_reconstruction_loss: 51.8995 - val_KL_loss: 9.7788 - lr: 0.0030\n",
      "Epoch 132/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 130.9706 - reconstruction_loss: 56.1847 - KL_loss: 10.1251 - val_loss: 61.6662 - val_reconstruction_loss: 51.8737 - val_KL_loss: 9.7925 - lr: 0.0030\n",
      "Epoch 133/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 130.4594 - reconstruction_loss: 56.1545 - KL_loss: 10.1400 - val_loss: 61.6475 - val_reconstruction_loss: 51.9326 - val_KL_loss: 9.7149 - lr: 0.0030\n",
      "Epoch 134/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 130.4467 - reconstruction_loss: 56.2087 - KL_loss: 10.0602 - val_loss: 61.6189 - val_reconstruction_loss: 52.0233 - val_KL_loss: 9.5956 - lr: 0.0030\n",
      "Epoch 135/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 130.1829 - reconstruction_loss: 56.2847 - KL_loss: 9.9457 - val_loss: 61.6839 - val_reconstruction_loss: 52.0119 - val_KL_loss: 9.6719 - lr: 0.0030\n",
      "Epoch 136/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 130.0810 - reconstruction_loss: 56.2740 - KL_loss: 10.0261 - val_loss: 61.5850 - val_reconstruction_loss: 51.9111 - val_KL_loss: 9.6739 - lr: 0.0030\n",
      "Epoch 137/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 130.0756 - reconstruction_loss: 56.1668 - KL_loss: 10.0235 - val_loss: 61.4408 - val_reconstruction_loss: 51.8151 - val_KL_loss: 9.6256 - lr: 0.0030\n",
      "Epoch 138/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 129.2166 - reconstruction_loss: 56.0621 - KL_loss: 9.9736 - val_loss: 61.7253 - val_reconstruction_loss: 52.0290 - val_KL_loss: 9.6963 - lr: 0.0030\n",
      "Epoch 139/1000\n",
      "412406/412406 [==============================] - 10s 24us/sample - loss: 130.7235 - reconstruction_loss: 56.3089 - KL_loss: 10.0423 - val_loss: 61.6858 - val_reconstruction_loss: 52.0806 - val_KL_loss: 9.6052 - lr: 0.0030\n",
      "Epoch 140/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 130.2813 - reconstruction_loss: 56.3426 - KL_loss: 9.9520 - val_loss: 61.5360 - val_reconstruction_loss: 52.0285 - val_KL_loss: 9.5074 - lr: 0.0030\n",
      "Epoch 141/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 129.7721 - reconstruction_loss: 56.2771 - KL_loss: 9.8567 - val_loss: 61.5077 - val_reconstruction_loss: 51.8561 - val_KL_loss: 9.6515 - lr: 0.0030\n",
      "Epoch 142/1000\n",
      "412406/412406 [==============================] - 9s 21us/sample - loss: 129.6999 - reconstruction_loss: 56.1019 - KL_loss: 9.9995 - val_loss: 61.6155 - val_reconstruction_loss: 51.7726 - val_KL_loss: 9.8429 - lr: 0.0030\n",
      "Epoch 143/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 129.6600 - reconstruction_loss: 56.0207 - KL_loss: 10.1861 - val_loss: 61.4659 - val_reconstruction_loss: 51.7208 - val_KL_loss: 9.7451 - lr: 0.0030\n",
      "Epoch 144/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 129.3444 - reconstruction_loss: 55.9754 - KL_loss: 10.0909 - val_loss: 61.5719 - val_reconstruction_loss: 52.0754 - val_KL_loss: 9.4965 - lr: 0.0030\n",
      "Epoch 145/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 129.0831 - reconstruction_loss: 56.2952 - KL_loss: 9.8483 - val_loss: 61.4759 - val_reconstruction_loss: 52.0004 - val_KL_loss: 9.4755 - lr: 0.0030\n",
      "Epoch 146/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 128.6985 - reconstruction_loss: 56.2455 - KL_loss: 9.8223 - val_loss: 61.4156 - val_reconstruction_loss: 51.8376 - val_KL_loss: 9.5780 - lr: 0.0030\n",
      "Epoch 147/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 128.7291 - reconstruction_loss: 56.0756 - KL_loss: 9.9146 - val_loss: 61.3663 - val_reconstruction_loss: 51.7339 - val_KL_loss: 9.6324 - lr: 0.0030\n",
      "Epoch 148/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 128.2697 - reconstruction_loss: 55.9698 - KL_loss: 9.9757 - val_loss: 61.3502 - val_reconstruction_loss: 51.7425 - val_KL_loss: 9.6077 - lr: 0.0030\n",
      "Epoch 149/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 128.3768 - reconstruction_loss: 55.9230 - KL_loss: 9.9639 - val_loss: 61.3465 - val_reconstruction_loss: 51.7113 - val_KL_loss: 9.6352 - lr: 0.0030\n",
      "Epoch 150/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 128.0214 - reconstruction_loss: 55.9196 - KL_loss: 9.9880 - val_loss: 61.2040 - val_reconstruction_loss: 51.5863 - val_KL_loss: 9.6178 - lr: 0.0030\n",
      "Epoch 151/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 128.0056 - reconstruction_loss: 55.8172 - KL_loss: 9.9598 - val_loss: 61.2235 - val_reconstruction_loss: 51.6979 - val_KL_loss: 9.5256 - lr: 0.0030\n",
      "Epoch 152/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 127.5438 - reconstruction_loss: 55.9187 - KL_loss: 9.8681 - val_loss: 61.2259 - val_reconstruction_loss: 51.7351 - val_KL_loss: 9.4908 - lr: 0.0030\n",
      "Epoch 153/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 127.6882 - reconstruction_loss: 55.9424 - KL_loss: 9.8426 - val_loss: 61.2119 - val_reconstruction_loss: 51.6738 - val_KL_loss: 9.5381 - lr: 0.0030\n",
      "Epoch 154/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 127.3195 - reconstruction_loss: 55.8433 - KL_loss: 9.8929 - val_loss: 61.2548 - val_reconstruction_loss: 51.6642 - val_KL_loss: 9.5906 - lr: 0.0030\n",
      "Epoch 155/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 127.0576 - reconstruction_loss: 55.8203 - KL_loss: 9.9402 - val_loss: 61.1340 - val_reconstruction_loss: 51.5845 - val_KL_loss: 9.5495 - lr: 0.0030\n",
      "Epoch 156/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 126.9489 - reconstruction_loss: 55.7621 - KL_loss: 9.8965 - val_loss: 61.2628 - val_reconstruction_loss: 51.8289 - val_KL_loss: 9.4339 - lr: 0.0030\n",
      "Epoch 157/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 127.0167 - reconstruction_loss: 55.9997 - KL_loss: 9.7862 - val_loss: 61.0572 - val_reconstruction_loss: 51.6362 - val_KL_loss: 9.4210 - lr: 0.0030\n",
      "Epoch 158/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 126.6103 - reconstruction_loss: 55.8134 - KL_loss: 9.7709 - val_loss: 61.0625 - val_reconstruction_loss: 51.5773 - val_KL_loss: 9.4852 - lr: 0.0030\n",
      "Epoch 159/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 126.8124 - reconstruction_loss: 55.7505 - KL_loss: 9.8374 - val_loss: 61.1917 - val_reconstruction_loss: 51.6291 - val_KL_loss: 9.5626 - lr: 0.0030\n",
      "Epoch 160/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 126.5905 - reconstruction_loss: 55.8387 - KL_loss: 9.9101 - val_loss: 61.0052 - val_reconstruction_loss: 51.4117 - val_KL_loss: 9.5935 - lr: 0.0030\n",
      "Epoch 161/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 126.1765 - reconstruction_loss: 55.5912 - KL_loss: 9.9433 - val_loss: 61.3460 - val_reconstruction_loss: 51.7970 - val_KL_loss: 9.5491 - lr: 0.0030\n",
      "Epoch 162/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 126.5013 - reconstruction_loss: 55.9404 - KL_loss: 9.9071 - val_loss: 60.9890 - val_reconstruction_loss: 51.5324 - val_KL_loss: 9.4567 - lr: 0.0030\n",
      "Epoch 163/1000\n",
      "412406/412406 [==============================] - 11s 27us/sample - loss: 125.8237 - reconstruction_loss: 55.7121 - KL_loss: 9.8061 - val_loss: 61.0857 - val_reconstruction_loss: 51.6197 - val_KL_loss: 9.4660 - lr: 0.0030\n",
      "Epoch 164/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 126.3368 - reconstruction_loss: 55.8035 - KL_loss: 9.8105 - val_loss: 61.0206 - val_reconstruction_loss: 51.5887 - val_KL_loss: 9.4319 - lr: 0.0030\n",
      "Epoch 165/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 126.1206 - reconstruction_loss: 55.7742 - KL_loss: 9.7786 - val_loss: 60.9929 - val_reconstruction_loss: 51.5604 - val_KL_loss: 9.4325 - lr: 0.0030\n",
      "Epoch 166/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 125.6433 - reconstruction_loss: 55.7238 - KL_loss: 9.7814 - val_loss: 61.0336 - val_reconstruction_loss: 51.4080 - val_KL_loss: 9.6257 - lr: 0.0030\n",
      "Epoch 167/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 126.0344 - reconstruction_loss: 55.5787 - KL_loss: 9.9783 - val_loss: 60.9782 - val_reconstruction_loss: 51.3069 - val_KL_loss: 9.6714 - lr: 0.0030\n",
      "Epoch 168/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 125.3117 - reconstruction_loss: 55.4769 - KL_loss: 10.0261 - val_loss: 61.0664 - val_reconstruction_loss: 51.5177 - val_KL_loss: 9.5488 - lr: 0.0030\n",
      "Epoch 169/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 125.6347 - reconstruction_loss: 55.6328 - KL_loss: 9.9018 - val_loss: 60.9928 - val_reconstruction_loss: 51.4932 - val_KL_loss: 9.4996 - lr: 0.0030\n",
      "Epoch 170/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 125.4006 - reconstruction_loss: 55.6450 - KL_loss: 9.8482 - val_loss: 60.9839 - val_reconstruction_loss: 51.5427 - val_KL_loss: 9.4412 - lr: 0.0030\n",
      "Epoch 171/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 124.9678 - reconstruction_loss: 55.6911 - KL_loss: 9.7835 - val_loss: 60.9642 - val_reconstruction_loss: 51.5490 - val_KL_loss: 9.4151 - lr: 0.0030\n",
      "Epoch 172/1000\n",
      "412406/412406 [==============================] - 8s 21us/sample - loss: 125.0576 - reconstruction_loss: 55.7037 - KL_loss: 9.7649 - val_loss: 60.8568 - val_reconstruction_loss: 51.3773 - val_KL_loss: 9.4794 - lr: 0.0030\n",
      "Epoch 173/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 124.7182 - reconstruction_loss: 55.5387 - KL_loss: 9.8264 - val_loss: 61.0668 - val_reconstruction_loss: 51.4389 - val_KL_loss: 9.6279 - lr: 0.0030\n",
      "Epoch 174/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 124.9580 - reconstruction_loss: 55.5712 - KL_loss: 9.9761 - val_loss: 60.8975 - val_reconstruction_loss: 51.3049 - val_KL_loss: 9.5927 - lr: 0.0030\n",
      "Epoch 175/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 124.4591 - reconstruction_loss: 55.4368 - KL_loss: 9.9385 - val_loss: 60.9163 - val_reconstruction_loss: 51.4032 - val_KL_loss: 9.5130 - lr: 0.0030\n",
      "Epoch 176/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 124.5190 - reconstruction_loss: 55.5409 - KL_loss: 9.8565 - val_loss: 60.9101 - val_reconstruction_loss: 51.4453 - val_KL_loss: 9.4647 - lr: 0.0030\n",
      "Epoch 177/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 124.2334 - reconstruction_loss: 55.6021 - KL_loss: 9.8054 - val_loss: 60.9391 - val_reconstruction_loss: 51.4659 - val_KL_loss: 9.4732 - lr: 0.0030\n",
      "Epoch 178/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 124.0397 - reconstruction_loss: 55.5664 - KL_loss: 9.8222 - val_loss: 60.8782 - val_reconstruction_loss: 51.3947 - val_KL_loss: 9.4835 - lr: 0.0030\n",
      "Epoch 179/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 124.1578 - reconstruction_loss: 55.5286 - KL_loss: 9.8329 - val_loss: 60.7999 - val_reconstruction_loss: 51.3654 - val_KL_loss: 9.4345 - lr: 0.0030\n",
      "Epoch 180/1000\n",
      "412406/412406 [==============================] - 10s 25us/sample - loss: 123.4080 - reconstruction_loss: 55.4551 - KL_loss: 9.7830 - val_loss: 61.0560 - val_reconstruction_loss: 51.6408 - val_KL_loss: 9.4151 - lr: 0.0030\n",
      "Epoch 181/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 124.9946 - reconstruction_loss: 55.7636 - KL_loss: 9.7568 - val_loss: 61.1054 - val_reconstruction_loss: 51.6469 - val_KL_loss: 9.4585 - lr: 0.0030\n",
      "Epoch 182/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 124.7519 - reconstruction_loss: 55.7898 - KL_loss: 9.7967 - val_loss: 60.9621 - val_reconstruction_loss: 51.4850 - val_KL_loss: 9.4771 - lr: 0.0030\n",
      "Epoch 183/1000\n",
      "412406/412406 [==============================] - 13s 33us/sample - loss: 124.0891 - reconstruction_loss: 55.6368 - KL_loss: 9.8121 - val_loss: 61.0286 - val_reconstruction_loss: 51.4466 - val_KL_loss: 9.5820 - lr: 0.0030\n",
      "Epoch 184/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 124.0502 - reconstruction_loss: 55.5680 - KL_loss: 9.9196 - val_loss: 61.0825 - val_reconstruction_loss: 51.4563 - val_KL_loss: 9.6262 - lr: 0.0030\n",
      "Epoch 185/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 123.7902 - reconstruction_loss: 55.5573 - KL_loss: 9.9735 - val_loss: 60.9639 - val_reconstruction_loss: 51.4152 - val_KL_loss: 9.5487 - lr: 0.0030\n",
      "Epoch 186/1000\n",
      "412406/412406 [==============================] - 9s 23us/sample - loss: 123.2556 - reconstruction_loss: 55.5181 - KL_loss: 9.8958 - val_loss: 61.0418 - val_reconstruction_loss: 51.6076 - val_KL_loss: 9.4343 - lr: 0.0030\n",
      "Epoch 187/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 123.7919 - reconstruction_loss: 55.6985 - KL_loss: 9.7761 - val_loss: 60.9042 - val_reconstruction_loss: 51.5448 - val_KL_loss: 9.3593 - lr: 0.0030\n",
      "Epoch 188/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 123.2661 - reconstruction_loss: 55.6814 - KL_loss: 9.6987 - val_loss: 60.8747 - val_reconstruction_loss: 51.4135 - val_KL_loss: 9.4612 - lr: 0.0030\n",
      "Epoch 189/1000\n",
      "412406/412406 [==============================] - 10s 25us/sample - loss: 123.1329 - reconstruction_loss: 55.5488 - KL_loss: 9.8006 - val_loss: 61.0239 - val_reconstruction_loss: 51.4563 - val_KL_loss: 9.5676 - lr: 0.0030\n",
      "Epoch 190/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 123.4596 - reconstruction_loss: 55.5505 - KL_loss: 9.9071 - val_loss: 60.8227 - val_reconstruction_loss: 51.3230 - val_KL_loss: 9.4997 - lr: 0.0027\n",
      "Epoch 191/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 122.7000 - reconstruction_loss: 55.4192 - KL_loss: 9.8433 - val_loss: 60.9150 - val_reconstruction_loss: 51.4473 - val_KL_loss: 9.4676 - lr: 0.0027\n",
      "Epoch 192/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 122.7959 - reconstruction_loss: 55.5549 - KL_loss: 9.8129 - val_loss: 60.8549 - val_reconstruction_loss: 51.3459 - val_KL_loss: 9.5089 - lr: 0.0027\n",
      "Epoch 193/1000\n",
      "412406/412406 [==============================] - 12s 29us/sample - loss: 122.8632 - reconstruction_loss: 55.4592 - KL_loss: 9.8528 - val_loss: 60.7533 - val_reconstruction_loss: 51.2337 - val_KL_loss: 9.5197 - lr: 0.0027\n",
      "Epoch 194/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 122.2299 - reconstruction_loss: 55.3358 - KL_loss: 9.8655 - val_loss: 60.8730 - val_reconstruction_loss: 51.3699 - val_KL_loss: 9.5031 - lr: 0.0027\n",
      "Epoch 195/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 122.6424 - reconstruction_loss: 55.4797 - KL_loss: 9.8481 - val_loss: 60.8505 - val_reconstruction_loss: 51.3414 - val_KL_loss: 9.5091 - lr: 0.0027\n",
      "Epoch 196/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 122.6456 - reconstruction_loss: 55.4384 - KL_loss: 9.8538 - val_loss: 60.8131 - val_reconstruction_loss: 51.3561 - val_KL_loss: 9.4571 - lr: 0.0027\n",
      "Epoch 197/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 122.1414 - reconstruction_loss: 55.4251 - KL_loss: 9.8068 - val_loss: 60.9278 - val_reconstruction_loss: 51.4572 - val_KL_loss: 9.4706 - lr: 0.0027\n",
      "Epoch 198/1000\n",
      "412406/412406 [==============================] - 14s 34us/sample - loss: 122.3282 - reconstruction_loss: 55.5290 - KL_loss: 9.8176 - val_loss: 60.8043 - val_reconstruction_loss: 51.3248 - val_KL_loss: 9.4795 - lr: 0.0027\n",
      "Epoch 199/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 122.0951 - reconstruction_loss: 55.4327 - KL_loss: 9.8237 - val_loss: 60.7636 - val_reconstruction_loss: 51.2793 - val_KL_loss: 9.4843 - lr: 0.0027\n",
      "Epoch 200/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 121.9251 - reconstruction_loss: 55.3558 - KL_loss: 9.8313 - val_loss: 60.8791 - val_reconstruction_loss: 51.3041 - val_KL_loss: 9.5750 - lr: 0.0027\n",
      "Epoch 201/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 121.7261 - reconstruction_loss: 55.3591 - KL_loss: 9.9198 - val_loss: 60.7148 - val_reconstruction_loss: 51.1628 - val_KL_loss: 9.5519 - lr: 0.0027\n",
      "Epoch 202/1000\n",
      "412406/412406 [==============================] - 11s 26us/sample - loss: 121.6584 - reconstruction_loss: 55.2304 - KL_loss: 9.8964 - val_loss: 60.8173 - val_reconstruction_loss: 51.3635 - val_KL_loss: 9.4538 - lr: 0.0027\n",
      "Epoch 203/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 121.6550 - reconstruction_loss: 55.4226 - KL_loss: 9.8053 - val_loss: 60.7757 - val_reconstruction_loss: 51.3707 - val_KL_loss: 9.4049 - lr: 0.0027\n",
      "Epoch 204/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 121.2078 - reconstruction_loss: 55.4501 - KL_loss: 9.7528 - val_loss: 60.7254 - val_reconstruction_loss: 51.2812 - val_KL_loss: 9.4442 - lr: 0.0027\n",
      "Epoch 205/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 121.3004 - reconstruction_loss: 55.3662 - KL_loss: 9.7861 - val_loss: 60.8942 - val_reconstruction_loss: 51.3776 - val_KL_loss: 9.5166 - lr: 0.0027\n",
      "Epoch 206/1000\n",
      "412406/412406 [==============================] - 9s 21us/sample - loss: 121.5484 - reconstruction_loss: 55.4475 - KL_loss: 9.8620 - val_loss: 60.7111 - val_reconstruction_loss: 51.1738 - val_KL_loss: 9.5373 - lr: 0.0027\n",
      "Epoch 207/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 120.8144 - reconstruction_loss: 55.2157 - KL_loss: 9.8819 - val_loss: 60.8977 - val_reconstruction_loss: 51.3447 - val_KL_loss: 9.5530 - lr: 0.0027\n",
      "Epoch 208/1000\n",
      "412406/412406 [==============================] - 9s 22us/sample - loss: 121.5478 - reconstruction_loss: 55.3848 - KL_loss: 9.8941 - val_loss: 60.8907 - val_reconstruction_loss: 51.4133 - val_KL_loss: 9.4773 - lr: 0.0027\n",
      "Epoch 209/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 121.2806 - reconstruction_loss: 55.4920 - KL_loss: 9.8200 - val_loss: 61.0424 - val_reconstruction_loss: 51.6749 - val_KL_loss: 9.3675 - lr: 0.0027\n",
      "Epoch 210/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 121.3678 - reconstruction_loss: 55.7414 - KL_loss: 9.7132 - val_loss: 60.7493 - val_reconstruction_loss: 51.3383 - val_KL_loss: 9.4110 - lr: 0.0027\n",
      "Epoch 211/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 120.7522 - reconstruction_loss: 55.3845 - KL_loss: 9.7539 - val_loss: 61.2180 - val_reconstruction_loss: 51.6163 - val_KL_loss: 9.6017 - lr: 0.0027\n",
      "Epoch 212/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 121.9054 - reconstruction_loss: 55.6923 - KL_loss: 9.9374 - val_loss: 60.9651 - val_reconstruction_loss: 51.2970 - val_KL_loss: 9.6681 - lr: 0.0027\n",
      "Epoch 213/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 121.1241 - reconstruction_loss: 55.3557 - KL_loss: 10.0045 - val_loss: 60.8389 - val_reconstruction_loss: 51.2570 - val_KL_loss: 9.5820 - lr: 0.0027\n",
      "Epoch 214/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 121.0247 - reconstruction_loss: 55.3011 - KL_loss: 9.9286 - val_loss: 60.8592 - val_reconstruction_loss: 51.3306 - val_KL_loss: 9.5286 - lr: 0.0027\n",
      "Epoch 215/1000\n",
      "412406/412406 [==============================] - 13s 32us/sample - loss: 121.1638 - reconstruction_loss: 55.3836 - KL_loss: 9.8769 - val_loss: 60.7049 - val_reconstruction_loss: 51.2385 - val_KL_loss: 9.4664 - lr: 0.0027\n",
      "Epoch 216/1000\n",
      "412406/412406 [==============================] - 9s 21us/sample - loss: 120.3049 - reconstruction_loss: 55.2870 - KL_loss: 9.8034 - val_loss: 60.7300 - val_reconstruction_loss: 51.2995 - val_KL_loss: 9.4305 - lr: 0.0027\n",
      "Epoch 217/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 120.9088 - reconstruction_loss: 55.3409 - KL_loss: 9.7648 - val_loss: 60.6555 - val_reconstruction_loss: 51.2586 - val_KL_loss: 9.3968 - lr: 0.0027\n",
      "Epoch 218/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 120.0812 - reconstruction_loss: 55.3168 - KL_loss: 9.7364 - val_loss: 60.8609 - val_reconstruction_loss: 51.3473 - val_KL_loss: 9.5136 - lr: 0.0027\n",
      "Epoch 219/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 121.5741 - reconstruction_loss: 55.4093 - KL_loss: 9.8605 - val_loss: 60.8300 - val_reconstruction_loss: 51.2286 - val_KL_loss: 9.6014 - lr: 0.0027\n",
      "Epoch 220/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 121.5964 - reconstruction_loss: 55.2974 - KL_loss: 9.9492 - val_loss: 60.8542 - val_reconstruction_loss: 51.2601 - val_KL_loss: 9.5941 - lr: 0.0027\n",
      "Epoch 221/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 120.4189 - reconstruction_loss: 55.3055 - KL_loss: 9.9362 - val_loss: 60.8580 - val_reconstruction_loss: 51.2466 - val_KL_loss: 9.6114 - lr: 0.0027\n",
      "Epoch 222/1000\n",
      "412406/412406 [==============================] - 9s 21us/sample - loss: 120.4052 - reconstruction_loss: 55.2905 - KL_loss: 9.9492 - val_loss: 60.8157 - val_reconstruction_loss: 51.2855 - val_KL_loss: 9.5303 - lr: 0.0027\n",
      "Epoch 223/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 120.2866 - reconstruction_loss: 55.3513 - KL_loss: 9.8694 - val_loss: 60.8049 - val_reconstruction_loss: 51.3225 - val_KL_loss: 9.4823 - lr: 0.0027\n",
      "Epoch 224/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 120.0255 - reconstruction_loss: 55.3539 - KL_loss: 9.8274 - val_loss: 60.7754 - val_reconstruction_loss: 51.3008 - val_KL_loss: 9.4746 - lr: 0.0027\n",
      "Epoch 225/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 119.7270 - reconstruction_loss: 55.3239 - KL_loss: 9.8161 - val_loss: 60.8026 - val_reconstruction_loss: 51.3089 - val_KL_loss: 9.4937 - lr: 0.0027\n",
      "Epoch 226/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 119.6448 - reconstruction_loss: 55.3465 - KL_loss: 9.8275 - val_loss: 60.6766 - val_reconstruction_loss: 51.1980 - val_KL_loss: 9.4786 - lr: 0.0027\n",
      "Epoch 227/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 119.3963 - reconstruction_loss: 55.2436 - KL_loss: 9.8155 - val_loss: 60.7530 - val_reconstruction_loss: 51.2732 - val_KL_loss: 9.4798 - lr: 0.0027\n",
      "Epoch 228/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 119.3870 - reconstruction_loss: 55.2941 - KL_loss: 9.8269 - val_loss: 60.6998 - val_reconstruction_loss: 51.2101 - val_KL_loss: 9.4897 - lr: 0.0024\n",
      "Epoch 229/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 119.3313 - reconstruction_loss: 55.1967 - KL_loss: 9.8357 - val_loss: 60.6378 - val_reconstruction_loss: 51.1784 - val_KL_loss: 9.4594 - lr: 0.0024\n",
      "Epoch 230/1000\n",
      "412406/412406 [==============================] - 16s 40us/sample - loss: 119.0620 - reconstruction_loss: 55.1994 - KL_loss: 9.7985 - val_loss: 60.6358 - val_reconstruction_loss: 51.1702 - val_KL_loss: 9.4657 - lr: 0.0024\n",
      "Epoch 231/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 119.0057 - reconstruction_loss: 55.1988 - KL_loss: 9.8069 - val_loss: 60.6739 - val_reconstruction_loss: 51.1710 - val_KL_loss: 9.5029 - lr: 0.0024\n",
      "Epoch 232/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 118.8921 - reconstruction_loss: 55.1781 - KL_loss: 9.8412 - val_loss: 60.6247 - val_reconstruction_loss: 51.1289 - val_KL_loss: 9.4957 - lr: 0.0024\n",
      "Epoch 233/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 118.6378 - reconstruction_loss: 55.1493 - KL_loss: 9.8360 - val_loss: 60.7095 - val_reconstruction_loss: 51.1944 - val_KL_loss: 9.5150 - lr: 0.0024\n",
      "Epoch 234/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 118.8465 - reconstruction_loss: 55.1873 - KL_loss: 9.8581 - val_loss: 60.6355 - val_reconstruction_loss: 51.0840 - val_KL_loss: 9.5515 - lr: 0.0024\n",
      "Epoch 235/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 118.5664 - reconstruction_loss: 55.0999 - KL_loss: 9.8936 - val_loss: 60.7164 - val_reconstruction_loss: 51.1821 - val_KL_loss: 9.5343 - lr: 0.0024\n",
      "Epoch 236/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 118.7730 - reconstruction_loss: 55.1903 - KL_loss: 9.8716 - val_loss: 60.6576 - val_reconstruction_loss: 51.1734 - val_KL_loss: 9.4842 - lr: 0.0024\n",
      "Epoch 237/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 118.6931 - reconstruction_loss: 55.1896 - KL_loss: 9.8218 - val_loss: 60.6730 - val_reconstruction_loss: 51.1623 - val_KL_loss: 9.5108 - lr: 0.0024\n",
      "Epoch 238/1000\n",
      "412406/412406 [==============================] - 9s 21us/sample - loss: 118.4310 - reconstruction_loss: 55.1756 - KL_loss: 9.8467 - val_loss: 60.6703 - val_reconstruction_loss: 51.1483 - val_KL_loss: 9.5221 - lr: 0.0024\n",
      "Epoch 239/1000\n",
      "412406/412406 [==============================] - 10s 24us/sample - loss: 118.2777 - reconstruction_loss: 55.1323 - KL_loss: 9.8608 - val_loss: 60.7595 - val_reconstruction_loss: 51.2489 - val_KL_loss: 9.5106 - lr: 0.0024\n",
      "Epoch 240/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 118.4209 - reconstruction_loss: 55.2494 - KL_loss: 9.8530 - val_loss: 60.6952 - val_reconstruction_loss: 51.1844 - val_KL_loss: 9.5109 - lr: 0.0024\n",
      "Epoch 241/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 118.0392 - reconstruction_loss: 55.1461 - KL_loss: 9.8499 - val_loss: 60.7236 - val_reconstruction_loss: 51.1533 - val_KL_loss: 9.5703 - lr: 0.0024\n",
      "Epoch 242/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 118.4937 - reconstruction_loss: 55.1547 - KL_loss: 9.9050 - val_loss: 60.7888 - val_reconstruction_loss: 51.2234 - val_KL_loss: 9.5653 - lr: 0.0024\n",
      "Epoch 243/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 118.4949 - reconstruction_loss: 55.2274 - KL_loss: 9.9015 - val_loss: 60.6480 - val_reconstruction_loss: 51.1552 - val_KL_loss: 9.4928 - lr: 0.0022\n",
      "Epoch 244/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 117.8131 - reconstruction_loss: 55.1334 - KL_loss: 9.8298 - val_loss: 60.6337 - val_reconstruction_loss: 51.1348 - val_KL_loss: 9.4989 - lr: 0.0022\n",
      "Epoch 245/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 118.0168 - reconstruction_loss: 55.1581 - KL_loss: 9.8378 - val_loss: 60.7204 - val_reconstruction_loss: 51.2239 - val_KL_loss: 9.4965 - lr: 0.0022\n",
      "Epoch 246/1000\n",
      "412406/412406 [==============================] - 9s 21us/sample - loss: 117.8155 - reconstruction_loss: 55.1988 - KL_loss: 9.8375 - val_loss: 60.6380 - val_reconstruction_loss: 51.1128 - val_KL_loss: 9.5252 - lr: 0.0022\n",
      "Epoch 247/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 117.6560 - reconstruction_loss: 55.1113 - KL_loss: 9.8635 - val_loss: 60.7016 - val_reconstruction_loss: 51.1366 - val_KL_loss: 9.5650 - lr: 0.0022\n",
      "Epoch 248/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 117.6810 - reconstruction_loss: 55.1193 - KL_loss: 9.8989 - val_loss: 60.6206 - val_reconstruction_loss: 51.1392 - val_KL_loss: 9.4815 - lr: 0.0022\n",
      "Epoch 249/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 117.5563 - reconstruction_loss: 55.1372 - KL_loss: 9.8156 - val_loss: 60.6228 - val_reconstruction_loss: 51.1806 - val_KL_loss: 9.4422 - lr: 0.0022\n",
      "Epoch 250/1000\n",
      "412406/412406 [==============================] - 16s 38us/sample - loss: 117.2447 - reconstruction_loss: 55.1462 - KL_loss: 9.7761 - val_loss: 60.7001 - val_reconstruction_loss: 51.2700 - val_KL_loss: 9.4302 - lr: 0.0022\n",
      "Epoch 251/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 117.5669 - reconstruction_loss: 55.2277 - KL_loss: 9.7652 - val_loss: 60.6761 - val_reconstruction_loss: 51.2181 - val_KL_loss: 9.4580 - lr: 0.0022\n",
      "Epoch 252/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 117.2519 - reconstruction_loss: 55.2005 - KL_loss: 9.7978 - val_loss: 60.6445 - val_reconstruction_loss: 51.1062 - val_KL_loss: 9.5383 - lr: 0.0022\n",
      "Epoch 253/1000\n",
      "412406/412406 [==============================] - 9s 22us/sample - loss: 117.1182 - reconstruction_loss: 55.0685 - KL_loss: 9.8727 - val_loss: 60.6875 - val_reconstruction_loss: 51.0481 - val_KL_loss: 9.6393 - lr: 0.0022\n",
      "Epoch 254/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 117.2065 - reconstruction_loss: 55.0231 - KL_loss: 9.9734 - val_loss: 60.6523 - val_reconstruction_loss: 51.0146 - val_KL_loss: 9.6377 - lr: 0.0022\n",
      "Epoch 255/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 116.9976 - reconstruction_loss: 54.9898 - KL_loss: 9.9705 - val_loss: 60.6535 - val_reconstruction_loss: 51.0692 - val_KL_loss: 9.5843 - lr: 0.0022\n",
      "Epoch 256/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 116.9838 - reconstruction_loss: 55.0424 - KL_loss: 9.9205 - val_loss: 60.7429 - val_reconstruction_loss: 51.2289 - val_KL_loss: 9.5141 - lr: 0.0022\n",
      "Epoch 257/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 116.8729 - reconstruction_loss: 55.1867 - KL_loss: 9.8492 - val_loss: 60.6389 - val_reconstruction_loss: 51.1624 - val_KL_loss: 9.4765 - lr: 0.0022\n",
      "Epoch 258/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 116.8227 - reconstruction_loss: 55.1508 - KL_loss: 9.8130 - val_loss: 60.6161 - val_reconstruction_loss: 51.0935 - val_KL_loss: 9.5226 - lr: 0.0022\n",
      "Epoch 259/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 116.6074 - reconstruction_loss: 55.0601 - KL_loss: 9.8552 - val_loss: 60.7255 - val_reconstruction_loss: 51.1293 - val_KL_loss: 9.5961 - lr: 0.0022\n",
      "Epoch 260/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 116.6688 - reconstruction_loss: 55.0755 - KL_loss: 9.9306 - val_loss: 60.6347 - val_reconstruction_loss: 51.0626 - val_KL_loss: 9.5721 - lr: 0.0022\n",
      "Epoch 261/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 116.4582 - reconstruction_loss: 55.0325 - KL_loss: 9.9066 - val_loss: 60.6152 - val_reconstruction_loss: 51.1112 - val_KL_loss: 9.5040 - lr: 0.0022\n",
      "Epoch 262/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 116.4738 - reconstruction_loss: 55.0901 - KL_loss: 9.8413 - val_loss: 60.6843 - val_reconstruction_loss: 51.1881 - val_KL_loss: 9.4962 - lr: 0.0022\n",
      "Epoch 263/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 116.3967 - reconstruction_loss: 55.1464 - KL_loss: 9.8287 - val_loss: 60.6474 - val_reconstruction_loss: 51.1360 - val_KL_loss: 9.5114 - lr: 0.0022\n",
      "Epoch 264/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 116.2499 - reconstruction_loss: 55.0957 - KL_loss: 9.8476 - val_loss: 60.6786 - val_reconstruction_loss: 51.1111 - val_KL_loss: 9.5675 - lr: 0.0022\n",
      "Epoch 265/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 116.3051 - reconstruction_loss: 55.0599 - KL_loss: 9.9006 - val_loss: 60.6810 - val_reconstruction_loss: 51.0846 - val_KL_loss: 9.5964 - lr: 0.0022\n",
      "Epoch 266/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 116.0415 - reconstruction_loss: 55.0388 - KL_loss: 9.9315 - val_loss: 60.6570 - val_reconstruction_loss: 51.1249 - val_KL_loss: 9.5321 - lr: 0.0022\n",
      "Epoch 267/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 116.0405 - reconstruction_loss: 55.0986 - KL_loss: 9.8650 - val_loss: 60.6484 - val_reconstruction_loss: 51.1569 - val_KL_loss: 9.4915 - lr: 0.0022\n",
      "Epoch 268/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.9127 - reconstruction_loss: 55.1026 - KL_loss: 9.8262 - val_loss: 60.6978 - val_reconstruction_loss: 51.1561 - val_KL_loss: 9.5416 - lr: 0.0022\n",
      "Epoch 269/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 116.1030 - reconstruction_loss: 55.0913 - KL_loss: 9.8737 - val_loss: 60.6513 - val_reconstruction_loss: 51.0291 - val_KL_loss: 9.6221 - lr: 0.0022\n",
      "Epoch 270/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.7858 - reconstruction_loss: 54.9842 - KL_loss: 9.9575 - val_loss: 60.6790 - val_reconstruction_loss: 51.0058 - val_KL_loss: 9.6732 - lr: 0.0022\n",
      "Epoch 271/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.7555 - reconstruction_loss: 54.9457 - KL_loss: 10.0059 - val_loss: 60.7249 - val_reconstruction_loss: 51.1083 - val_KL_loss: 9.6166 - lr: 0.0022\n",
      "Epoch 272/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.9770 - reconstruction_loss: 55.0747 - KL_loss: 9.9501 - val_loss: 60.6384 - val_reconstruction_loss: 51.0999 - val_KL_loss: 9.5385 - lr: 0.0020\n",
      "Epoch 273/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.4361 - reconstruction_loss: 55.0639 - KL_loss: 9.8701 - val_loss: 60.6945 - val_reconstruction_loss: 51.1842 - val_KL_loss: 9.5104 - lr: 0.0020\n",
      "Epoch 274/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.8142 - reconstruction_loss: 55.1089 - KL_loss: 9.8409 - val_loss: 60.7028 - val_reconstruction_loss: 51.1848 - val_KL_loss: 9.5180 - lr: 0.0020\n",
      "Epoch 275/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.5055 - reconstruction_loss: 55.1310 - KL_loss: 9.8523 - val_loss: 60.7627 - val_reconstruction_loss: 51.2163 - val_KL_loss: 9.5465 - lr: 0.0020\n",
      "Epoch 276/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.7529 - reconstruction_loss: 55.1555 - KL_loss: 9.8807 - val_loss: 60.6984 - val_reconstruction_loss: 51.0565 - val_KL_loss: 9.6419 - lr: 0.0020\n",
      "Epoch 277/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.4834 - reconstruction_loss: 55.0143 - KL_loss: 9.9740 - val_loss: 60.8448 - val_reconstruction_loss: 51.1345 - val_KL_loss: 9.7103 - lr: 0.0020\n",
      "Epoch 278/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.7078 - reconstruction_loss: 55.0695 - KL_loss: 10.0385 - val_loss: 60.7134 - val_reconstruction_loss: 51.0409 - val_KL_loss: 9.6725 - lr: 0.0020\n",
      "Epoch 279/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.6079 - reconstruction_loss: 55.0163 - KL_loss: 10.0016 - val_loss: 60.8098 - val_reconstruction_loss: 51.2401 - val_KL_loss: 9.5697 - lr: 0.0020\n",
      "Epoch 280/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.6210 - reconstruction_loss: 55.1730 - KL_loss: 9.9036 - val_loss: 60.7684 - val_reconstruction_loss: 51.2451 - val_KL_loss: 9.5232 - lr: 0.0020\n",
      "Epoch 281/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.3863 - reconstruction_loss: 55.1685 - KL_loss: 9.8552 - val_loss: 60.7611 - val_reconstruction_loss: 51.2298 - val_KL_loss: 9.5312 - lr: 0.0020\n",
      "Epoch 282/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.3820 - reconstruction_loss: 55.1517 - KL_loss: 9.8579 - val_loss: 60.7828 - val_reconstruction_loss: 51.2362 - val_KL_loss: 9.5466 - lr: 0.0020\n",
      "Epoch 283/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.4354 - reconstruction_loss: 55.1652 - KL_loss: 9.8746 - val_loss: 60.7215 - val_reconstruction_loss: 51.1345 - val_KL_loss: 9.5870 - lr: 0.0020\n",
      "Epoch 284/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.0543 - reconstruction_loss: 55.0444 - KL_loss: 9.9201 - val_loss: 60.7734 - val_reconstruction_loss: 51.0883 - val_KL_loss: 9.6851 - lr: 0.0020\n",
      "Epoch 285/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.2615 - reconstruction_loss: 55.0048 - KL_loss: 10.0134 - val_loss: 60.6889 - val_reconstruction_loss: 50.9910 - val_KL_loss: 9.6980 - lr: 0.0020\n",
      "Epoch 286/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 114.8885 - reconstruction_loss: 54.9270 - KL_loss: 10.0253 - val_loss: 60.7326 - val_reconstruction_loss: 51.1071 - val_KL_loss: 9.6255 - lr: 0.0018\n",
      "Epoch 287/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 115.0699 - reconstruction_loss: 55.0358 - KL_loss: 9.9588 - val_loss: 60.7048 - val_reconstruction_loss: 51.0827 - val_KL_loss: 9.6222 - lr: 0.0018\n",
      "Epoch 288/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 114.8442 - reconstruction_loss: 55.0000 - KL_loss: 9.9546 - val_loss: 60.7424 - val_reconstruction_loss: 51.1012 - val_KL_loss: 9.6412 - lr: 0.0018\n",
      "Epoch 289/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 114.8718 - reconstruction_loss: 55.0140 - KL_loss: 9.9687 - val_loss: 60.6931 - val_reconstruction_loss: 51.0720 - val_KL_loss: 9.6212 - lr: 0.0018\n",
      "Epoch 290/1000\n",
      "412406/412406 [==============================] - 8s 19us/sample - loss: 114.6271 - reconstruction_loss: 54.9994 - KL_loss: 9.9516 - val_loss: 60.7667 - val_reconstruction_loss: 51.1948 - val_KL_loss: 9.5719 - lr: 0.0018\n",
      "Epoch 291/1000\n",
      "412406/412406 [==============================] - 8s 20us/sample - loss: 114.7809 - reconstruction_loss: 55.0977 - KL_loss: 9.9052 - val_loss: 60.7544 - val_reconstruction_loss: 51.1838 - val_KL_loss: 9.5706 - lr: 0.0018\n",
      "\n",
      "training finished in 291 epochs (early stop), transform data to adjust the platform effect...\n",
      "\n",
      "WARNING: when transforming data, after reversed Min-Max Scaling, apply exp transformation then multiple the factor and round to integer\n",
      "\n",
      "re-run DE on CVAE transformed scRNA-seq data!\n",
      "filtering genes present in <10 cells: 5 genes removed\n",
      "\n",
      "Differential analysis across cell-types on scRNA-seq data...\n",
      "0%...8%...WARNING: only 4 genes passing filtering (<20) for Endo vs Smc\n",
      "17%...WARNING: only 17 genes passing filtering (<20) for Micro vs PVALB\n",
      "25%...33%...42%...WARNING: only 15 genes passing filtering (<20) for SST vs PVALB\n",
      "WARNING: only 19 genes passing filtering (<20) for SST vs VIP\n",
      "50%...58%...67%...75%...WARNING: only 12 genes passing filtering (<20) for eL4 vs eL5\n",
      "83%...WARNING: only 18 genes passing filtering (<20) for eL5 vs eL2/3\n",
      "WARNING: only 18 genes passing filtering (<20) for eL5 vs eL4\n",
      "WARNING: only 19 genes passing filtering (<20) for eL5 vs eL6\n",
      "92%...WARNING: only 10 genes passing filtering (<20) for eL6 vs eL5\n",
      "finally selected 424 cell-type marker genes\n",
      "\n",
      "\n",
      "save variables related to CVAE to files!\n",
      "\n",
      "platform effect adjustment by CVAE finished. Elapsed time: 83.84 minutes.\n",
      "\n",
      "\n",
      "use the marker genes derived from CVAE transformed scRNA-seq for downstream regression!\n",
      "\n",
      "gene filtering before modeling...\n",
      "all genes passed filtering\n",
      "\n",
      "spot filtering before modeling...\n",
      "all spots passed filtering\n",
      "\n",
      "\n",
      "######### Start GLRM modeling... #########\n",
      "\n",
      "GLRM settings:\n",
      "use SciPy minimize method:  L-BFGS-B\n",
      "global optimization turned off, local minimum will be used in GLRM\n",
      "use hybrid version of GLRM\n",
      "Numba detected total 64 available CPU cores. Use 64 CPU cores\n",
      "use 2001 points to calculate the heavy-tail density\n",
      "use weight threshold for Adaptive Lasso:  0.001\n",
      "total 372 unique nUMIs, min: 0.0, max: 2019.0\n",
      "\n",
      "Build graph: \n",
      " Graph with 581 nodes and 1029 edges\n",
      "\n",
      "estimation of gene-specific platform effect gamma_g is skipped as already using CVAE to adjust platform effect\n",
      "\n",
      "\n",
      "Start GLRM fitting...\n",
      "\n",
      "first estimate MLE theta and corresponding e^alpha and sigma^2...\n",
      "\n",
      "GLRM model initialization...\n",
      "calculate MLE theta and sigma^2 iteratively...\n",
      "  iter | time_opt | time_sig | sigma2\n",
      "     0 |   17.583 |    4.093 |  0.333\n",
      "     1 |   12.751 |    2.552 |  0.202\n",
      "     2 |   10.770 |    2.551 |  0.174\n",
      "     3 |    8.811 |    2.191 |  0.168\n",
      "     4 |    7.149 |    2.194 |  0.167\n",
      "     5 |    4.812 |    1.827 |  0.167\n",
      "MLE theta and sigma^2 calculation finished. Elapsed time: 1.29 minutes.\n",
      "MLE theta estimation finished. Elapsed time: 1.29 minutes.\n",
      "\n",
      "calculate weights of Adaptive Lasso...\n",
      "\n",
      "Stage 1: variable selection using Adaptive Lasso starts with the MLE theta and e^alpha, using already estimated sigma^2 and gamma_g...\n",
      "specified hyper-parameter for Adaptive Lasso is: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "hyper-parameter for Adaptive Lasso: use cross-validation to find the optimal value from 8 candidates...\n",
      "\n",
      "Start cross-validation for hyper-parameter lambda_r...\n",
      "directly estimate theta by Adaptive Lasso loss function as NO Graph Laplacian constrain!\n",
      "0%...11%...22%...33%...44%...56%...67%...78%...early stop\n",
      "find optimal lambda_r 1.931 with average negative log-likelihood 60004.5246 by 5 fold cross-validation. Elapsed time: 8.20 minutes.\n",
      "\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |     12.909 |     12.909 |      0.131 |      0.131 |       1.00 |       1.00 |    3.058 |    0.000 |    0.004 |   0.212818 |   0.106409\n",
      "     1 |     12.902 |      0.034 |      0.131 |      0.144 |       1.00 |       2.00 |    5.903 |    0.000 |    0.003 |   0.212680 |   0.106340\n",
      "     2 |     12.831 |      0.313 |      0.131 |      0.170 |       2.00 |       4.00 |    6.341 |    0.000 |    0.003 |   0.211701 |   0.105851\n",
      "     3 |      9.987 |     17.338 |      0.135 |      0.209 |       4.00 |       8.00 |    8.068 |    0.000 |    0.003 |   0.167269 |   0.083634\n",
      "     4 |      7.103 |     35.761 |      0.154 |      0.260 |       8.00 |       8.00 |    7.684 |    0.000 |    0.003 |   0.109099 |   0.054550\n",
      "     5 |      5.366 |     33.820 |      0.152 |      0.289 |       8.00 |       8.00 |    7.321 |    0.000 |    0.003 |   0.080348 |   0.040174\n",
      "     6 |      4.880 |     22.528 |      0.141 |      0.307 |       8.00 |      16.00 |    7.387 |    0.000 |    0.003 |   0.077630 |   0.038815\n",
      "     7 |      3.969 |     25.803 |      0.144 |      0.344 |      16.00 |      16.00 |    7.869 |    0.000 |    0.003 |   0.061065 |   0.030533\n",
      "     8 |      3.135 |     29.475 |      0.148 |      0.378 |      16.00 |      16.00 |    7.053 |    0.000 |    0.003 |   0.043863 |   0.021932\n",
      "     9 |      2.793 |     22.870 |      0.141 |      0.406 |      16.00 |      32.00 |    6.987 |    0.000 |    0.003 |   0.040158 |   0.020079\n",
      "    10 |      2.394 |     25.873 |      0.144 |      0.461 |      32.00 |      32.00 |    7.211 |    0.000 |    0.003 |   0.034325 |   0.017163\n",
      "    11 |      2.044 |     29.134 |      0.147 |      0.509 |      32.00 |      32.00 |    6.812 |    0.000 |    0.003 |   0.028265 |   0.014133\n",
      "    12 |      1.831 |     24.898 |      0.143 |      0.549 |      32.00 |      64.00 |    6.742 |    0.000 |    0.003 |   0.025151 |   0.012575\n",
      "    13 |      1.600 |     31.553 |      0.150 |      0.624 |      64.00 |      64.00 |    7.282 |    0.000 |    0.003 |   0.022116 |   0.011058\n",
      "    14 |      1.367 |     36.908 |      0.155 |      0.688 |      64.00 |      64.00 |    7.109 |    0.000 |    0.003 |   0.018915 |   0.009458\n",
      "    15 |      1.210 |     33.424 |      0.152 |      0.740 |      64.00 |     128.00 |    6.975 |    0.000 |    0.003 |   0.016896 |   0.008448\n",
      "    16 |      1.044 |     43.377 |      0.161 |      0.834 |     128.00 |     128.00 |    7.246 |    0.000 |    0.003 |   0.014712 |   0.007356\n",
      "    17 |      0.885 |     49.150 |      0.167 |      0.913 |     128.00 |     128.00 |    7.127 |    0.000 |    0.003 |   0.012448 |   0.006224\n",
      "    18 |      0.790 |     42.692 |      0.161 |      0.978 |     128.00 |     256.00 |    6.830 |    0.000 |    0.003 |   0.011122 |   0.005561\n",
      "    19 |      0.677 |     56.423 |      0.175 |      1.097 |     256.00 |     256.00 |    6.799 |    0.000 |    0.003 |   0.009611 |   0.004805\n",
      "    20 |      0.578 |     63.080 |      0.181 |      1.199 |     256.00 |     256.00 |    6.791 |    0.000 |    0.003 |   0.008166 |   0.004083\n",
      "    21 |      0.519 |     53.872 |      0.172 |      1.284 |     256.00 |     512.00 |    6.267 |    0.000 |    0.003 |   0.007342 |   0.003671\n",
      "    22 |      0.456 |     65.487 |      0.184 |      1.446 |     512.00 |     512.00 |    6.558 |    0.000 |    0.003 |   0.006466 |   0.003233\n",
      "    23 |      0.393 |     76.853 |      0.195 |      1.589 |     512.00 |     512.00 |    6.372 |    0.000 |    0.003 |   0.005528 |   0.002764\n",
      "    24 |      0.345 |     72.387 |      0.190 |      1.708 |     512.00 |    1024.00 |    7.081 |    0.000 |    0.003 |   0.004832 |   0.002416\n",
      "    25 |      0.297 |     95.500 |      0.214 |      1.921 |    1024.00 |    1024.00 |    6.348 |    0.000 |    0.003 |   0.004133 |   0.002067\n",
      "    26 |      0.254 |    104.599 |      0.223 |      2.100 |    1024.00 |    1024.00 |    6.471 |    0.000 |    0.003 |   0.003493 |   0.001746\n",
      "    27 |      0.228 |     91.215 |      0.209 |      2.251 |    1024.00 |    2048.00 |    6.888 |    0.000 |    0.003 |   0.003108 |   0.001554\n",
      "    28 |      0.195 |    118.307 |      0.236 |      2.527 |    2048.00 |    2048.00 |   24.268 |    0.000 |    0.003 |   0.002646 |   0.001323\n",
      "    29 |      0.165 |    139.366 |      0.257 |      2.762 |    2048.00 |    2048.00 |    6.062 |    0.000 |    0.003 |   0.002194 |   0.001097\n",
      "    30 |      0.143 |    127.854 |      0.246 |      2.951 |    2048.00 |    4096.00 |    6.034 |    0.000 |    0.003 |   0.001904 |   0.000952\n",
      "    31 |      0.123 |    159.186 |      0.277 |      3.291 |    4096.00 |    4096.00 |    6.105 |    0.000 |    0.003 |   0.001623 |   0.000812\n",
      "    32 |      0.108 |    213.083 |      0.331 |      3.582 |    4096.00 |    4096.00 |    5.862 |    0.000 |    0.003 |   0.001375 |   0.000687\n",
      "    33 |      0.093 |    193.712 |      0.312 |      3.827 |    4096.00 |    8192.00 |    6.000 |    0.000 |    0.003 |   0.001184 |   0.000592\n",
      "    34 |      0.078 |    216.389 |      0.334 |      4.264 |    8192.00 |    8192.00 |    5.683 |    0.000 |    0.003 |   0.000991 |   0.000495\n",
      "    35 |      0.066 |    234.913 |      0.353 |      4.632 |    8192.00 |    8192.00 |    5.571 |    0.000 |    0.003 |   0.000817 |   0.000408\n",
      "    36 |      0.058 |    209.679 |      0.328 |      4.936 |    8192.00 |   16384.00 |    5.724 |    0.000 |    0.003 |   0.000710 |   0.000355\n",
      "    37 |      0.049 |    259.747 |      0.378 |      5.473 |   16384.00 |   16384.00 |    5.660 |    0.000 |    0.003 |   0.000588 |   0.000294\n",
      "    38 |      0.041 |    281.958 |      0.400 |      5.918 |   16384.00 |   16384.00 |    5.648 |    0.000 |    0.003 |   0.000476 |   0.000238\n",
      "    39 |      0.036 |    256.116 |      0.374 |      6.282 |   16384.00 |   32768.00 |    5.128 |    0.000 |    0.003 |   0.000409 |   0.000205\n",
      "    40 |      0.030 |    316.257 |      0.434 |      6.937 |   32768.00 |   32768.00 |    5.221 |    0.000 |    0.003 |   0.000342 |   0.000171\n",
      "    41 |      0.026 |    344.046 |      0.462 |      7.502 |   32768.00 |   32768.00 |    4.897 |    0.000 |    0.003 |   0.000282 |   0.000141\n",
      "    42 |      0.023 |    319.373 |      0.437 |      7.988 |   32768.00 |   65536.00 |    4.785 |    0.000 |    0.003 |   0.000245 |   0.000123\n",
      "    43 |      0.019 |    422.100 |      0.540 |      8.860 |   65536.00 |   65536.00 |    4.592 |    0.000 |    0.003 |   0.000203 |   0.000101\n",
      "    44 |      0.016 |    468.968 |      0.587 |      9.594 |   65536.00 |   65536.00 |    4.686 |    0.000 |    0.003 |   0.000165 |   0.000082\n",
      "    45 |      0.014 |    459.887 |      0.578 |     10.176 |   65536.00 |  131072.00 |    4.517 |    0.000 |    0.003 |   0.000137 |   0.000069\n",
      "    46 |      0.011 |    565.037 |      0.683 |     11.177 |  131072.00 |  131072.00 |    4.112 |    0.000 |    0.003 |   0.000111 |   0.000056\n",
      "    47 |      0.009 |    575.302 |      0.693 |     11.995 |  131072.00 |  131072.00 |    4.134 |    0.000 |    0.003 |   0.000089 |   0.000045\n",
      "    48 |      0.009 |    716.211 |      0.834 |     12.632 |  131072.00 |  262144.00 |    4.088 |    0.000 |    0.003 |   0.000076 |   0.000038\n",
      "    49 |      0.007 |    912.356 |      1.030 |     13.765 |  262144.00 |          / |    3.782 |    0.000 |    0.003 |   0.000060 |   0.000030\n",
      "early stop!\n",
      "Terminated (optimal) in 50 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 5.40 minutes.\n",
      "\n",
      "Stage 1 variable selection finished. Elapsed time: 13.61 minutes.\n",
      "\n",
      "Stage 2: final theta estimation with Graph Laplacian Constrain using already estimated sigma^2 and gamma_g\n",
      "specified hyper-parameter for Graph Laplacian Constrain is: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "hyper-parameter for Graph Laplacian Constrain: use cross-validation to find the optimal value from 8 candidates...\n",
      "\n",
      "Start cross-validation for hyper-parameter lambda_g...\n",
      "still use ADMM even NO Graph Laplacian constrain (lambda_g=0)\n",
      "0%...11%...22%...33%...44%...56%...67%...78%...89%...early stop\n",
      "find optimal lambda_g 5.179 with average negative log-likelihood 61841.4897 by 5 fold cross-validation. Elapsed time: 62.92 minutes.\n",
      "\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |     14.188 |     12.757 |      0.132 |      0.132 |       1.00 |       1.00 |    8.716 |    0.000 |    0.007 |   0.137003 |   0.135418\n",
      "     1 |     11.516 |      8.246 |      0.130 |      0.141 |       1.00 |       1.00 |    3.951 |    0.000 |    0.008 |   0.155658 |   0.116606\n",
      "     2 |      9.478 |      9.376 |      0.128 |      0.149 |       1.00 |       1.00 |    3.981 |    0.000 |    0.007 |   0.111633 |   0.100270\n",
      "     3 |      8.383 |     10.316 |      0.128 |      0.157 |       1.00 |       2.00 |    3.741 |    0.000 |    0.007 |   0.089894 |   0.088752\n",
      "     4 |      7.710 |     16.024 |      0.134 |      0.172 |       2.00 |       2.00 |    4.686 |    0.000 |    0.007 |   0.083971 |   0.081873\n",
      "     5 |      6.735 |     17.106 |      0.135 |      0.185 |       2.00 |       2.00 |    4.296 |    0.000 |    0.007 |   0.076465 |   0.071650\n",
      "     6 |      5.829 |     18.409 |      0.136 |      0.196 |       2.00 |       4.00 |    4.256 |    0.000 |    0.008 |   0.062190 |   0.062433\n",
      "     7 |      5.210 |     27.258 |      0.145 |      0.217 |       4.00 |       4.00 |    4.713 |    0.000 |    0.007 |   0.057914 |   0.055269\n",
      "     8 |      4.295 |     28.647 |      0.147 |      0.233 |       4.00 |       4.00 |    4.632 |    0.000 |    0.007 |   0.049627 |   0.045273\n",
      "     9 |      3.421 |     30.298 |      0.148 |      0.246 |       4.00 |       8.00 |    4.270 |    0.000 |    0.007 |   0.035664 |   0.036345\n",
      "    10 |      2.866 |     41.949 |      0.160 |      0.268 |       8.00 |       8.00 |    4.838 |    0.000 |    0.007 |   0.031885 |   0.029944\n",
      "    11 |      2.118 |     43.461 |      0.162 |      0.284 |       8.00 |       8.00 |    4.380 |    0.000 |    0.007 |   0.024811 |   0.021919\n",
      "    12 |      1.455 |     44.959 |      0.163 |      0.295 |       8.00 |      16.00 |    3.905 |    0.000 |    0.007 |   0.014216 |   0.015197\n",
      "    13 |      1.133 |     57.226 |      0.175 |      0.311 |      16.00 |      16.00 |    4.463 |    0.000 |    0.006 |   0.011865 |   0.011566\n",
      "    14 |      0.724 |     58.284 |      0.176 |      0.321 |      16.00 |      16.00 |    3.822 |    0.000 |    0.006 |   0.008894 |   0.007213\n",
      "    15 |      0.383 |     59.201 |      0.177 |      0.326 |      16.00 |      32.00 |    3.000 |    0.000 |    0.006 |   0.003268 |   0.003798\n",
      "    16 |      0.304 |     69.359 |      0.187 |      0.332 |      32.00 |      32.00 |    3.484 |    0.000 |    0.005 |   0.002180 |   0.002922\n",
      "    17 |      0.184 |     69.822 |      0.188 |      0.336 |      32.00 |      32.00 |    3.280 |    0.000 |    0.005 |   0.002525 |   0.001626\n",
      "    18 |      0.074 |     70.284 |      0.188 |      0.336 |      32.00 |      64.00 |    2.408 |    0.000 |    0.005 |   0.000810 |   0.000586\n",
      "    19 |      0.085 |     77.447 |      0.196 |      0.338 |      64.00 |      64.00 |    2.465 |    0.000 |    0.005 |   0.000325 |   0.000649\n",
      "    20 |      0.052 |     77.596 |      0.196 |      0.339 |      64.00 |      64.00 |    2.980 |    0.000 |    0.005 |   0.000751 |   0.000403\n",
      "    21 |      0.016 |     77.781 |      0.196 |      0.339 |      64.00 |     128.00 |    2.262 |    0.000 |    0.005 |   0.000220 |   0.000138\n",
      "    22 |      0.025 |     82.236 |      0.200 |      0.340 |     128.00 |     128.00 |    2.153 |    0.000 |    0.004 |   0.000069 |   0.000174\n",
      "    23 |      0.015 |     82.270 |      0.200 |      0.341 |     128.00 |     128.00 |    2.433 |    0.000 |    0.004 |   0.000216 |   0.000111\n",
      "    24 |      0.005 |     82.324 |      0.200 |      0.341 |     128.00 |     256.00 |    2.103 |    0.000 |    0.004 |   0.000068 |   0.000037\n",
      "    25 |      0.007 |     84.838 |      0.203 |      0.342 |     256.00 |          / |    2.113 |    0.000 |    0.004 |   0.000022 |   0.000049\n",
      "early stop!\n",
      "Terminated (optimal) in 26 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 1.63 minutes.\n",
      "\n",
      "\n",
      "stage 2 finished. Elapsed time: 64.55 minutes.\n",
      "\n",
      "GLRM fitting finished. Elapsed time: 79.45 minutes.\n",
      "\n",
      "\n",
      "Post-processing estimated cell-type proportion theta...\n",
      "hard thresholding small theta values with threshold 0\n",
      "\n",
      "\n",
      "cell type deconvolution finished. Estimate results saved in /home/exouser/Spatial/celltype_proportions.csv. Elapsed time: 2.72 hours.\n",
      "\n",
      "\n",
      "######### No imputation #########\n",
      "\n",
      "\n",
      "whole pipeline finished. Total elapsed time: 2.72 hours.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='runDeconvolution -q sim_seq_based_6x_spatial_spot_nUMI.csv                           -r GSE102827_scRNA_cell_nUMI.csv                           -c GSE102827_6x_scRNA_cell_celltype.csv                           -a sim_spatial_spot_adjacency_matrix.csv                           --n_marker_per_cmp 20                           -n 64                           --cvae_init_lr 0.003                           --num_hidden_layer 1                           --use_batch_norm false                           --cvae_train_epoch 1000                           --diagnosis true\\n', returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = '''runDeconvolution -q sim_seq_based_6x_spatial_spot_nUMI.csv \\\n",
    "                          -r GSE102827_scRNA_cell_nUMI.csv \\\n",
    "                          -c GSE102827_6x_scRNA_cell_celltype.csv \\\n",
    "                          -a sim_spatial_spot_adjacency_matrix.csv \\\n",
    "                          --n_marker_per_cmp 20 \\\n",
    "                          -n 64 \\\n",
    "                          --cvae_init_lr 0.003 \\\n",
    "                          --num_hidden_layer 1 \\\n",
    "                          --use_batch_norm false \\\n",
    "                          --cvae_train_epoch 1000 \\\n",
    "                          --diagnosis true\n",
    "'''\n",
    "\n",
    "subprocess.run(cmd, check=True, text=True, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
