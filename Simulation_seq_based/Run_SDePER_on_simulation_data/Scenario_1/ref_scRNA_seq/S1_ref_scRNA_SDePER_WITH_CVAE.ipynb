{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db496b92",
   "metadata": {},
   "source": [
    "# Run *SDePER* on sequencing-based simulated data: Scenario 1 + scRNA-seq data as reference + WITH CVAE\n",
    "\n",
    "In this Notebook we run SDePER on simulated data. For generating **sequencing-based** simulated data via coarse-graining procedure please refer [generate_simulated_spatial_data.nb.html](https://rawcdn.githack.com/az7jh2/SDePER_Analysis/c963d08f74f4591c2ef6f132177795297793d878/Simulation_seq_based/Generate_simulation_data/generate_simulated_spatial_data.nb.html) in [Simulation_seq_based](https://github.com/az7jh2/SDePER_Analysis/tree/main/Simulation_seq_based) folder.\n",
    "\n",
    "**Scenario 1** means the reference data for deconvolution includes all single cells with the **matched 12 cell types**.\n",
    "\n",
    "**scRNA-seq data as reference** means the reference data is another scRNA-seq data ([GSE115746](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115746)) from the same tissue with simulated spatial data, therefore **platform effect exists**.\n",
    "\n",
    "**WITH CVAE** means we use CVAE to remove platform effect.\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "So here we use the **4 input files** as shown below:\n",
    "\n",
    "1. raw nUMI counts of simulated spatial transcriptomic data (spots × genes): [sim_seq_based_spatial_spot_nUMI.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation_seq_based/Generate_simulation_data/sim_seq_based_spatial_spot_nUMI.csv)\n",
    "2. raw nUMI counts of reference scRNA-seq data (cells × genes): `scRNA_data_full.csv`. Since the file size of csv file of raw nUMI matrix of all 23,178 cells and 45,768 genes is up to 2.3 GB, we do not provide this file in our repository. It's just a **matrix transpose** of [GSE115746_cells_exon_counts.csv.gz](https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE115746&format=file&file=GSE115746%5Fcells%5Fexon%5Fcounts%2Ecsv%2Egz) in [GSE115746](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115746) to satisty the file format requirement that rows as cells and columns as genes.\n",
    "3. cell type annotations for cells of **the matched 12 cell types** in reference scRNA-seq data (cells × 1): [ref_scRNA_cell_celltype.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Run_SDePER_on_simulation_data/Scenario_1/ref_scRNA_seq/ref_scRNA_cell_celltype.csv)\n",
    "4. adjacency matrix of spots in simulated spatial transcriptomic data (spots × spots): [sim_spatial_spot_adjacency_matrix.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Generate_simulation_data/sim_spatial_spot_adjacency_matrix.csv)\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "SDePER settings are:\n",
    "\n",
    "* number of selected TOP marker genes for each comparison in Differential `n_marker_per_cmp`: 20\n",
    "* number of used CPU cores `n_core`: 64\n",
    "* initial learning rate for training CVAE `cvae_init_lr`: 0.003\n",
    "* number of hidden layers in encoder and decoder of CVAE `num_hidden_layer`: 1\n",
    "* whether to use Batch Normalization `use_batch_norm`: false\n",
    "* CVAE training epochs `cvae_train_epoch`: 1000\n",
    "* for diagnostic purposes set `diagnosis`: true\n",
    "\n",
    "ALL other options are left as default.\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "the `bash` command to start cell type deconvolution is\n",
    "\n",
    "`runDeconvolution -q sim_seq_based_spatial_spot_nUMI.csv -r scRNA_data_full.csv -c ref_scRNA_cell_celltype.csv -a sim_spatial_spot_adjacency_matrix.csv --n_marker_per_cmp 20 -n 64 --cvae_init_lr 0.003 --num_hidden_layer 1 --use_batch_norm false --cvae_train_epoch 1000 --diagnosis true`\n",
    "\n",
    "Note this Notebook uses **SDePER v1.2.1**. Cell type deconvolution result is renamed as [S1_ref_scRNA_SDePER_WITH_CVAE_celltype_proportions.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation_seq_based/Run_SDePER_on_simulation_data/Scenario_1/ref_scRNA_seq/S1_ref_scRNA_SDePER_WITH_CVAE_celltype_proportions.csv). Folder of diagnostic plots is compressed and renamed as [S1_ref_scRNA_SDePER_WITH_CVAE_diagnosis.tar](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation_seq_based/Run_SDePER_on_simulation_data/Scenario_1/ref_scRNA_seq/S1_ref_scRNA_SDePER_WITH_CVAE_diagnosis.tar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df9e3939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SDePER (Spatial Deconvolution method with Platform Effect Removal) v1.2.1\n",
      "\n",
      "\n",
      "running options:\n",
      "spatial_file: /home/exouser/Spatial/sim_seq_based_spatial_spot_nUMI.csv\n",
      "ref_file: /home/exouser/Spatial/scRNA_data_full.csv\n",
      "ref_celltype_file: /home/exouser/Spatial/ref_scRNA_cell_celltype.csv\n",
      "marker_file: None\n",
      "loc_file: None\n",
      "A_file: /home/exouser/Spatial/sim_spatial_spot_adjacency_matrix.csv\n",
      "n_cores: 64\n",
      "threshold: 0\n",
      "use_cvae: True\n",
      "use_imputation: False\n",
      "diagnosis: True\n",
      "verbose: True\n",
      "use_fdr: True\n",
      "p_val_cutoff: 0.05\n",
      "fc_cutoff: 1.2\n",
      "pct1_cutoff: 0.3\n",
      "pct2_cutoff: 0.1\n",
      "sortby_fc: True\n",
      "n_marker_per_cmp: 20\n",
      "filter_cell: True\n",
      "filter_gene: True\n",
      "n_hv_gene: 200\n",
      "n_pseudo_spot: 500000\n",
      "pseudo_spot_min_cell: 2\n",
      "pseudo_spot_max_cell: 8\n",
      "seq_depth_scaler: 10000\n",
      "cvae_input_scaler: 10\n",
      "cvae_init_lr: 0.003\n",
      "num_hidden_layer: 1\n",
      "use_batch_norm: False\n",
      "cvae_train_epoch: 1000\n",
      "use_spatial_pseudo: False\n",
      "redo_de: True\n",
      "seed: 383\n",
      "lambda_r: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "lambda_g: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "diameter: 200\n",
      "impute_diameter: [160, 114, 80]\n",
      "\n",
      "\n",
      "######### Preprocessing... #########\n",
      "\n",
      "first build CVAE...\n",
      "\n",
      "read spatial data from file /home/exouser/Spatial/sim_seq_based_spatial_spot_nUMI.csv\n",
      "total 581 spots; 25187 genes\n",
      "\n",
      "filtering genes present in <3 spots: 9510 genes removed\n",
      "\n",
      "read scRNA-seq data from file /home/exouser/Spatial/scRNA_data_full.csv\n",
      "total 23178 cells; 45768 genes\n",
      "read scRNA-seq cell-type annotation from file /home/exouser/Spatial/ref_scRNA_cell_celltype.csv\n",
      "total 12 cell-types\n",
      "subset cells with cell-type annotation, finally keep 11835 cells; 45768 genes\n",
      "\n",
      "filtering cells with <200 genes: No cells removed\n",
      "\n",
      "filtering genes present in <10 cells: 12063 genes removed\n",
      "\n",
      "total 14682 overlapped genes\n",
      "\n",
      "identify 200 highly variable genes from scRNA-seq data...\n",
      "\n",
      "identify cell-type marker genes...\n",
      "no marker gene profile provided. Perform DE to get cell-type marker genes on scRNA-seq data...\n",
      "\n",
      "Differential analysis across cell-types on scRNA-seq data...\n",
      "0%...8%...17%...25%...33%...42%...50%...58%...67%...75%...83%...92%...finally selected 748 cell-type marker genes\n",
      "\n",
      "\n",
      "use union of highly variable gene list and cell-type marker gene list derived from scRNA-seq data, finally get 829 genes for downstream analysis\n",
      "\n",
      "start CVAE building...\n",
      "\n",
      "generate 500000 pseudo-spots containing 2 to 8 cells from scRNA-seq cells...\n",
      "10%...20%...30%...40%...50%...60%...70%...80%...90%...100%...\n",
      "\n",
      "generate 0 pseudo-spots containing 2 to 6 spots from spatial spots...\n",
      "\n",
      "WARNING: first apply log transformation on sequencing depth normalized gene expressions, followed by Min-Max scaling\n",
      "\n",
      "                         |  training | validation\n",
      "spatial spots            |       581 |         0\n",
      "spatial pseudo-spots     |         0 |         0\n",
      "scRNA-seq cells          |     11835 |         0\n",
      "scRNA-seq pseudo-spots   |    400000 |    100000\n",
      "\n",
      "scaling inputs to range 0 to 10\n",
      "\n",
      "CVAE structure:\n",
      "Encoder: 830 - 172 - 36\n",
      "Decoder: 37 - 172 - 829\n",
      "\n",
      "\n",
      "Start training...\n",
      "\n",
      "Train on 412416 samples, validate on 100000 samples\n",
      "Epoch 1/1000\n",
      "412416/412416 [==============================] - 16s 40us/sample - loss: 612.4476 - reconstruction_loss: 398.5582 - KL_loss: 23.4396 - val_loss: 421.1667 - val_reconstruction_loss: 383.5538 - val_KL_loss: 37.6129 - lr: 0.0030\n",
      "Epoch 2/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 619.5544 - reconstruction_loss: 399.2352 - KL_loss: 38.2207 - val_loss: 373.7478 - val_reconstruction_loss: 342.6559 - val_KL_loss: 31.0919 - lr: 0.0030\n",
      "Epoch 3/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 556.6381 - reconstruction_loss: 357.2718 - KL_loss: 31.6878 - val_loss: 329.1243 - val_reconstruction_loss: 293.9166 - val_KL_loss: 35.2076 - lr: 0.0030\n",
      "Epoch 4/1000\n",
      "412416/412416 [==============================] - 11s 26us/sample - loss: 499.7844 - reconstruction_loss: 306.8799 - KL_loss: 36.0332 - val_loss: 304.7878 - val_reconstruction_loss: 275.4344 - val_KL_loss: 29.3534 - lr: 0.0030\n",
      "Epoch 5/1000\n",
      "412416/412416 [==============================] - 11s 26us/sample - loss: 465.6989 - reconstruction_loss: 287.4109 - KL_loss: 30.0841 - val_loss: 278.0384 - val_reconstruction_loss: 244.4417 - val_KL_loss: 33.5967 - lr: 0.0030\n",
      "Epoch 6/1000\n",
      "412416/412416 [==============================] - 11s 26us/sample - loss: 432.5580 - reconstruction_loss: 255.5308 - KL_loss: 34.5550 - val_loss: 254.2064 - val_reconstruction_loss: 226.8708 - val_KL_loss: 27.3356 - lr: 0.0030\n",
      "Epoch 7/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 400.8260 - reconstruction_loss: 237.3403 - KL_loss: 28.1521 - val_loss: 234.0262 - val_reconstruction_loss: 206.3951 - val_KL_loss: 27.6311 - lr: 0.0030\n",
      "Epoch 8/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 274.5157 - reconstruction_loss: 141.1525 - KL_loss: 26.5007 - val_loss: 166.3871 - val_reconstruction_loss: 137.7870 - val_KL_loss: 28.6001 - lr: 0.0030\n",
      "Epoch 19/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 283.7426 - reconstruction_loss: 146.5720 - KL_loss: 28.9775 - val_loss: 164.2413 - val_reconstruction_loss: 135.8737 - val_KL_loss: 28.3676 - lr: 0.0030\n",
      "Epoch 20/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 281.2051 - reconstruction_loss: 144.5246 - KL_loss: 28.7698 - val_loss: 151.1076 - val_reconstruction_loss: 125.1716 - val_KL_loss: 25.9360 - lr: 0.0030\n",
      "Epoch 21/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 265.0558 - reconstruction_loss: 133.6904 - KL_loss: 26.2943 - val_loss: 150.7514 - val_reconstruction_loss: 125.7197 - val_KL_loss: 25.0317 - lr: 0.0030\n",
      "Epoch 22/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 263.9682 - reconstruction_loss: 134.3442 - KL_loss: 25.3453 - val_loss: 149.2573 - val_reconstruction_loss: 123.3433 - val_KL_loss: 25.9139 - lr: 0.0030\n",
      "Epoch 23/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 261.5450 - reconstruction_loss: 131.6073 - KL_loss: 26.2488 - val_loss: 144.5921 - val_reconstruction_loss: 119.0155 - val_KL_loss: 25.5766 - lr: 0.0030\n",
      "Epoch 24/1000\n",
      "412416/412416 [==============================] - 13s 31us/sample - loss: 256.0234 - reconstruction_loss: 127.2350 - KL_loss: 25.9384 - val_loss: 149.0462 - val_reconstruction_loss: 124.7724 - val_KL_loss: 24.2738 - lr: 0.0030\n",
      "Epoch 25/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 260.4366 - reconstruction_loss: 133.1029 - KL_loss: 24.6313 - val_loss: 147.3234 - val_reconstruction_loss: 123.0861 - val_KL_loss: 24.2373 - lr: 0.0030\n",
      "Epoch 26/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 257.7134 - reconstruction_loss: 131.1497 - KL_loss: 24.6180 - val_loss: 143.0843 - val_reconstruction_loss: 118.2347 - val_KL_loss: 24.8496 - lr: 0.0030\n",
      "Epoch 27/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 253.3414 - reconstruction_loss: 126.1825 - KL_loss: 25.2681 - val_loss: 141.1589 - val_reconstruction_loss: 117.0554 - val_KL_loss: 24.1036 - lr: 0.0030\n",
      "Epoch 28/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 250.9324 - reconstruction_loss: 125.0064 - KL_loss: 24.5001 - val_loss: 142.4957 - val_reconstruction_loss: 119.6654 - val_KL_loss: 22.8303 - lr: 0.0030\n",
      "Epoch 29/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 251.5106 - reconstruction_loss: 127.6627 - KL_loss: 23.1715 - val_loss: 140.4811 - val_reconstruction_loss: 117.2179 - val_KL_loss: 23.2632 - lr: 0.0030\n",
      "Epoch 30/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 248.8298 - reconstruction_loss: 125.1059 - KL_loss: 23.6110 - val_loss: 139.4808 - val_reconstruction_loss: 114.4852 - val_KL_loss: 24.9956 - lr: 0.0030\n",
      "Epoch 31/1000\n",
      "412416/412416 [==============================] - 17s 41us/sample - loss: 247.6599 - reconstruction_loss: 122.2578 - KL_loss: 25.4006 - val_loss: 138.0843 - val_reconstruction_loss: 112.9106 - val_KL_loss: 25.1737 - lr: 0.0030\n",
      "Epoch 32/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 245.6624 - reconstruction_loss: 120.6441 - KL_loss: 25.5803 - val_loss: 138.1515 - val_reconstruction_loss: 114.2682 - val_KL_loss: 23.8833 - lr: 0.0030\n",
      "Epoch 33/1000\n",
      "412416/412416 [==============================] - 13s 31us/sample - loss: 245.5450 - reconstruction_loss: 122.1056 - KL_loss: 24.2406 - val_loss: 136.2877 - val_reconstruction_loss: 112.5604 - val_KL_loss: 23.7273 - lr: 0.0030\n",
      "Epoch 34/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 243.1769 - reconstruction_loss: 120.3203 - KL_loss: 24.0831 - val_loss: 135.4086 - val_reconstruction_loss: 110.6140 - val_KL_loss: 24.7945 - lr: 0.0030\n",
      "Epoch 35/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 241.6753 - reconstruction_loss: 118.2477 - KL_loss: 25.1979 - val_loss: 133.7683 - val_reconstruction_loss: 109.4017 - val_KL_loss: 24.3666 - lr: 0.0030\n",
      "Epoch 36/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 239.6608 - reconstruction_loss: 117.0571 - KL_loss: 24.7677 - val_loss: 134.8014 - val_reconstruction_loss: 112.0148 - val_KL_loss: 22.7866 - lr: 0.0030\n",
      "Epoch 37/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 240.2100 - reconstruction_loss: 119.5305 - KL_loss: 23.1555 - val_loss: 133.2498 - val_reconstruction_loss: 110.5311 - val_KL_loss: 22.7187 - lr: 0.0030\n",
      "Epoch 38/1000\n",
      "412416/412416 [==============================] - 14s 33us/sample - loss: 238.1178 - reconstruction_loss: 118.0229 - KL_loss: 23.1137 - val_loss: 133.2912 - val_reconstruction_loss: 109.2565 - val_KL_loss: 24.0346 - lr: 0.0030\n",
      "Epoch 39/1000\n",
      "412416/412416 [==============================] - 17s 41us/sample - loss: 238.3974 - reconstruction_loss: 116.7489 - KL_loss: 24.5007 - val_loss: 132.1601 - val_reconstruction_loss: 108.3000 - val_KL_loss: 23.8601 - lr: 0.0030\n",
      "Epoch 40/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 236.7925 - reconstruction_loss: 115.6562 - KL_loss: 24.3269 - val_loss: 131.0526 - val_reconstruction_loss: 108.3840 - val_KL_loss: 22.6686 - lr: 0.0030\n",
      "Epoch 41/1000\n",
      "412416/412416 [==============================] - 18s 44us/sample - loss: 235.1214 - reconstruction_loss: 115.6602 - KL_loss: 23.0721 - val_loss: 129.7568 - val_reconstruction_loss: 106.7464 - val_KL_loss: 23.0104 - lr: 0.0030\n",
      "Epoch 42/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 233.5002 - reconstruction_loss: 114.0095 - KL_loss: 23.3993 - val_loss: 130.5728 - val_reconstruction_loss: 106.1813 - val_KL_loss: 24.3916 - lr: 0.0030\n",
      "Epoch 43/1000\n",
      "412416/412416 [==============================] - 15s 37us/sample - loss: 234.0641 - reconstruction_loss: 113.4169 - KL_loss: 24.8252 - val_loss: 128.6581 - val_reconstruction_loss: 104.5655 - val_KL_loss: 24.0927 - lr: 0.0030\n",
      "Epoch 44/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 231.6183 - reconstruction_loss: 111.7217 - KL_loss: 24.5336 - val_loss: 129.3028 - val_reconstruction_loss: 106.6227 - val_KL_loss: 22.6801 - lr: 0.0030\n",
      "Epoch 45/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 231.9765 - reconstruction_loss: 113.7554 - KL_loss: 23.0830 - val_loss: 128.5881 - val_reconstruction_loss: 105.9842 - val_KL_loss: 22.6039 - lr: 0.0030\n",
      "Epoch 46/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 230.6547 - reconstruction_loss: 113.0233 - KL_loss: 23.0150 - val_loss: 126.7642 - val_reconstruction_loss: 103.1150 - val_KL_loss: 23.6492 - lr: 0.0030\n",
      "Epoch 47/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 228.8282 - reconstruction_loss: 110.2079 - KL_loss: 24.1061 - val_loss: 126.0998 - val_reconstruction_loss: 102.4391 - val_KL_loss: 23.6607 - lr: 0.0030\n",
      "Epoch 48/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 227.6828 - reconstruction_loss: 109.4216 - KL_loss: 24.1029 - val_loss: 126.8622 - val_reconstruction_loss: 104.1882 - val_KL_loss: 22.6740 - lr: 0.0030\n",
      "Epoch 49/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 228.1340 - reconstruction_loss: 111.1883 - KL_loss: 23.0677 - val_loss: 125.5150 - val_reconstruction_loss: 102.7812 - val_KL_loss: 22.7339 - lr: 0.0030\n",
      "Epoch 50/1000\n",
      "412416/412416 [==============================] - 13s 32us/sample - loss: 226.2785 - reconstruction_loss: 109.7363 - KL_loss: 23.1420 - val_loss: 125.8424 - val_reconstruction_loss: 102.1706 - val_KL_loss: 23.6717 - lr: 0.0030\n",
      "Epoch 51/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 226.4638 - reconstruction_loss: 108.9867 - KL_loss: 24.1466 - val_loss: 124.8309 - val_reconstruction_loss: 101.2483 - val_KL_loss: 23.5826 - lr: 0.0030\n",
      "Epoch 52/1000\n",
      "412416/412416 [==============================] - 15s 36us/sample - loss: 225.0641 - reconstruction_loss: 108.0467 - KL_loss: 24.0611 - val_loss: 124.6377 - val_reconstruction_loss: 101.8174 - val_KL_loss: 22.8203 - lr: 0.0030\n",
      "Epoch 53/1000\n",
      "412416/412416 [==============================] - 14s 33us/sample - loss: 224.6149 - reconstruction_loss: 108.5872 - KL_loss: 23.2474 - val_loss: 123.8170 - val_reconstruction_loss: 100.8905 - val_KL_loss: 22.9265 - lr: 0.0030\n",
      "Epoch 54/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 223.6038 - reconstruction_loss: 107.6824 - KL_loss: 23.3594 - val_loss: 123.7315 - val_reconstruction_loss: 100.1339 - val_KL_loss: 23.5976 - lr: 0.0030\n",
      "Epoch 55/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 223.4059 - reconstruction_loss: 106.8484 - KL_loss: 24.0934 - val_loss: 122.8483 - val_reconstruction_loss: 99.5431 - val_KL_loss: 23.3052 - lr: 0.0030\n",
      "Epoch 56/1000\n",
      "412416/412416 [==============================] - 13s 32us/sample - loss: 222.1449 - reconstruction_loss: 106.1885 - KL_loss: 23.8012 - val_loss: 123.1997 - val_reconstruction_loss: 100.5986 - val_KL_loss: 22.6011 - lr: 0.0030\n",
      "Epoch 57/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 222.0461 - reconstruction_loss: 107.2538 - KL_loss: 23.0464 - val_loss: 122.5393 - val_reconstruction_loss: 99.8469 - val_KL_loss: 22.6923 - lr: 0.0030\n",
      "Epoch 58/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 221.1220 - reconstruction_loss: 106.4582 - KL_loss: 23.1417 - val_loss: 122.2341 - val_reconstruction_loss: 98.9213 - val_KL_loss: 23.3129 - lr: 0.0030\n",
      "Epoch 59/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 220.6195 - reconstruction_loss: 105.5036 - KL_loss: 23.8192 - val_loss: 121.5125 - val_reconstruction_loss: 98.4740 - val_KL_loss: 23.0385 - lr: 0.0030\n",
      "Epoch 60/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 219.7417 - reconstruction_loss: 105.0717 - KL_loss: 23.5519 - val_loss: 121.8085 - val_reconstruction_loss: 99.5460 - val_KL_loss: 22.2625 - lr: 0.0030\n",
      "Epoch 61/1000\n",
      "412416/412416 [==============================] - 15s 37us/sample - loss: 219.8883 - reconstruction_loss: 106.1433 - KL_loss: 22.7303 - val_loss: 121.2169 - val_reconstruction_loss: 98.8736 - val_KL_loss: 22.3433 - lr: 0.0030\n",
      "Epoch 62/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 218.9375 - reconstruction_loss: 105.4305 - KL_loss: 22.8153 - val_loss: 120.4481 - val_reconstruction_loss: 97.3366 - val_KL_loss: 23.1116 - lr: 0.0030\n",
      "Epoch 63/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 217.9812 - reconstruction_loss: 103.8601 - KL_loss: 23.6330 - val_loss: 119.9239 - val_reconstruction_loss: 96.9537 - val_KL_loss: 22.9702 - lr: 0.0030\n",
      "Epoch 64/1000\n",
      "412416/412416 [==============================] - 13s 30us/sample - loss: 217.1969 - reconstruction_loss: 103.4473 - KL_loss: 23.4883 - val_loss: 120.1681 - val_reconstruction_loss: 97.9783 - val_KL_loss: 22.1898 - lr: 0.0030\n",
      "Epoch 65/1000\n",
      "412416/412416 [==============================] - 13s 31us/sample - loss: 217.1552 - reconstruction_loss: 104.4859 - KL_loss: 22.6621 - val_loss: 119.5866 - val_reconstruction_loss: 97.3859 - val_KL_loss: 22.2007 - lr: 0.0030\n",
      "Epoch 66/1000\n",
      "412416/412416 [==============================] - 23s 55us/sample - loss: 216.4173 - reconstruction_loss: 103.8613 - KL_loss: 22.6782 - val_loss: 119.7300 - val_reconstruction_loss: 96.9238 - val_KL_loss: 22.8062 - lr: 0.0030\n",
      "Epoch 67/1000\n",
      "412416/412416 [==============================] - 18s 42us/sample - loss: 216.3740 - reconstruction_loss: 103.3792 - KL_loss: 23.3245 - val_loss: 119.2497 - val_reconstruction_loss: 96.4369 - val_KL_loss: 22.8128 - lr: 0.0030\n",
      "Epoch 68/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 215.6093 - reconstruction_loss: 102.8566 - KL_loss: 23.3196 - val_loss: 119.2231 - val_reconstruction_loss: 97.1409 - val_KL_loss: 22.0822 - lr: 0.0030\n",
      "Epoch 69/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 215.5138 - reconstruction_loss: 103.5817 - KL_loss: 22.5471 - val_loss: 118.5816 - val_reconstruction_loss: 96.7736 - val_KL_loss: 21.8080 - lr: 0.0030\n",
      "Epoch 70/1000\n",
      "412416/412416 [==============================] - 13s 32us/sample - loss: 214.3534 - reconstruction_loss: 103.1524 - KL_loss: 22.2799 - val_loss: 118.9355 - val_reconstruction_loss: 96.6511 - val_KL_loss: 22.2844 - lr: 0.0030\n",
      "Epoch 71/1000\n",
      "412416/412416 [==============================] - 14s 33us/sample - loss: 215.0204 - reconstruction_loss: 103.0433 - KL_loss: 22.8023 - val_loss: 118.2229 - val_reconstruction_loss: 95.8505 - val_KL_loss: 22.3724 - lr: 0.0030\n",
      "Epoch 72/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 213.8780 - reconstruction_loss: 102.2204 - KL_loss: 22.8822 - val_loss: 118.1453 - val_reconstruction_loss: 95.9991 - val_KL_loss: 22.1462 - lr: 0.0030\n",
      "Epoch 73/1000\n",
      "412416/412416 [==============================] - 16s 38us/sample - loss: 213.4579 - reconstruction_loss: 102.3642 - KL_loss: 22.6115 - val_loss: 117.7430 - val_reconstruction_loss: 95.6368 - val_KL_loss: 22.1061 - lr: 0.0030\n",
      "Epoch 74/1000\n",
      "412416/412416 [==============================] - 16s 39us/sample - loss: 212.8746 - reconstruction_loss: 101.9804 - KL_loss: 22.5782 - val_loss: 117.4603 - val_reconstruction_loss: 95.3365 - val_KL_loss: 22.1238 - lr: 0.0030\n",
      "Epoch 75/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 212.3913 - reconstruction_loss: 101.6550 - KL_loss: 22.6300 - val_loss: 117.0469 - val_reconstruction_loss: 95.0138 - val_KL_loss: 22.0331 - lr: 0.0030\n",
      "Epoch 76/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 211.7882 - reconstruction_loss: 101.3366 - KL_loss: 22.5339 - val_loss: 117.5408 - val_reconstruction_loss: 95.9663 - val_KL_loss: 21.5745 - lr: 0.0030\n",
      "Epoch 77/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 211.9431 - reconstruction_loss: 102.2475 - KL_loss: 22.0427 - val_loss: 116.9333 - val_reconstruction_loss: 95.2554 - val_KL_loss: 21.6779 - lr: 0.0030\n",
      "Epoch 78/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 211.2203 - reconstruction_loss: 101.5623 - KL_loss: 22.1476 - val_loss: 116.4454 - val_reconstruction_loss: 94.6751 - val_KL_loss: 21.7704 - lr: 0.0030\n",
      "Epoch 79/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 210.5827 - reconstruction_loss: 100.9347 - KL_loss: 22.2681 - val_loss: 116.1964 - val_reconstruction_loss: 94.5478 - val_KL_loss: 21.6487 - lr: 0.0030\n",
      "Epoch 80/1000\n",
      "412416/412416 [==============================] - 21s 52us/sample - loss: 210.0293 - reconstruction_loss: 100.7952 - KL_loss: 22.1473 - val_loss: 116.4169 - val_reconstruction_loss: 94.7191 - val_KL_loss: 21.6978 - lr: 0.0030\n",
      "Epoch 81/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 210.3648 - reconstruction_loss: 100.9927 - KL_loss: 22.1715 - val_loss: 116.2145 - val_reconstruction_loss: 94.6763 - val_KL_loss: 21.5382 - lr: 0.0030\n",
      "Epoch 82/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 209.7030 - reconstruction_loss: 100.9189 - KL_loss: 22.0021 - val_loss: 115.0620 - val_reconstruction_loss: 93.4810 - val_KL_loss: 21.5811 - lr: 0.0030\n",
      "Epoch 83/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 208.3203 - reconstruction_loss: 99.7101 - KL_loss: 22.0778 - val_loss: 116.0162 - val_reconstruction_loss: 94.4392 - val_KL_loss: 21.5769 - lr: 0.0030\n",
      "Epoch 84/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 209.2702 - reconstruction_loss: 100.6997 - KL_loss: 22.0734 - val_loss: 115.2571 - val_reconstruction_loss: 94.0076 - val_KL_loss: 21.2495 - lr: 0.0030\n",
      "Epoch 85/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 208.2520 - reconstruction_loss: 100.2275 - KL_loss: 21.7352 - val_loss: 114.8953 - val_reconstruction_loss: 93.3803 - val_KL_loss: 21.5149 - lr: 0.0030\n",
      "Epoch 86/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 207.5722 - reconstruction_loss: 99.5473 - KL_loss: 22.0123 - val_loss: 114.8476 - val_reconstruction_loss: 93.2582 - val_KL_loss: 21.5894 - lr: 0.0030\n",
      "Epoch 87/1000\n",
      "412416/412416 [==============================] - 14s 34us/sample - loss: 207.4772 - reconstruction_loss: 99.4369 - KL_loss: 22.0697 - val_loss: 113.9766 - val_reconstruction_loss: 92.6863 - val_KL_loss: 21.2903 - lr: 0.0030\n",
      "Epoch 88/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 206.2879 - reconstruction_loss: 98.8853 - KL_loss: 21.7836 - val_loss: 114.3142 - val_reconstruction_loss: 93.3289 - val_KL_loss: 20.9853 - lr: 0.0030\n",
      "Epoch 89/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 206.5951 - reconstruction_loss: 99.4569 - KL_loss: 21.4668 - val_loss: 114.3820 - val_reconstruction_loss: 93.2100 - val_KL_loss: 21.1721 - lr: 0.0030\n",
      "Epoch 90/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 206.5351 - reconstruction_loss: 99.3645 - KL_loss: 21.6619 - val_loss: 113.4846 - val_reconstruction_loss: 92.4308 - val_KL_loss: 21.0538 - lr: 0.0030\n",
      "Epoch 91/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 205.1897 - reconstruction_loss: 98.5260 - KL_loss: 21.5398 - val_loss: 114.3257 - val_reconstruction_loss: 92.9377 - val_KL_loss: 21.3880 - lr: 0.0030\n",
      "Epoch 92/1000\n",
      "412416/412416 [==============================] - 16s 39us/sample - loss: 206.1017 - reconstruction_loss: 99.0720 - KL_loss: 21.9004 - val_loss: 113.6127 - val_reconstruction_loss: 92.5396 - val_KL_loss: 21.0731 - lr: 0.0030\n",
      "Epoch 93/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 205.0784 - reconstruction_loss: 98.6441 - KL_loss: 21.5623 - val_loss: 113.3833 - val_reconstruction_loss: 92.7374 - val_KL_loss: 20.6459 - lr: 0.0030\n",
      "Epoch 94/1000\n",
      "412416/412416 [==============================] - 25s 60us/sample - loss: 204.5005 - reconstruction_loss: 98.8217 - KL_loss: 21.1179 - val_loss: 113.7721 - val_reconstruction_loss: 93.0109 - val_KL_loss: 20.7611 - lr: 0.0030\n",
      "Epoch 95/1000\n",
      "412416/412416 [==============================] - 14s 33us/sample - loss: 205.0947 - reconstruction_loss: 99.0854 - KL_loss: 21.2543 - val_loss: 113.2051 - val_reconstruction_loss: 92.5460 - val_KL_loss: 20.6591 - lr: 0.0030\n",
      "Epoch 96/1000\n",
      "412416/412416 [==============================] - 17s 42us/sample - loss: 204.1667 - reconstruction_loss: 98.6485 - KL_loss: 21.1410 - val_loss: 112.1401 - val_reconstruction_loss: 91.4328 - val_KL_loss: 20.7073 - lr: 0.0030\n",
      "Epoch 97/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 202.6973 - reconstruction_loss: 97.4582 - KL_loss: 21.1957 - val_loss: 114.2337 - val_reconstruction_loss: 92.9415 - val_KL_loss: 21.2923 - lr: 0.0030\n",
      "Epoch 98/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 205.1270 - reconstruction_loss: 98.9986 - KL_loss: 21.7914 - val_loss: 113.0750 - val_reconstruction_loss: 91.9388 - val_KL_loss: 21.1362 - lr: 0.0030\n",
      "Epoch 99/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 203.7284 - reconstruction_loss: 98.0307 - KL_loss: 21.6209 - val_loss: 113.8396 - val_reconstruction_loss: 93.4197 - val_KL_loss: 20.4199 - lr: 0.0030\n",
      "Epoch 100/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 204.2119 - reconstruction_loss: 99.4698 - KL_loss: 20.8943 - val_loss: 112.8992 - val_reconstruction_loss: 92.7946 - val_KL_loss: 20.1047 - lr: 0.0030\n",
      "Epoch 101/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 203.1272 - reconstruction_loss: 98.8336 - KL_loss: 20.5897 - val_loss: 113.8951 - val_reconstruction_loss: 93.6065 - val_KL_loss: 20.2886 - lr: 0.0030\n",
      "Epoch 102/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 204.1075 - reconstruction_loss: 99.6224 - KL_loss: 20.7736 - val_loss: 113.6246 - val_reconstruction_loss: 93.0235 - val_KL_loss: 20.6011 - lr: 0.0030\n",
      "Epoch 103/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 203.4070 - reconstruction_loss: 99.0150 - KL_loss: 21.0751 - val_loss: 112.1461 - val_reconstruction_loss: 91.4410 - val_KL_loss: 20.7051 - lr: 0.0030\n",
      "Epoch 104/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 201.8378 - reconstruction_loss: 97.4620 - KL_loss: 21.1801 - val_loss: 112.4769 - val_reconstruction_loss: 91.9965 - val_KL_loss: 20.4804 - lr: 0.0030\n",
      "Epoch 105/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 201.7988 - reconstruction_loss: 97.9825 - KL_loss: 20.9656 - val_loss: 112.2955 - val_reconstruction_loss: 91.8081 - val_KL_loss: 20.4874 - lr: 0.0030\n",
      "Epoch 106/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 201.6032 - reconstruction_loss: 97.8022 - KL_loss: 20.9812 - val_loss: 111.3859 - val_reconstruction_loss: 91.0081 - val_KL_loss: 20.3778 - lr: 0.0030\n",
      "Epoch 107/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 200.4467 - reconstruction_loss: 97.0032 - KL_loss: 20.8474 - val_loss: 111.8009 - val_reconstruction_loss: 91.5393 - val_KL_loss: 20.2615 - lr: 0.0030\n",
      "Epoch 108/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 200.7106 - reconstruction_loss: 97.5218 - KL_loss: 20.7448 - val_loss: 111.2880 - val_reconstruction_loss: 91.2730 - val_KL_loss: 20.0150 - lr: 0.0030\n",
      "Epoch 109/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 199.9341 - reconstruction_loss: 97.2119 - KL_loss: 20.5011 - val_loss: 111.3085 - val_reconstruction_loss: 91.0039 - val_KL_loss: 20.3046 - lr: 0.0030\n",
      "Epoch 110/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 199.9284 - reconstruction_loss: 96.9817 - KL_loss: 20.8117 - val_loss: 111.1992 - val_reconstruction_loss: 90.8108 - val_KL_loss: 20.3884 - lr: 0.0030\n",
      "Epoch 111/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 199.5883 - reconstruction_loss: 96.7506 - KL_loss: 20.8776 - val_loss: 110.6079 - val_reconstruction_loss: 90.2072 - val_KL_loss: 20.4007 - lr: 0.0030\n",
      "Epoch 112/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 198.6573 - reconstruction_loss: 96.1531 - KL_loss: 20.8911 - val_loss: 110.7412 - val_reconstruction_loss: 90.5579 - val_KL_loss: 20.1833 - lr: 0.0030\n",
      "Epoch 113/1000\n",
      "412416/412416 [==============================] - 13s 30us/sample - loss: 198.6817 - reconstruction_loss: 96.4806 - KL_loss: 20.6606 - val_loss: 110.5494 - val_reconstruction_loss: 90.4792 - val_KL_loss: 20.0702 - lr: 0.0030\n",
      "Epoch 114/1000\n",
      "412416/412416 [==============================] - 13s 32us/sample - loss: 198.4613 - reconstruction_loss: 96.3929 - KL_loss: 20.5686 - val_loss: 110.5868 - val_reconstruction_loss: 90.7597 - val_KL_loss: 19.8270 - lr: 0.0030\n",
      "Epoch 115/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 198.1179 - reconstruction_loss: 96.6898 - KL_loss: 20.3079 - val_loss: 109.6208 - val_reconstruction_loss: 89.7709 - val_KL_loss: 19.8499 - lr: 0.0030\n",
      "Epoch 116/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 196.9215 - reconstruction_loss: 95.6887 - KL_loss: 20.3351 - val_loss: 111.0270 - val_reconstruction_loss: 91.1298 - val_KL_loss: 19.8972 - lr: 0.0030\n",
      "Epoch 117/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 198.5602 - reconstruction_loss: 97.0495 - KL_loss: 20.3665 - val_loss: 109.8604 - val_reconstruction_loss: 89.7020 - val_KL_loss: 20.1584 - lr: 0.0030\n",
      "Epoch 118/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 196.9780 - reconstruction_loss: 95.5805 - KL_loss: 20.6420 - val_loss: 110.9828 - val_reconstruction_loss: 90.6138 - val_KL_loss: 20.3690 - lr: 0.0030\n",
      "Epoch 119/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 198.0169 - reconstruction_loss: 96.4731 - KL_loss: 20.8618 - val_loss: 111.0073 - val_reconstruction_loss: 90.6882 - val_KL_loss: 20.3190 - lr: 0.0030\n",
      "Epoch 120/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 197.9861 - reconstruction_loss: 96.5230 - KL_loss: 20.8007 - val_loss: 110.0534 - val_reconstruction_loss: 90.1725 - val_KL_loss: 19.8809 - lr: 0.0030\n",
      "Epoch 121/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 196.5969 - reconstruction_loss: 96.0075 - KL_loss: 20.3659 - val_loss: 110.5875 - val_reconstruction_loss: 90.7654 - val_KL_loss: 19.8221 - lr: 0.0030\n",
      "Epoch 122/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 197.3280 - reconstruction_loss: 96.6094 - KL_loss: 20.3231 - val_loss: 110.2452 - val_reconstruction_loss: 90.5080 - val_KL_loss: 19.7373 - lr: 0.0030\n",
      "Epoch 123/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 196.6828 - reconstruction_loss: 96.3494 - KL_loss: 20.2247 - val_loss: 109.5649 - val_reconstruction_loss: 89.7842 - val_KL_loss: 19.7807 - lr: 0.0030\n",
      "Epoch 124/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 195.8455 - reconstruction_loss: 95.6654 - KL_loss: 20.2596 - val_loss: 110.3277 - val_reconstruction_loss: 90.2611 - val_KL_loss: 20.0666 - lr: 0.0030\n",
      "Epoch 125/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 196.3658 - reconstruction_loss: 96.1306 - KL_loss: 20.5549 - val_loss: 109.4514 - val_reconstruction_loss: 89.4045 - val_KL_loss: 20.0469 - lr: 0.0030\n",
      "Epoch 126/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 195.1559 - reconstruction_loss: 95.2391 - KL_loss: 20.5203 - val_loss: 110.0088 - val_reconstruction_loss: 90.2898 - val_KL_loss: 19.7190 - lr: 0.0030\n",
      "Epoch 127/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 195.6788 - reconstruction_loss: 96.1280 - KL_loss: 20.2005 - val_loss: 109.8749 - val_reconstruction_loss: 90.1382 - val_KL_loss: 19.7367 - lr: 0.0030\n",
      "Epoch 128/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 195.6102 - reconstruction_loss: 95.9700 - KL_loss: 20.2433 - val_loss: 109.2454 - val_reconstruction_loss: 89.6052 - val_KL_loss: 19.6402 - lr: 0.0030\n",
      "Epoch 129/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 194.6521 - reconstruction_loss: 95.4420 - KL_loss: 20.1318 - val_loss: 109.0246 - val_reconstruction_loss: 89.4675 - val_KL_loss: 19.5571 - lr: 0.0030\n",
      "Epoch 130/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 194.0635 - reconstruction_loss: 95.2600 - KL_loss: 20.0340 - val_loss: 109.3468 - val_reconstruction_loss: 89.6006 - val_KL_loss: 19.7462 - lr: 0.0030\n",
      "Epoch 131/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 194.3089 - reconstruction_loss: 95.4007 - KL_loss: 20.2260 - val_loss: 109.0685 - val_reconstruction_loss: 89.2532 - val_KL_loss: 19.8153 - lr: 0.0030\n",
      "Epoch 132/1000\n",
      "412416/412416 [==============================] - 13s 31us/sample - loss: 193.8153 - reconstruction_loss: 95.0590 - KL_loss: 20.2853 - val_loss: 108.6745 - val_reconstruction_loss: 88.8247 - val_KL_loss: 19.8498 - lr: 0.0030\n",
      "Epoch 133/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 193.1292 - reconstruction_loss: 94.6194 - KL_loss: 20.3470 - val_loss: 108.3600 - val_reconstruction_loss: 88.7204 - val_KL_loss: 19.6396 - lr: 0.0030\n",
      "Epoch 134/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 192.7016 - reconstruction_loss: 94.5016 - KL_loss: 20.1384 - val_loss: 108.3121 - val_reconstruction_loss: 88.7385 - val_KL_loss: 19.5735 - lr: 0.0030\n",
      "Epoch 135/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 192.4679 - reconstruction_loss: 94.5392 - KL_loss: 20.0650 - val_loss: 108.5718 - val_reconstruction_loss: 89.1262 - val_KL_loss: 19.4456 - lr: 0.0030\n",
      "Epoch 136/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 192.7070 - reconstruction_loss: 94.9256 - KL_loss: 19.9122 - val_loss: 108.3043 - val_reconstruction_loss: 88.6328 - val_KL_loss: 19.6715 - lr: 0.0030\n",
      "Epoch 137/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 192.1339 - reconstruction_loss: 94.3768 - KL_loss: 20.1529 - val_loss: 108.3446 - val_reconstruction_loss: 88.5735 - val_KL_loss: 19.7711 - lr: 0.0030\n",
      "Epoch 138/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 191.8310 - reconstruction_loss: 94.2849 - KL_loss: 20.2557 - val_loss: 107.8448 - val_reconstruction_loss: 88.1582 - val_KL_loss: 19.6867 - lr: 0.0030\n",
      "Epoch 139/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 191.2595 - reconstruction_loss: 93.8856 - KL_loss: 20.1853 - val_loss: 108.4911 - val_reconstruction_loss: 89.0383 - val_KL_loss: 19.4528 - lr: 0.0030\n",
      "Epoch 140/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 191.7920 - reconstruction_loss: 94.7545 - KL_loss: 19.9300 - val_loss: 107.6261 - val_reconstruction_loss: 88.2234 - val_KL_loss: 19.4027 - lr: 0.0030\n",
      "Epoch 141/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 190.5536 - reconstruction_loss: 93.9352 - KL_loss: 19.8819 - val_loss: 108.3422 - val_reconstruction_loss: 88.9083 - val_KL_loss: 19.4339 - lr: 0.0030\n",
      "Epoch 142/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 191.2095 - reconstruction_loss: 94.6150 - KL_loss: 19.9075 - val_loss: 107.7460 - val_reconstruction_loss: 88.1341 - val_KL_loss: 19.6119 - lr: 0.0030\n",
      "Epoch 143/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 190.4750 - reconstruction_loss: 93.8234 - KL_loss: 20.1012 - val_loss: 107.9834 - val_reconstruction_loss: 88.3376 - val_KL_loss: 19.6457 - lr: 0.0030\n",
      "Epoch 144/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 190.3320 - reconstruction_loss: 94.0312 - KL_loss: 20.1152 - val_loss: 107.7355 - val_reconstruction_loss: 88.0489 - val_KL_loss: 19.6866 - lr: 0.0030\n",
      "Epoch 145/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 190.0001 - reconstruction_loss: 93.7536 - KL_loss: 20.1598 - val_loss: 108.0945 - val_reconstruction_loss: 88.5743 - val_KL_loss: 19.5202 - lr: 0.0030\n",
      "Epoch 146/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 190.2455 - reconstruction_loss: 94.2519 - KL_loss: 19.9881 - val_loss: 107.4502 - val_reconstruction_loss: 88.0254 - val_KL_loss: 19.4249 - lr: 0.0030\n",
      "Epoch 147/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 189.2612 - reconstruction_loss: 93.6777 - KL_loss: 19.9068 - val_loss: 107.8033 - val_reconstruction_loss: 88.3849 - val_KL_loss: 19.4184 - lr: 0.0030\n",
      "Epoch 148/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 189.4875 - reconstruction_loss: 94.0684 - KL_loss: 19.8796 - val_loss: 107.4690 - val_reconstruction_loss: 87.8937 - val_KL_loss: 19.5753 - lr: 0.0030\n",
      "Epoch 149/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 188.9697 - reconstruction_loss: 93.5503 - KL_loss: 20.0404 - val_loss: 107.5408 - val_reconstruction_loss: 87.8808 - val_KL_loss: 19.6600 - lr: 0.0030\n",
      "Epoch 150/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 188.8109 - reconstruction_loss: 93.5248 - KL_loss: 20.1176 - val_loss: 107.2594 - val_reconstruction_loss: 87.6652 - val_KL_loss: 19.5943 - lr: 0.0030\n",
      "Epoch 151/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 188.2584 - reconstruction_loss: 93.2888 - KL_loss: 20.0693 - val_loss: 107.4765 - val_reconstruction_loss: 88.0567 - val_KL_loss: 19.4198 - lr: 0.0030\n",
      "Epoch 152/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 188.3663 - reconstruction_loss: 93.6712 - KL_loss: 19.8852 - val_loss: 107.1763 - val_reconstruction_loss: 87.7749 - val_KL_loss: 19.4014 - lr: 0.0030\n",
      "Epoch 153/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 187.8750 - reconstruction_loss: 93.4235 - KL_loss: 19.8679 - val_loss: 107.2455 - val_reconstruction_loss: 87.7793 - val_KL_loss: 19.4662 - lr: 0.0030\n",
      "Epoch 154/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 187.8167 - reconstruction_loss: 93.4201 - KL_loss: 19.9170 - val_loss: 107.1799 - val_reconstruction_loss: 87.6066 - val_KL_loss: 19.5734 - lr: 0.0030\n",
      "Epoch 155/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 187.4175 - reconstruction_loss: 93.2004 - KL_loss: 20.0361 - val_loss: 107.0939 - val_reconstruction_loss: 87.4585 - val_KL_loss: 19.6354 - lr: 0.0030\n",
      "Epoch 156/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 187.1959 - reconstruction_loss: 93.0722 - KL_loss: 20.0915 - val_loss: 107.0149 - val_reconstruction_loss: 87.4220 - val_KL_loss: 19.5929 - lr: 0.0030\n",
      "Epoch 157/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 186.9449 - reconstruction_loss: 93.0093 - KL_loss: 20.0636 - val_loss: 106.9108 - val_reconstruction_loss: 87.4518 - val_KL_loss: 19.4591 - lr: 0.0030\n",
      "Epoch 158/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 186.6961 - reconstruction_loss: 93.0494 - KL_loss: 19.9185 - val_loss: 107.0149 - val_reconstruction_loss: 87.6274 - val_KL_loss: 19.3874 - lr: 0.0030\n",
      "Epoch 159/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 186.5748 - reconstruction_loss: 93.2298 - KL_loss: 19.8571 - val_loss: 106.6299 - val_reconstruction_loss: 87.2115 - val_KL_loss: 19.4184 - lr: 0.0030\n",
      "Epoch 160/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 185.9241 - reconstruction_loss: 92.8159 - KL_loss: 19.8752 - val_loss: 107.0109 - val_reconstruction_loss: 87.5099 - val_KL_loss: 19.5010 - lr: 0.0030\n",
      "Epoch 161/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 186.1681 - reconstruction_loss: 93.0614 - KL_loss: 19.9700 - val_loss: 106.6598 - val_reconstruction_loss: 87.0173 - val_KL_loss: 19.6425 - lr: 0.0030\n",
      "Epoch 162/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 185.5488 - reconstruction_loss: 92.5456 - KL_loss: 20.1034 - val_loss: 106.8053 - val_reconstruction_loss: 87.1912 - val_KL_loss: 19.6141 - lr: 0.0030\n",
      "Epoch 163/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 185.5076 - reconstruction_loss: 92.7322 - KL_loss: 20.0870 - val_loss: 106.5634 - val_reconstruction_loss: 87.1393 - val_KL_loss: 19.4241 - lr: 0.0030\n",
      "Epoch 164/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 185.0011 - reconstruction_loss: 92.6744 - KL_loss: 19.8863 - val_loss: 106.8642 - val_reconstruction_loss: 87.5093 - val_KL_loss: 19.3550 - lr: 0.0030\n",
      "Epoch 165/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 185.2680 - reconstruction_loss: 93.0506 - KL_loss: 19.8265 - val_loss: 106.4419 - val_reconstruction_loss: 87.0042 - val_KL_loss: 19.4378 - lr: 0.0030\n",
      "Epoch 166/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 184.5210 - reconstruction_loss: 92.5395 - KL_loss: 19.8947 - val_loss: 106.8701 - val_reconstruction_loss: 87.2781 - val_KL_loss: 19.5919 - lr: 0.0030\n",
      "Epoch 167/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 184.8951 - reconstruction_loss: 92.8148 - KL_loss: 20.0676 - val_loss: 106.4348 - val_reconstruction_loss: 86.7329 - val_KL_loss: 19.7018 - lr: 0.0030\n",
      "Epoch 168/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 184.1175 - reconstruction_loss: 92.2549 - KL_loss: 20.1665 - val_loss: 106.7402 - val_reconstruction_loss: 87.0961 - val_KL_loss: 19.6441 - lr: 0.0030\n",
      "Epoch 169/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 184.3437 - reconstruction_loss: 92.6121 - KL_loss: 20.1202 - val_loss: 106.3383 - val_reconstruction_loss: 86.9300 - val_KL_loss: 19.4083 - lr: 0.0030\n",
      "Epoch 170/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 183.6187 - reconstruction_loss: 92.4396 - KL_loss: 19.8723 - val_loss: 106.6199 - val_reconstruction_loss: 87.1790 - val_KL_loss: 19.4409 - lr: 0.0030\n",
      "Epoch 171/1000\n",
      "412416/412416 [==============================] - 13s 32us/sample - loss: 183.7767 - reconstruction_loss: 92.6460 - KL_loss: 19.9134 - val_loss: 106.3104 - val_reconstruction_loss: 86.8609 - val_KL_loss: 19.4495 - lr: 0.0030\n",
      "Epoch 172/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 183.3011 - reconstruction_loss: 92.3647 - KL_loss: 19.9181 - val_loss: 106.5350 - val_reconstruction_loss: 86.8702 - val_KL_loss: 19.6648 - lr: 0.0030\n",
      "Epoch 173/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 183.2961 - reconstruction_loss: 92.3731 - KL_loss: 20.1381 - val_loss: 106.4238 - val_reconstruction_loss: 86.7984 - val_KL_loss: 19.6255 - lr: 0.0030\n",
      "Epoch 174/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 183.0601 - reconstruction_loss: 92.2854 - KL_loss: 20.1029 - val_loss: 106.3096 - val_reconstruction_loss: 86.7973 - val_KL_loss: 19.5123 - lr: 0.0030\n",
      "Epoch 175/1000\n",
      "412416/412416 [==============================] - 17s 42us/sample - loss: 182.6903 - reconstruction_loss: 92.2759 - KL_loss: 19.9788 - val_loss: 106.6363 - val_reconstruction_loss: 87.1109 - val_KL_loss: 19.5254 - lr: 0.0030\n",
      "Epoch 176/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 182.8765 - reconstruction_loss: 92.5687 - KL_loss: 20.0045 - val_loss: 105.9329 - val_reconstruction_loss: 86.4034 - val_KL_loss: 19.5295 - lr: 0.0030\n",
      "Epoch 177/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 181.9704 - reconstruction_loss: 91.8895 - KL_loss: 19.9999 - val_loss: 106.7411 - val_reconstruction_loss: 87.0287 - val_KL_loss: 19.7123 - lr: 0.0030\n",
      "Epoch 178/1000\n",
      "412416/412416 [==============================] - 14s 33us/sample - loss: 182.7245 - reconstruction_loss: 92.4707 - KL_loss: 20.1926 - val_loss: 105.9205 - val_reconstruction_loss: 86.2550 - val_KL_loss: 19.6656 - lr: 0.0030\n",
      "Epoch 179/1000\n",
      "412416/412416 [==============================] - 15s 36us/sample - loss: 181.5467 - reconstruction_loss: 91.6878 - KL_loss: 20.1406 - val_loss: 106.9811 - val_reconstruction_loss: 87.3658 - val_KL_loss: 19.6152 - lr: 0.0030\n",
      "Epoch 180/1000\n",
      "412416/412416 [==============================] - 13s 31us/sample - loss: 182.8853 - reconstruction_loss: 92.8351 - KL_loss: 20.0873 - val_loss: 106.4237 - val_reconstruction_loss: 86.8774 - val_KL_loss: 19.5463 - lr: 0.0030\n",
      "Epoch 181/1000\n",
      "412416/412416 [==============================] - 15s 35us/sample - loss: 182.2001 - reconstruction_loss: 92.3444 - KL_loss: 20.0177 - val_loss: 106.3334 - val_reconstruction_loss: 86.7073 - val_KL_loss: 19.6261 - lr: 0.0030\n",
      "Epoch 182/1000\n",
      "412416/412416 [==============================] - 17s 41us/sample - loss: 181.5717 - reconstruction_loss: 92.1269 - KL_loss: 20.1012 - val_loss: 106.7456 - val_reconstruction_loss: 86.9839 - val_KL_loss: 19.7617 - lr: 0.0030\n",
      "Epoch 183/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 181.8572 - reconstruction_loss: 92.4215 - KL_loss: 20.2270 - val_loss: 106.2627 - val_reconstruction_loss: 86.7238 - val_KL_loss: 19.5389 - lr: 0.0030\n",
      "Epoch 184/1000\n",
      "412416/412416 [==============================] - 14s 34us/sample - loss: 181.1507 - reconstruction_loss: 92.1623 - KL_loss: 20.0137 - val_loss: 106.1908 - val_reconstruction_loss: 86.8512 - val_KL_loss: 19.3395 - lr: 0.0030\n",
      "Epoch 185/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 181.0166 - reconstruction_loss: 92.2860 - KL_loss: 19.8053 - val_loss: 106.5652 - val_reconstruction_loss: 87.0355 - val_KL_loss: 19.5296 - lr: 0.0030\n",
      "Epoch 186/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 181.2060 - reconstruction_loss: 92.4534 - KL_loss: 20.0115 - val_loss: 105.8471 - val_reconstruction_loss: 86.2296 - val_KL_loss: 19.6175 - lr: 0.0030\n",
      "Epoch 187/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 180.2006 - reconstruction_loss: 91.6543 - KL_loss: 20.0884 - val_loss: 106.5034 - val_reconstruction_loss: 86.6252 - val_KL_loss: 19.8782 - lr: 0.0030\n",
      "Epoch 188/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 180.8394 - reconstruction_loss: 92.0751 - KL_loss: 20.3517 - val_loss: 106.2753 - val_reconstruction_loss: 86.5115 - val_KL_loss: 19.7638 - lr: 0.0030\n",
      "Epoch 189/1000\n",
      "412416/412416 [==============================] - 15s 35us/sample - loss: 180.4595 - reconstruction_loss: 91.9294 - KL_loss: 20.2292 - val_loss: 105.6957 - val_reconstruction_loss: 86.4196 - val_KL_loss: 19.2761 - lr: 0.0030\n",
      "Epoch 190/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 179.4895 - reconstruction_loss: 91.8414 - KL_loss: 19.7521 - val_loss: 106.7052 - val_reconstruction_loss: 87.2759 - val_KL_loss: 19.4293 - lr: 0.0030\n",
      "Epoch 191/1000\n",
      "412416/412416 [==============================] - 17s 41us/sample - loss: 180.7872 - reconstruction_loss: 92.6987 - KL_loss: 19.8981 - val_loss: 106.2207 - val_reconstruction_loss: 86.6775 - val_KL_loss: 19.5433 - lr: 0.0030\n",
      "Epoch 192/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 180.0047 - reconstruction_loss: 92.0978 - KL_loss: 20.0140 - val_loss: 105.8138 - val_reconstruction_loss: 86.2459 - val_KL_loss: 19.5679 - lr: 0.0030\n",
      "Epoch 193/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 179.2264 - reconstruction_loss: 91.6606 - KL_loss: 20.0306 - val_loss: 106.5564 - val_reconstruction_loss: 86.6714 - val_KL_loss: 19.8849 - lr: 0.0030\n",
      "Epoch 194/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 180.1457 - reconstruction_loss: 92.0641 - KL_loss: 20.3606 - val_loss: 106.0377 - val_reconstruction_loss: 86.3339 - val_KL_loss: 19.7038 - lr: 0.0030\n",
      "Epoch 195/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 179.3550 - reconstruction_loss: 91.7537 - KL_loss: 20.1794 - val_loss: 106.4772 - val_reconstruction_loss: 87.0886 - val_KL_loss: 19.3886 - lr: 0.0030\n",
      "Epoch 196/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 179.5408 - reconstruction_loss: 92.4728 - KL_loss: 19.8624 - val_loss: 105.9242 - val_reconstruction_loss: 86.4688 - val_KL_loss: 19.4554 - lr: 0.0030\n",
      "Epoch 197/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 178.9011 - reconstruction_loss: 91.8605 - KL_loss: 19.9255 - val_loss: 107.0170 - val_reconstruction_loss: 87.3509 - val_KL_loss: 19.6661 - lr: 0.0030\n",
      "Epoch 198/1000\n",
      "412416/412416 [==============================] - 15s 35us/sample - loss: 179.8107 - reconstruction_loss: 92.6866 - KL_loss: 20.1285 - val_loss: 106.0529 - val_reconstruction_loss: 86.1901 - val_KL_loss: 19.8628 - lr: 0.0030\n",
      "Epoch 199/1000\n",
      "412416/412416 [==============================] - 14s 33us/sample - loss: 178.7191 - reconstruction_loss: 91.5468 - KL_loss: 20.3343 - val_loss: 107.2884 - val_reconstruction_loss: 87.6307 - val_KL_loss: 19.6577 - lr: 0.0030\n",
      "Epoch 200/1000\n",
      "412416/412416 [==============================] - 16s 39us/sample - loss: 179.8526 - reconstruction_loss: 92.9802 - KL_loss: 20.1277 - val_loss: 106.4364 - val_reconstruction_loss: 86.9852 - val_KL_loss: 19.4513 - lr: 0.0027\n",
      "Epoch 201/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 178.8442 - reconstruction_loss: 92.3031 - KL_loss: 19.9242 - val_loss: 106.3996 - val_reconstruction_loss: 86.9586 - val_KL_loss: 19.4410 - lr: 0.0027\n",
      "Epoch 202/1000\n",
      "412416/412416 [==============================] - 15s 37us/sample - loss: 178.5834 - reconstruction_loss: 92.2873 - KL_loss: 19.9073 - val_loss: 106.4478 - val_reconstruction_loss: 86.9441 - val_KL_loss: 19.5037 - lr: 0.0027\n",
      "Epoch 203/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 178.5683 - reconstruction_loss: 92.3038 - KL_loss: 19.9658 - val_loss: 105.7742 - val_reconstruction_loss: 86.2262 - val_KL_loss: 19.5480 - lr: 0.0027\n",
      "Epoch 204/1000\n",
      "412416/412416 [==============================] - 16s 40us/sample - loss: 177.6995 - reconstruction_loss: 91.5753 - KL_loss: 20.0146 - val_loss: 106.1276 - val_reconstruction_loss: 86.3797 - val_KL_loss: 19.7479 - lr: 0.0027\n",
      "Epoch 205/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 178.0111 - reconstruction_loss: 91.7054 - KL_loss: 20.2065 - val_loss: 105.9148 - val_reconstruction_loss: 86.1431 - val_KL_loss: 19.7717 - lr: 0.0027\n",
      "Epoch 206/1000\n",
      "412416/412416 [==============================] - 14s 34us/sample - loss: 177.8676 - reconstruction_loss: 91.5178 - KL_loss: 20.2342 - val_loss: 105.6627 - val_reconstruction_loss: 86.0722 - val_KL_loss: 19.5906 - lr: 0.0027\n",
      "Epoch 207/1000\n",
      "412416/412416 [==============================] - 19s 46us/sample - loss: 177.3156 - reconstruction_loss: 91.4269 - KL_loss: 20.0443 - val_loss: 105.8608 - val_reconstruction_loss: 86.4644 - val_KL_loss: 19.3964 - lr: 0.0027\n",
      "Epoch 208/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 177.3729 - reconstruction_loss: 91.8082 - KL_loss: 19.8646 - val_loss: 105.7613 - val_reconstruction_loss: 86.3842 - val_KL_loss: 19.3771 - lr: 0.0027\n",
      "Epoch 209/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 177.1011 - reconstruction_loss: 91.6913 - KL_loss: 19.8395 - val_loss: 105.6574 - val_reconstruction_loss: 85.9590 - val_KL_loss: 19.6984 - lr: 0.0027\n",
      "Epoch 210/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 176.7595 - reconstruction_loss: 91.2753 - KL_loss: 20.1600 - val_loss: 105.7950 - val_reconstruction_loss: 86.0014 - val_KL_loss: 19.7937 - lr: 0.0027\n",
      "Epoch 211/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 176.8248 - reconstruction_loss: 91.3205 - KL_loss: 20.2377 - val_loss: 105.5114 - val_reconstruction_loss: 85.8807 - val_KL_loss: 19.6307 - lr: 0.0027\n",
      "Epoch 212/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 176.3408 - reconstruction_loss: 91.1544 - KL_loss: 20.0877 - val_loss: 105.6397 - val_reconstruction_loss: 86.2345 - val_KL_loss: 19.4052 - lr: 0.0027\n",
      "Epoch 213/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 176.2629 - reconstruction_loss: 91.5287 - KL_loss: 19.8672 - val_loss: 105.6432 - val_reconstruction_loss: 86.1846 - val_KL_loss: 19.4586 - lr: 0.0027\n",
      "Epoch 214/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 176.2832 - reconstruction_loss: 91.4714 - KL_loss: 19.9267 - val_loss: 105.3238 - val_reconstruction_loss: 85.6477 - val_KL_loss: 19.6761 - lr: 0.0027\n",
      "Epoch 215/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 175.7826 - reconstruction_loss: 90.9632 - KL_loss: 20.1290 - val_loss: 105.5611 - val_reconstruction_loss: 85.7238 - val_KL_loss: 19.8373 - lr: 0.0027\n",
      "Epoch 216/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 175.8721 - reconstruction_loss: 91.0288 - KL_loss: 20.2977 - val_loss: 105.6664 - val_reconstruction_loss: 85.9061 - val_KL_loss: 19.7603 - lr: 0.0027\n",
      "Epoch 217/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 175.8179 - reconstruction_loss: 91.1935 - KL_loss: 20.2138 - val_loss: 105.2577 - val_reconstruction_loss: 85.6991 - val_KL_loss: 19.5586 - lr: 0.0027\n",
      "Epoch 218/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 175.2924 - reconstruction_loss: 90.9885 - KL_loss: 20.0270 - val_loss: 105.5506 - val_reconstruction_loss: 86.0950 - val_KL_loss: 19.4556 - lr: 0.0027\n",
      "Epoch 219/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 175.5280 - reconstruction_loss: 91.3805 - KL_loss: 19.9133 - val_loss: 105.5084 - val_reconstruction_loss: 85.7563 - val_KL_loss: 19.7521 - lr: 0.0027\n",
      "Epoch 220/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 175.3661 - reconstruction_loss: 91.0284 - KL_loss: 20.2146 - val_loss: 105.2089 - val_reconstruction_loss: 85.3111 - val_KL_loss: 19.8978 - lr: 0.0027\n",
      "Epoch 221/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 174.8241 - reconstruction_loss: 90.6020 - KL_loss: 20.3556 - val_loss: 105.7705 - val_reconstruction_loss: 85.8816 - val_KL_loss: 19.8889 - lr: 0.0027\n",
      "Epoch 222/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 175.6271 - reconstruction_loss: 91.1893 - KL_loss: 20.3636 - val_loss: 105.1852 - val_reconstruction_loss: 85.3956 - val_KL_loss: 19.7896 - lr: 0.0027\n",
      "Epoch 223/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 174.8181 - reconstruction_loss: 90.6894 - KL_loss: 20.2553 - val_loss: 106.0530 - val_reconstruction_loss: 86.4154 - val_KL_loss: 19.6376 - lr: 0.0027\n",
      "Epoch 224/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 175.5328 - reconstruction_loss: 91.6842 - KL_loss: 20.0855 - val_loss: 106.2222 - val_reconstruction_loss: 86.3155 - val_KL_loss: 19.9066 - lr: 0.0027\n",
      "Epoch 225/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 175.9147 - reconstruction_loss: 91.5732 - KL_loss: 20.3642 - val_loss: 105.6814 - val_reconstruction_loss: 85.8755 - val_KL_loss: 19.8059 - lr: 0.0027\n",
      "Epoch 226/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 174.9723 - reconstruction_loss: 91.1488 - KL_loss: 20.2672 - val_loss: 106.1607 - val_reconstruction_loss: 86.2976 - val_KL_loss: 19.8631 - lr: 0.0027\n",
      "Epoch 227/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 175.2468 - reconstruction_loss: 91.5411 - KL_loss: 20.3267 - val_loss: 105.6939 - val_reconstruction_loss: 85.6384 - val_KL_loss: 20.0554 - lr: 0.0027\n",
      "Epoch 228/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 175.1904 - reconstruction_loss: 90.9018 - KL_loss: 20.5249 - val_loss: 106.1362 - val_reconstruction_loss: 86.2833 - val_KL_loss: 19.8529 - lr: 0.0027\n",
      "Epoch 229/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 175.1366 - reconstruction_loss: 91.5313 - KL_loss: 20.3070 - val_loss: 105.2395 - val_reconstruction_loss: 85.6939 - val_KL_loss: 19.5455 - lr: 0.0027\n",
      "Epoch 230/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 173.7481 - reconstruction_loss: 90.9642 - KL_loss: 19.9970 - val_loss: 105.7643 - val_reconstruction_loss: 86.2267 - val_KL_loss: 19.5375 - lr: 0.0027\n",
      "Epoch 231/1000\n",
      "412416/412416 [==============================] - 13s 31us/sample - loss: 174.6136 - reconstruction_loss: 91.4917 - KL_loss: 19.9877 - val_loss: 105.6351 - val_reconstruction_loss: 85.8666 - val_KL_loss: 19.7686 - lr: 0.0027\n",
      "Epoch 232/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 174.4410 - reconstruction_loss: 91.1167 - KL_loss: 20.2305 - val_loss: 105.1497 - val_reconstruction_loss: 85.2084 - val_KL_loss: 19.9413 - lr: 0.0027\n",
      "Epoch 233/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 173.4881 - reconstruction_loss: 90.4185 - KL_loss: 20.3969 - val_loss: 106.1777 - val_reconstruction_loss: 86.0688 - val_KL_loss: 20.1088 - lr: 0.0027\n",
      "Epoch 234/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 174.7223 - reconstruction_loss: 91.3056 - KL_loss: 20.5699 - val_loss: 105.6458 - val_reconstruction_loss: 85.6398 - val_KL_loss: 20.0061 - lr: 0.0027\n",
      "Epoch 235/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 174.4782 - reconstruction_loss: 90.8753 - KL_loss: 20.4617 - val_loss: 106.2282 - val_reconstruction_loss: 86.6246 - val_KL_loss: 19.6036 - lr: 0.0027\n",
      "Epoch 236/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 174.5673 - reconstruction_loss: 91.8461 - KL_loss: 20.0597 - val_loss: 105.4680 - val_reconstruction_loss: 85.7438 - val_KL_loss: 19.7242 - lr: 0.0027\n",
      "Epoch 237/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 173.5190 - reconstruction_loss: 90.9660 - KL_loss: 20.1898 - val_loss: 106.3077 - val_reconstruction_loss: 86.2161 - val_KL_loss: 20.0916 - lr: 0.0027\n",
      "Epoch 238/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 174.3846 - reconstruction_loss: 91.4352 - KL_loss: 20.5473 - val_loss: 105.9521 - val_reconstruction_loss: 85.7860 - val_KL_loss: 20.1661 - lr: 0.0027\n",
      "Epoch 239/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 174.0362 - reconstruction_loss: 90.9895 - KL_loss: 20.6207 - val_loss: 105.6896 - val_reconstruction_loss: 85.9265 - val_KL_loss: 19.7631 - lr: 0.0027\n",
      "Epoch 240/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 173.5163 - reconstruction_loss: 91.1501 - KL_loss: 20.2179 - val_loss: 105.6300 - val_reconstruction_loss: 86.1737 - val_KL_loss: 19.4563 - lr: 0.0027\n",
      "Epoch 241/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 173.2564 - reconstruction_loss: 91.4145 - KL_loss: 19.9140 - val_loss: 105.3047 - val_reconstruction_loss: 85.5734 - val_KL_loss: 19.7313 - lr: 0.0027\n",
      "Epoch 242/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 173.1272 - reconstruction_loss: 90.8029 - KL_loss: 20.1876 - val_loss: 105.8713 - val_reconstruction_loss: 85.6879 - val_KL_loss: 20.1834 - lr: 0.0027\n",
      "Epoch 243/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 173.4141 - reconstruction_loss: 90.9194 - KL_loss: 20.6240 - val_loss: 105.1422 - val_reconstruction_loss: 84.9913 - val_KL_loss: 20.1509 - lr: 0.0024\n",
      "Epoch 244/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 172.4299 - reconstruction_loss: 90.2476 - KL_loss: 20.5995 - val_loss: 105.6018 - val_reconstruction_loss: 85.8363 - val_KL_loss: 19.7655 - lr: 0.0024\n",
      "Epoch 245/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 172.7630 - reconstruction_loss: 91.0124 - KL_loss: 20.2269 - val_loss: 105.2993 - val_reconstruction_loss: 85.6597 - val_KL_loss: 19.6396 - lr: 0.0024\n",
      "Epoch 246/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 172.5685 - reconstruction_loss: 90.8769 - KL_loss: 20.0982 - val_loss: 105.1488 - val_reconstruction_loss: 85.4952 - val_KL_loss: 19.6536 - lr: 0.0024\n",
      "Epoch 247/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 172.0644 - reconstruction_loss: 90.7155 - KL_loss: 20.0998 - val_loss: 105.4271 - val_reconstruction_loss: 85.6450 - val_KL_loss: 19.7822 - lr: 0.0024\n",
      "Epoch 248/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 172.4105 - reconstruction_loss: 90.8653 - KL_loss: 20.2285 - val_loss: 105.2194 - val_reconstruction_loss: 85.3588 - val_KL_loss: 19.8606 - lr: 0.0024\n",
      "Epoch 249/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 172.0052 - reconstruction_loss: 90.5505 - KL_loss: 20.3118 - val_loss: 105.4557 - val_reconstruction_loss: 85.5162 - val_KL_loss: 19.9394 - lr: 0.0024\n",
      "Epoch 250/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 172.0857 - reconstruction_loss: 90.6762 - KL_loss: 20.3957 - val_loss: 105.2957 - val_reconstruction_loss: 85.3349 - val_KL_loss: 19.9608 - lr: 0.0024\n",
      "Epoch 251/1000\n",
      "412416/412416 [==============================] - 17s 41us/sample - loss: 171.8607 - reconstruction_loss: 90.5116 - KL_loss: 20.4172 - val_loss: 105.1864 - val_reconstruction_loss: 85.2203 - val_KL_loss: 19.9661 - lr: 0.0024\n",
      "Epoch 252/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 171.5521 - reconstruction_loss: 90.3908 - KL_loss: 20.4163 - val_loss: 105.3231 - val_reconstruction_loss: 85.3957 - val_KL_loss: 19.9274 - lr: 0.0024\n",
      "Epoch 253/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 171.8638 - reconstruction_loss: 90.5727 - KL_loss: 20.3832 - val_loss: 105.2845 - val_reconstruction_loss: 85.4683 - val_KL_loss: 19.8162 - lr: 0.0024\n",
      "Epoch 254/1000\n",
      "412416/412416 [==============================] - 15s 36us/sample - loss: 171.4963 - reconstruction_loss: 90.6340 - KL_loss: 20.2630 - val_loss: 105.0434 - val_reconstruction_loss: 85.3253 - val_KL_loss: 19.7181 - lr: 0.0024\n",
      "Epoch 255/1000\n",
      "412416/412416 [==============================] - 14s 33us/sample - loss: 171.2771 - reconstruction_loss: 90.5210 - KL_loss: 20.1700 - val_loss: 105.0924 - val_reconstruction_loss: 85.2425 - val_KL_loss: 19.8498 - lr: 0.0024\n",
      "Epoch 256/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 171.0599 - reconstruction_loss: 90.4161 - KL_loss: 20.2999 - val_loss: 105.0016 - val_reconstruction_loss: 84.9521 - val_KL_loss: 20.0495 - lr: 0.0024\n",
      "Epoch 257/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 171.0340 - reconstruction_loss: 90.1737 - KL_loss: 20.5003 - val_loss: 105.0563 - val_reconstruction_loss: 85.0207 - val_KL_loss: 20.0356 - lr: 0.0024\n",
      "Epoch 258/1000\n",
      "412416/412416 [==============================] - 13s 30us/sample - loss: 170.8167 - reconstruction_loss: 90.1853 - KL_loss: 20.4829 - val_loss: 105.3086 - val_reconstruction_loss: 85.4233 - val_KL_loss: 19.8854 - lr: 0.0024\n",
      "Epoch 259/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 170.9797 - reconstruction_loss: 90.5830 - KL_loss: 20.3400 - val_loss: 104.8114 - val_reconstruction_loss: 84.9654 - val_KL_loss: 19.8460 - lr: 0.0024\n",
      "Epoch 260/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 170.2775 - reconstruction_loss: 90.1251 - KL_loss: 20.2924 - val_loss: 105.6244 - val_reconstruction_loss: 85.7243 - val_KL_loss: 19.9001 - lr: 0.0024\n",
      "Epoch 261/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 171.1639 - reconstruction_loss: 90.8484 - KL_loss: 20.3555 - val_loss: 104.9276 - val_reconstruction_loss: 84.9386 - val_KL_loss: 19.9890 - lr: 0.0024\n",
      "Epoch 262/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 170.4174 - reconstruction_loss: 90.1194 - KL_loss: 20.4346 - val_loss: 105.9467 - val_reconstruction_loss: 85.9117 - val_KL_loss: 20.0351 - lr: 0.0024\n",
      "Epoch 263/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 171.3001 - reconstruction_loss: 91.0645 - KL_loss: 20.4788 - val_loss: 105.2232 - val_reconstruction_loss: 85.2824 - val_KL_loss: 19.9407 - lr: 0.0024\n",
      "Epoch 264/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 170.5056 - reconstruction_loss: 90.4370 - KL_loss: 20.3930 - val_loss: 106.3220 - val_reconstruction_loss: 86.3423 - val_KL_loss: 19.9797 - lr: 0.0024\n",
      "Epoch 265/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 171.7494 - reconstruction_loss: 91.4845 - KL_loss: 20.4318 - val_loss: 106.1046 - val_reconstruction_loss: 85.8932 - val_KL_loss: 20.2114 - lr: 0.0024\n",
      "Epoch 266/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 171.4212 - reconstruction_loss: 91.0060 - KL_loss: 20.6642 - val_loss: 105.4737 - val_reconstruction_loss: 85.2931 - val_KL_loss: 20.1806 - lr: 0.0024\n",
      "Epoch 267/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 170.4211 - reconstruction_loss: 90.4561 - KL_loss: 20.6277 - val_loss: 106.0379 - val_reconstruction_loss: 86.1049 - val_KL_loss: 19.9330 - lr: 0.0024\n",
      "Epoch 268/1000\n",
      "412416/412416 [==============================] - 16s 39us/sample - loss: 171.1225 - reconstruction_loss: 91.2398 - KL_loss: 20.3769 - val_loss: 105.2541 - val_reconstruction_loss: 85.4476 - val_KL_loss: 19.8065 - lr: 0.0024\n",
      "Epoch 269/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 170.1377 - reconstruction_loss: 90.5919 - KL_loss: 20.2572 - val_loss: 105.9792 - val_reconstruction_loss: 85.8698 - val_KL_loss: 20.1094 - lr: 0.0024\n",
      "Epoch 270/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 170.9006 - reconstruction_loss: 90.9898 - KL_loss: 20.5590 - val_loss: 105.7417 - val_reconstruction_loss: 85.5655 - val_KL_loss: 20.1761 - lr: 0.0022\n",
      "Epoch 271/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 170.5404 - reconstruction_loss: 90.6708 - KL_loss: 20.6228 - val_loss: 105.1781 - val_reconstruction_loss: 85.2294 - val_KL_loss: 19.9487 - lr: 0.0022\n",
      "Epoch 272/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 169.6852 - reconstruction_loss: 90.3298 - KL_loss: 20.3970 - val_loss: 105.7639 - val_reconstruction_loss: 85.7913 - val_KL_loss: 19.9726 - lr: 0.0022\n",
      "Epoch 273/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 170.4113 - reconstruction_loss: 90.9186 - KL_loss: 20.4225 - val_loss: 105.1569 - val_reconstruction_loss: 85.1654 - val_KL_loss: 19.9915 - lr: 0.0022\n",
      "Epoch 274/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 169.5527 - reconstruction_loss: 90.2886 - KL_loss: 20.4375 - val_loss: 105.6416 - val_reconstruction_loss: 85.6085 - val_KL_loss: 20.0331 - lr: 0.0022\n",
      "Epoch 275/1000\n",
      "412416/412416 [==============================] - 13s 30us/sample - loss: 170.0396 - reconstruction_loss: 90.7299 - KL_loss: 20.4829 - val_loss: 105.7195 - val_reconstruction_loss: 85.4654 - val_KL_loss: 20.2541 - lr: 0.0022\n",
      "Epoch 276/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 170.1965 - reconstruction_loss: 90.5667 - KL_loss: 20.7074 - val_loss: 105.0579 - val_reconstruction_loss: 84.9152 - val_KL_loss: 20.1426 - lr: 0.0022\n",
      "Epoch 277/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 169.2927 - reconstruction_loss: 90.0465 - KL_loss: 20.5871 - val_loss: 105.7092 - val_reconstruction_loss: 85.8231 - val_KL_loss: 19.8861 - lr: 0.0022\n",
      "Epoch 278/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 170.0055 - reconstruction_loss: 90.9733 - KL_loss: 20.3292 - val_loss: 105.4114 - val_reconstruction_loss: 85.5566 - val_KL_loss: 19.8549 - lr: 0.0022\n",
      "Epoch 279/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 169.7447 - reconstruction_loss: 90.6819 - KL_loss: 20.3004 - val_loss: 105.5424 - val_reconstruction_loss: 85.4721 - val_KL_loss: 20.0703 - lr: 0.0022\n",
      "Epoch 280/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 169.4914 - reconstruction_loss: 90.5959 - KL_loss: 20.5146 - val_loss: 105.3224 - val_reconstruction_loss: 85.0713 - val_KL_loss: 20.2512 - lr: 0.0022\n",
      "Epoch 281/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 169.3936 - reconstruction_loss: 90.1922 - KL_loss: 20.6964 - val_loss: 105.4905 - val_reconstruction_loss: 85.2820 - val_KL_loss: 20.2085 - lr: 0.0022\n",
      "Epoch 282/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 169.3706 - reconstruction_loss: 90.4041 - KL_loss: 20.6549 - val_loss: 105.1410 - val_reconstruction_loss: 84.9631 - val_KL_loss: 20.1779 - lr: 0.0022\n",
      "Epoch 283/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 168.8134 - reconstruction_loss: 90.0320 - KL_loss: 20.6224 - val_loss: 105.1285 - val_reconstruction_loss: 85.1767 - val_KL_loss: 19.9518 - lr: 0.0022\n",
      "Epoch 284/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 168.7731 - reconstruction_loss: 90.2845 - KL_loss: 20.3908 - val_loss: 104.9791 - val_reconstruction_loss: 85.1439 - val_KL_loss: 19.8352 - lr: 0.0020\n",
      "Epoch 285/1000\n",
      "412416/412416 [==============================] - 20s 49us/sample - loss: 168.6404 - reconstruction_loss: 90.2597 - KL_loss: 20.2785 - val_loss: 104.7068 - val_reconstruction_loss: 84.7438 - val_KL_loss: 19.9630 - lr: 0.0020\n",
      "Epoch 286/1000\n",
      "412416/412416 [==============================] - 15s 37us/sample - loss: 168.1567 - reconstruction_loss: 89.8379 - KL_loss: 20.4081 - val_loss: 105.5878 - val_reconstruction_loss: 85.1974 - val_KL_loss: 20.3905 - lr: 0.0020\n",
      "Epoch 287/1000\n",
      "412416/412416 [==============================] - 13s 30us/sample - loss: 169.2147 - reconstruction_loss: 90.3241 - KL_loss: 20.8250 - val_loss: 105.2636 - val_reconstruction_loss: 84.7962 - val_KL_loss: 20.4674 - lr: 0.0020\n",
      "Epoch 288/1000\n",
      "412416/412416 [==============================] - 14s 34us/sample - loss: 168.9248 - reconstruction_loss: 89.9013 - KL_loss: 20.9043 - val_loss: 105.5097 - val_reconstruction_loss: 85.4376 - val_KL_loss: 20.0721 - lr: 0.0020\n",
      "Epoch 289/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 168.8325 - reconstruction_loss: 90.5263 - KL_loss: 20.5177 - val_loss: 105.2270 - val_reconstruction_loss: 85.2967 - val_KL_loss: 19.9303 - lr: 0.0020\n",
      "Epoch 290/1000\n",
      "412416/412416 [==============================] - 13s 32us/sample - loss: 168.6002 - reconstruction_loss: 90.3680 - KL_loss: 20.3774 - val_loss: 105.7705 - val_reconstruction_loss: 85.6457 - val_KL_loss: 20.1248 - lr: 0.0020\n",
      "Epoch 291/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 168.9939 - reconstruction_loss: 90.7387 - KL_loss: 20.5643 - val_loss: 105.2710 - val_reconstruction_loss: 85.1117 - val_KL_loss: 20.1594 - lr: 0.0020\n",
      "Epoch 292/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 168.3709 - reconstruction_loss: 90.2215 - KL_loss: 20.5972 - val_loss: 105.5400 - val_reconstruction_loss: 85.4239 - val_KL_loss: 20.1162 - lr: 0.0020\n",
      "Epoch 293/1000\n",
      "412416/412416 [==============================] - 13s 31us/sample - loss: 168.7450 - reconstruction_loss: 90.4997 - KL_loss: 20.5599 - val_loss: 105.6612 - val_reconstruction_loss: 85.5345 - val_KL_loss: 20.1267 - lr: 0.0020\n",
      "Epoch 294/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 168.7346 - reconstruction_loss: 90.6013 - KL_loss: 20.5698 - val_loss: 105.0128 - val_reconstruction_loss: 84.7931 - val_KL_loss: 20.2197 - lr: 0.0020\n",
      "Epoch 295/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 168.1081 - reconstruction_loss: 89.8924 - KL_loss: 20.6567 - val_loss: 105.1944 - val_reconstruction_loss: 85.1203 - val_KL_loss: 20.0740 - lr: 0.0020\n",
      "Epoch 296/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 168.0705 - reconstruction_loss: 90.1954 - KL_loss: 20.5110 - val_loss: 104.8487 - val_reconstruction_loss: 84.9601 - val_KL_loss: 19.8885 - lr: 0.0020\n",
      "Epoch 297/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 167.7209 - reconstruction_loss: 90.0528 - KL_loss: 20.3328 - val_loss: 104.9524 - val_reconstruction_loss: 84.8639 - val_KL_loss: 20.0885 - lr: 0.0020\n",
      "Epoch 298/1000\n",
      "412416/412416 [==============================] - 17s 41us/sample - loss: 167.7140 - reconstruction_loss: 89.9476 - KL_loss: 20.5270 - val_loss: 104.7558 - val_reconstruction_loss: 84.4006 - val_KL_loss: 20.3552 - lr: 0.0018\n",
      "Epoch 299/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 167.3844 - reconstruction_loss: 89.4859 - KL_loss: 20.7888 - val_loss: 104.6209 - val_reconstruction_loss: 84.3070 - val_KL_loss: 20.3140 - lr: 0.0018\n",
      "Epoch 300/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 167.2580 - reconstruction_loss: 89.3892 - KL_loss: 20.7516 - val_loss: 105.0712 - val_reconstruction_loss: 85.0005 - val_KL_loss: 20.0707 - lr: 0.0018\n",
      "Epoch 301/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 167.5589 - reconstruction_loss: 90.0780 - KL_loss: 20.5108 - val_loss: 104.8333 - val_reconstruction_loss: 84.8897 - val_KL_loss: 19.9436 - lr: 0.0018\n",
      "Epoch 302/1000\n",
      "412416/412416 [==============================] - 17s 42us/sample - loss: 167.3252 - reconstruction_loss: 89.9657 - KL_loss: 20.3864 - val_loss: 104.9904 - val_reconstruction_loss: 84.9025 - val_KL_loss: 20.0879 - lr: 0.0018\n",
      "Epoch 303/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 167.3252 - reconstruction_loss: 89.9681 - KL_loss: 20.5196 - val_loss: 104.7754 - val_reconstruction_loss: 84.4949 - val_KL_loss: 20.2804 - lr: 0.0018\n",
      "Epoch 304/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 167.0755 - reconstruction_loss: 89.5556 - KL_loss: 20.7141 - val_loss: 104.9821 - val_reconstruction_loss: 84.7004 - val_KL_loss: 20.2816 - lr: 0.0018\n",
      "Epoch 305/1000\n",
      "412416/412416 [==============================] - 16s 40us/sample - loss: 167.2798 - reconstruction_loss: 89.7659 - KL_loss: 20.7244 - val_loss: 104.6985 - val_reconstruction_loss: 84.3907 - val_KL_loss: 20.3078 - lr: 0.0018\n",
      "Epoch 306/1000\n",
      "412416/412416 [==============================] - 17s 42us/sample - loss: 166.8514 - reconstruction_loss: 89.4750 - KL_loss: 20.7481 - val_loss: 105.2680 - val_reconstruction_loss: 85.0199 - val_KL_loss: 20.2481 - lr: 0.0018\n",
      "Epoch 307/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 167.4579 - reconstruction_loss: 90.0691 - KL_loss: 20.6769 - val_loss: 105.1420 - val_reconstruction_loss: 85.0648 - val_KL_loss: 20.0771 - lr: 0.0018\n",
      "Epoch 308/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 167.3610 - reconstruction_loss: 90.1512 - KL_loss: 20.5065 - val_loss: 104.7844 - val_reconstruction_loss: 84.7630 - val_KL_loss: 20.0213 - lr: 0.0018\n",
      "Epoch 309/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 166.8109 - reconstruction_loss: 89.8583 - KL_loss: 20.4605 - val_loss: 104.8951 - val_reconstruction_loss: 84.6160 - val_KL_loss: 20.2792 - lr: 0.0018\n",
      "Epoch 310/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 166.9504 - reconstruction_loss: 89.6799 - KL_loss: 20.7192 - val_loss: 104.7819 - val_reconstruction_loss: 84.3587 - val_KL_loss: 20.4233 - lr: 0.0018\n",
      "Epoch 311/1000\n",
      "412416/412416 [==============================] - 13s 32us/sample - loss: 166.6978 - reconstruction_loss: 89.4220 - KL_loss: 20.8533 - val_loss: 104.8215 - val_reconstruction_loss: 84.5351 - val_KL_loss: 20.2864 - lr: 0.0018\n",
      "Epoch 312/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 166.7132 - reconstruction_loss: 89.6155 - KL_loss: 20.7181 - val_loss: 104.6513 - val_reconstruction_loss: 84.5040 - val_KL_loss: 20.1473 - lr: 0.0016\n",
      "Epoch 313/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 166.5041 - reconstruction_loss: 89.5681 - KL_loss: 20.5781 - val_loss: 104.6957 - val_reconstruction_loss: 84.5212 - val_KL_loss: 20.1745 - lr: 0.0016\n",
      "Epoch 314/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 166.4063 - reconstruction_loss: 89.5798 - KL_loss: 20.6114 - val_loss: 104.6751 - val_reconstruction_loss: 84.4723 - val_KL_loss: 20.2028 - lr: 0.0016\n",
      "Epoch 315/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 166.3223 - reconstruction_loss: 89.5218 - KL_loss: 20.6381 - val_loss: 104.5867 - val_reconstruction_loss: 84.3055 - val_KL_loss: 20.2813 - lr: 0.0016\n",
      "Epoch 316/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 166.2071 - reconstruction_loss: 89.3809 - KL_loss: 20.7145 - val_loss: 104.6270 - val_reconstruction_loss: 84.2637 - val_KL_loss: 20.3633 - lr: 0.0016\n",
      "Epoch 317/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 166.1313 - reconstruction_loss: 89.3284 - KL_loss: 20.7929 - val_loss: 104.7014 - val_reconstruction_loss: 84.3653 - val_KL_loss: 20.3361 - lr: 0.0016\n",
      "Epoch 318/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 166.1434 - reconstruction_loss: 89.4159 - KL_loss: 20.7743 - val_loss: 104.6622 - val_reconstruction_loss: 84.4497 - val_KL_loss: 20.2125 - lr: 0.0016\n",
      "Epoch 319/1000\n",
      "412416/412416 [==============================] - 14s 33us/sample - loss: 165.9236 - reconstruction_loss: 89.4762 - KL_loss: 20.6484 - val_loss: 104.5089 - val_reconstruction_loss: 84.4428 - val_KL_loss: 20.0661 - lr: 0.0016\n",
      "Epoch 320/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 165.7353 - reconstruction_loss: 89.4983 - KL_loss: 20.5003 - val_loss: 104.6460 - val_reconstruction_loss: 84.4288 - val_KL_loss: 20.2172 - lr: 0.0016\n",
      "Epoch 321/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 165.8377 - reconstruction_loss: 89.5049 - KL_loss: 20.6496 - val_loss: 104.5907 - val_reconstruction_loss: 84.1991 - val_KL_loss: 20.3916 - lr: 0.0016\n",
      "Epoch 322/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 165.6847 - reconstruction_loss: 89.2496 - KL_loss: 20.8242 - val_loss: 104.6037 - val_reconstruction_loss: 84.2068 - val_KL_loss: 20.3969 - lr: 0.0016\n",
      "Epoch 323/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 165.6786 - reconstruction_loss: 89.2535 - KL_loss: 20.8377 - val_loss: 104.6511 - val_reconstruction_loss: 84.3190 - val_KL_loss: 20.3321 - lr: 0.0016\n",
      "Epoch 324/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 165.6635 - reconstruction_loss: 89.3652 - KL_loss: 20.7652 - val_loss: 104.5844 - val_reconstruction_loss: 84.3156 - val_KL_loss: 20.2688 - lr: 0.0016\n",
      "Epoch 325/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 165.4905 - reconstruction_loss: 89.3640 - KL_loss: 20.7019 - val_loss: 104.5345 - val_reconstruction_loss: 84.3350 - val_KL_loss: 20.1995 - lr: 0.0016\n",
      "Epoch 326/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 165.3338 - reconstruction_loss: 89.3603 - KL_loss: 20.6320 - val_loss: 104.4786 - val_reconstruction_loss: 84.2050 - val_KL_loss: 20.2736 - lr: 0.0016\n",
      "Epoch 327/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 165.2617 - reconstruction_loss: 89.2608 - KL_loss: 20.7112 - val_loss: 104.6460 - val_reconstruction_loss: 84.1979 - val_KL_loss: 20.4480 - lr: 0.0016\n",
      "Epoch 328/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 165.3371 - reconstruction_loss: 89.2376 - KL_loss: 20.8784 - val_loss: 104.4970 - val_reconstruction_loss: 84.0226 - val_KL_loss: 20.4745 - lr: 0.0016\n",
      "Epoch 329/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 165.0262 - reconstruction_loss: 89.0238 - KL_loss: 20.9086 - val_loss: 104.6114 - val_reconstruction_loss: 84.3047 - val_KL_loss: 20.3067 - lr: 0.0016\n",
      "Epoch 330/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 165.2451 - reconstruction_loss: 89.3437 - KL_loss: 20.7383 - val_loss: 104.5156 - val_reconstruction_loss: 84.1915 - val_KL_loss: 20.3242 - lr: 0.0016\n",
      "Epoch 331/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 164.9657 - reconstruction_loss: 89.2132 - KL_loss: 20.7597 - val_loss: 104.6663 - val_reconstruction_loss: 84.2258 - val_KL_loss: 20.4405 - lr: 0.0016\n",
      "Epoch 332/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 165.1494 - reconstruction_loss: 89.2725 - KL_loss: 20.8684 - val_loss: 104.4931 - val_reconstruction_loss: 83.9934 - val_KL_loss: 20.4997 - lr: 0.0016\n",
      "Epoch 333/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 164.8985 - reconstruction_loss: 89.0169 - KL_loss: 20.9308 - val_loss: 104.6382 - val_reconstruction_loss: 84.2833 - val_KL_loss: 20.3548 - lr: 0.0016\n",
      "Epoch 334/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 164.9030 - reconstruction_loss: 89.2768 - KL_loss: 20.7848 - val_loss: 104.5505 - val_reconstruction_loss: 84.2516 - val_KL_loss: 20.2989 - lr: 0.0016\n",
      "Epoch 335/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 164.8292 - reconstruction_loss: 89.2620 - KL_loss: 20.7340 - val_loss: 104.5644 - val_reconstruction_loss: 84.0995 - val_KL_loss: 20.4649 - lr: 0.0016\n",
      "Epoch 336/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 164.7105 - reconstruction_loss: 89.1175 - KL_loss: 20.8935 - val_loss: 104.6071 - val_reconstruction_loss: 84.0068 - val_KL_loss: 20.6004 - lr: 0.0016\n",
      "Epoch 337/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 164.6796 - reconstruction_loss: 89.0049 - KL_loss: 21.0328 - val_loss: 104.4423 - val_reconstruction_loss: 83.8948 - val_KL_loss: 20.5475 - lr: 0.0014\n",
      "Epoch 338/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 164.4791 - reconstruction_loss: 88.9120 - KL_loss: 20.9749 - val_loss: 104.5816 - val_reconstruction_loss: 84.2019 - val_KL_loss: 20.3797 - lr: 0.0014\n",
      "Epoch 339/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 164.6580 - reconstruction_loss: 89.2373 - KL_loss: 20.8094 - val_loss: 104.4015 - val_reconstruction_loss: 84.1299 - val_KL_loss: 20.2717 - lr: 0.0014\n",
      "Epoch 340/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 164.3792 - reconstruction_loss: 89.1457 - KL_loss: 20.7029 - val_loss: 104.6508 - val_reconstruction_loss: 84.1331 - val_KL_loss: 20.5177 - lr: 0.0014\n",
      "Epoch 341/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 164.5550 - reconstruction_loss: 89.1505 - KL_loss: 20.9494 - val_loss: 104.5501 - val_reconstruction_loss: 83.8925 - val_KL_loss: 20.6576 - lr: 0.0014\n",
      "Epoch 342/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 164.3998 - reconstruction_loss: 88.9057 - KL_loss: 21.0831 - val_loss: 104.5909 - val_reconstruction_loss: 84.1083 - val_KL_loss: 20.4826 - lr: 0.0014\n",
      "Epoch 343/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 164.3171 - reconstruction_loss: 89.1215 - KL_loss: 20.9130 - val_loss: 104.5272 - val_reconstruction_loss: 84.1979 - val_KL_loss: 20.3293 - lr: 0.0014\n",
      "Epoch 344/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 164.3045 - reconstruction_loss: 89.1996 - KL_loss: 20.7626 - val_loss: 104.4818 - val_reconstruction_loss: 83.9959 - val_KL_loss: 20.4860 - lr: 0.0014\n",
      "Epoch 345/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 164.0970 - reconstruction_loss: 89.0131 - KL_loss: 20.9166 - val_loss: 104.5783 - val_reconstruction_loss: 83.8852 - val_KL_loss: 20.6931 - lr: 0.0014\n",
      "Epoch 346/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 164.1689 - reconstruction_loss: 88.8867 - KL_loss: 21.1175 - val_loss: 104.4171 - val_reconstruction_loss: 83.9075 - val_KL_loss: 20.5097 - lr: 0.0014\n",
      "Epoch 347/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 163.8439 - reconstruction_loss: 88.8914 - KL_loss: 20.9380 - val_loss: 104.8214 - val_reconstruction_loss: 84.5245 - val_KL_loss: 20.2970 - lr: 0.0014\n",
      "Epoch 348/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 164.4062 - reconstruction_loss: 89.5286 - KL_loss: 20.7295 - val_loss: 104.6312 - val_reconstruction_loss: 84.2753 - val_KL_loss: 20.3559 - lr: 0.0014\n",
      "Epoch 349/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 164.1218 - reconstruction_loss: 89.2731 - KL_loss: 20.7895 - val_loss: 104.8287 - val_reconstruction_loss: 84.1110 - val_KL_loss: 20.7177 - lr: 0.0014\n",
      "Epoch 350/1000\n",
      "412416/412416 [==============================] - 17s 42us/sample - loss: 164.2494 - reconstruction_loss: 89.1496 - KL_loss: 21.1422 - val_loss: 104.8349 - val_reconstruction_loss: 83.9416 - val_KL_loss: 20.8933 - lr: 0.0014\n",
      "Epoch 351/1000\n",
      "412416/412416 [==============================] - 15s 36us/sample - loss: 164.1814 - reconstruction_loss: 88.9359 - KL_loss: 21.3139 - val_loss: 104.6069 - val_reconstruction_loss: 83.8849 - val_KL_loss: 20.7220 - lr: 0.0013\n",
      "Epoch 352/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 163.9018 - reconstruction_loss: 88.9010 - KL_loss: 21.1479 - val_loss: 104.6507 - val_reconstruction_loss: 84.2477 - val_KL_loss: 20.4030 - lr: 0.0013\n",
      "Epoch 353/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 163.8790 - reconstruction_loss: 89.2458 - KL_loss: 20.8323 - val_loss: 104.5847 - val_reconstruction_loss: 84.2306 - val_KL_loss: 20.3541 - lr: 0.0013\n",
      "Epoch 354/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 163.7689 - reconstruction_loss: 89.2388 - KL_loss: 20.7797 - val_loss: 104.5238 - val_reconstruction_loss: 84.0056 - val_KL_loss: 20.5183 - lr: 0.0013\n",
      "Epoch 355/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 163.5194 - reconstruction_loss: 88.9970 - KL_loss: 20.9436 - val_loss: 104.5659 - val_reconstruction_loss: 83.9295 - val_KL_loss: 20.6364 - lr: 0.0013\n",
      "Epoch 356/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 163.5864 - reconstruction_loss: 88.9359 - KL_loss: 21.0628 - val_loss: 104.4050 - val_reconstruction_loss: 83.7291 - val_KL_loss: 20.6759 - lr: 0.0013\n",
      "Epoch 357/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 163.3915 - reconstruction_loss: 88.7490 - KL_loss: 21.1005 - val_loss: 104.8020 - val_reconstruction_loss: 84.2047 - val_KL_loss: 20.5973 - lr: 0.0013\n",
      "Epoch 358/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 163.6934 - reconstruction_loss: 89.1841 - KL_loss: 21.0226 - val_loss: 104.7108 - val_reconstruction_loss: 84.2123 - val_KL_loss: 20.4986 - lr: 0.0013\n",
      "Epoch 359/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 163.7226 - reconstruction_loss: 89.2077 - KL_loss: 20.9235 - val_loss: 104.5837 - val_reconstruction_loss: 84.0500 - val_KL_loss: 20.5337 - lr: 0.0013\n",
      "Epoch 360/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 163.3509 - reconstruction_loss: 89.0452 - KL_loss: 20.9593 - val_loss: 104.6853 - val_reconstruction_loss: 83.9111 - val_KL_loss: 20.7741 - lr: 0.0013\n",
      "Epoch 361/1000\n",
      "412416/412416 [==============================] - 14s 34us/sample - loss: 163.4940 - reconstruction_loss: 88.8865 - KL_loss: 21.2009 - val_loss: 104.4770 - val_reconstruction_loss: 83.6337 - val_KL_loss: 20.8433 - lr: 0.0013\n",
      "Epoch 362/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 163.2454 - reconstruction_loss: 88.6277 - KL_loss: 21.2640 - val_loss: 104.5794 - val_reconstruction_loss: 83.9952 - val_KL_loss: 20.5841 - lr: 0.0013\n",
      "Epoch 363/1000\n",
      "412416/412416 [==============================] - 15s 36us/sample - loss: 163.3137 - reconstruction_loss: 88.9816 - KL_loss: 21.0057 - val_loss: 104.4808 - val_reconstruction_loss: 84.0741 - val_KL_loss: 20.4067 - lr: 0.0013\n",
      "Epoch 364/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 163.1857 - reconstruction_loss: 89.0893 - KL_loss: 20.8352 - val_loss: 104.4985 - val_reconstruction_loss: 83.9969 - val_KL_loss: 20.5016 - lr: 0.0013\n",
      "Epoch 365/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 163.1053 - reconstruction_loss: 88.9832 - KL_loss: 20.9276 - val_loss: 104.4289 - val_reconstruction_loss: 83.6965 - val_KL_loss: 20.7325 - lr: 0.0012\n",
      "Epoch 366/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 163.0465 - reconstruction_loss: 88.6873 - KL_loss: 21.1558 - val_loss: 104.3490 - val_reconstruction_loss: 83.5458 - val_KL_loss: 20.8031 - lr: 0.0012\n",
      "Epoch 367/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 162.7948 - reconstruction_loss: 88.5420 - KL_loss: 21.2242 - val_loss: 104.5710 - val_reconstruction_loss: 83.9505 - val_KL_loss: 20.6205 - lr: 0.0012\n",
      "Epoch 368/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 163.1088 - reconstruction_loss: 88.9341 - KL_loss: 21.0483 - val_loss: 104.4939 - val_reconstruction_loss: 83.9661 - val_KL_loss: 20.5277 - lr: 0.0012\n",
      "Epoch 369/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 162.9999 - reconstruction_loss: 88.9510 - KL_loss: 20.9583 - val_loss: 104.4901 - val_reconstruction_loss: 83.8164 - val_KL_loss: 20.6737 - lr: 0.0012\n",
      "Epoch 370/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 162.8565 - reconstruction_loss: 88.8149 - KL_loss: 21.0975 - val_loss: 104.5373 - val_reconstruction_loss: 83.7330 - val_KL_loss: 20.8043 - lr: 0.0012\n",
      "Epoch 371/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 162.9265 - reconstruction_loss: 88.7038 - KL_loss: 21.2258 - val_loss: 104.4621 - val_reconstruction_loss: 83.7847 - val_KL_loss: 20.6774 - lr: 0.0012\n",
      "Epoch 372/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 162.7249 - reconstruction_loss: 88.7441 - KL_loss: 21.1021 - val_loss: 104.5089 - val_reconstruction_loss: 84.0080 - val_KL_loss: 20.5009 - lr: 0.0012\n",
      "Epoch 373/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 162.8809 - reconstruction_loss: 88.9945 - KL_loss: 20.9256 - val_loss: 104.4613 - val_reconstruction_loss: 83.9246 - val_KL_loss: 20.5366 - lr: 0.0012\n",
      "Epoch 374/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 162.6726 - reconstruction_loss: 88.8912 - KL_loss: 20.9623 - val_loss: 104.5155 - val_reconstruction_loss: 83.8015 - val_KL_loss: 20.7140 - lr: 0.0012\n",
      "Epoch 375/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 162.6404 - reconstruction_loss: 88.7512 - KL_loss: 21.1386 - val_loss: 104.4929 - val_reconstruction_loss: 83.6754 - val_KL_loss: 20.8174 - lr: 0.0012\n",
      "Epoch 376/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 162.6316 - reconstruction_loss: 88.6260 - KL_loss: 21.2401 - val_loss: 104.4037 - val_reconstruction_loss: 83.6636 - val_KL_loss: 20.7402 - lr: 0.0012\n",
      "Epoch 377/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 162.4561 - reconstruction_loss: 88.6165 - KL_loss: 21.1628 - val_loss: 104.5508 - val_reconstruction_loss: 83.9718 - val_KL_loss: 20.5790 - lr: 0.0012\n",
      "Epoch 378/1000\n",
      "412416/412416 [==============================] - 18s 45us/sample - loss: 162.6282 - reconstruction_loss: 88.9527 - KL_loss: 21.0028 - val_loss: 104.5163 - val_reconstruction_loss: 84.0221 - val_KL_loss: 20.4942 - lr: 0.0012\n",
      "Epoch 379/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 162.5565 - reconstruction_loss: 88.9683 - KL_loss: 20.9202 - val_loss: 104.4763 - val_reconstruction_loss: 83.8980 - val_KL_loss: 20.5783 - lr: 0.0010\n",
      "Epoch 380/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 162.3818 - reconstruction_loss: 88.8599 - KL_loss: 21.0039 - val_loss: 104.4542 - val_reconstruction_loss: 83.6597 - val_KL_loss: 20.7945 - lr: 0.0010\n",
      "Epoch 381/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 162.4172 - reconstruction_loss: 88.6270 - KL_loss: 21.2185 - val_loss: 104.5120 - val_reconstruction_loss: 83.5773 - val_KL_loss: 20.9347 - lr: 0.0010\n",
      "Epoch 382/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 162.4178 - reconstruction_loss: 88.5606 - KL_loss: 21.3595 - val_loss: 104.3420 - val_reconstruction_loss: 83.4681 - val_KL_loss: 20.8739 - lr: 0.0010\n",
      "Epoch 383/1000\n",
      "412416/412416 [==============================] - 22s 53us/sample - loss: 162.1276 - reconstruction_loss: 88.4439 - KL_loss: 21.2967 - val_loss: 104.6815 - val_reconstruction_loss: 84.0373 - val_KL_loss: 20.6442 - lr: 0.0010\n",
      "Epoch 384/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 162.5786 - reconstruction_loss: 89.0088 - KL_loss: 21.0663 - val_loss: 104.5657 - val_reconstruction_loss: 84.0422 - val_KL_loss: 20.5235 - lr: 0.0010\n",
      "Epoch 385/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 162.3389 - reconstruction_loss: 88.9933 - KL_loss: 20.9479 - val_loss: 104.5175 - val_reconstruction_loss: 83.9203 - val_KL_loss: 20.5972 - lr: 0.0010\n",
      "Epoch 386/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 162.3038 - reconstruction_loss: 88.8951 - KL_loss: 21.0234 - val_loss: 104.5478 - val_reconstruction_loss: 83.8115 - val_KL_loss: 20.7363 - lr: 0.0010\n",
      "Epoch 387/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 162.2640 - reconstruction_loss: 88.7617 - KL_loss: 21.1589 - val_loss: 104.4425 - val_reconstruction_loss: 83.6335 - val_KL_loss: 20.8091 - lr: 0.0010\n",
      "Epoch 388/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 162.0385 - reconstruction_loss: 88.6048 - KL_loss: 21.2280 - val_loss: 104.4112 - val_reconstruction_loss: 83.6225 - val_KL_loss: 20.7887 - lr: 0.0010\n",
      "Epoch 389/1000\n",
      "412416/412416 [==============================] - 13s 32us/sample - loss: 162.0632 - reconstruction_loss: 88.6243 - KL_loss: 21.2108 - val_loss: 104.4143 - val_reconstruction_loss: 83.6844 - val_KL_loss: 20.7299 - lr: 0.0010\n",
      "Epoch 390/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 161.9940 - reconstruction_loss: 88.6228 - KL_loss: 21.1537 - val_loss: 104.3664 - val_reconstruction_loss: 83.6486 - val_KL_loss: 20.7178 - lr: 0.0010\n",
      "Epoch 391/1000\n",
      "412416/412416 [==============================] - 12s 30us/sample - loss: 161.9233 - reconstruction_loss: 88.5955 - KL_loss: 21.1408 - val_loss: 104.5351 - val_reconstruction_loss: 83.7427 - val_KL_loss: 20.7924 - lr: 0.0010\n",
      "Epoch 392/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 162.0788 - reconstruction_loss: 88.6915 - KL_loss: 21.2146 - val_loss: 104.4349 - val_reconstruction_loss: 83.6080 - val_KL_loss: 20.8269 - lr: 0.0010\n",
      "Epoch 393/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 161.9022 - reconstruction_loss: 88.5700 - KL_loss: 21.2477 - val_loss: 104.4159 - val_reconstruction_loss: 83.6482 - val_KL_loss: 20.7677 - lr: 9.4143e-04\n",
      "Epoch 394/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 161.8344 - reconstruction_loss: 88.5921 - KL_loss: 21.1891 - val_loss: 104.4241 - val_reconstruction_loss: 83.7233 - val_KL_loss: 20.7008 - lr: 9.4143e-04\n",
      "Epoch 395/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 161.8389 - reconstruction_loss: 88.6724 - KL_loss: 21.1230 - val_loss: 104.3538 - val_reconstruction_loss: 83.6740 - val_KL_loss: 20.6798 - lr: 9.4143e-04\n",
      "Epoch 396/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 161.7184 - reconstruction_loss: 88.6240 - KL_loss: 21.0998 - val_loss: 104.3463 - val_reconstruction_loss: 83.5992 - val_KL_loss: 20.7471 - lr: 9.4143e-04\n",
      "Epoch 397/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 161.6667 - reconstruction_loss: 88.5574 - KL_loss: 21.1667 - val_loss: 104.3266 - val_reconstruction_loss: 83.4687 - val_KL_loss: 20.8579 - lr: 9.4143e-04\n",
      "Epoch 398/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 161.6059 - reconstruction_loss: 88.4235 - KL_loss: 21.2811 - val_loss: 104.2845 - val_reconstruction_loss: 83.4092 - val_KL_loss: 20.8752 - lr: 9.4143e-04\n",
      "Epoch 399/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 161.5440 - reconstruction_loss: 88.3875 - KL_loss: 21.2961 - val_loss: 104.3892 - val_reconstruction_loss: 83.6010 - val_KL_loss: 20.7882 - lr: 9.4143e-04\n",
      "Epoch 400/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 161.5523 - reconstruction_loss: 88.5340 - KL_loss: 21.2064 - val_loss: 104.2964 - val_reconstruction_loss: 83.6090 - val_KL_loss: 20.6874 - lr: 9.4143e-04\n",
      "Epoch 401/1000\n",
      "412416/412416 [==============================] - 13s 31us/sample - loss: 161.5029 - reconstruction_loss: 88.5774 - KL_loss: 21.1075 - val_loss: 104.4232 - val_reconstruction_loss: 83.7318 - val_KL_loss: 20.6915 - lr: 9.4143e-04\n",
      "Epoch 402/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 161.5694 - reconstruction_loss: 88.6759 - KL_loss: 21.1131 - val_loss: 104.3461 - val_reconstruction_loss: 83.5492 - val_KL_loss: 20.7969 - lr: 9.4143e-04\n",
      "Epoch 403/1000\n",
      "412416/412416 [==============================] - 17s 41us/sample - loss: 161.4556 - reconstruction_loss: 88.4950 - KL_loss: 21.2173 - val_loss: 104.4668 - val_reconstruction_loss: 83.5504 - val_KL_loss: 20.9164 - lr: 9.4143e-04\n",
      "Epoch 404/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 161.5673 - reconstruction_loss: 88.4707 - KL_loss: 21.3338 - val_loss: 104.4064 - val_reconstruction_loss: 83.4809 - val_KL_loss: 20.9255 - lr: 9.4143e-04\n",
      "Epoch 405/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 161.3999 - reconstruction_loss: 88.4373 - KL_loss: 21.3416 - val_loss: 104.3311 - val_reconstruction_loss: 83.5191 - val_KL_loss: 20.8121 - lr: 9.4143e-04\n",
      "Epoch 406/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 161.3125 - reconstruction_loss: 88.4635 - KL_loss: 21.2303 - val_loss: 104.3775 - val_reconstruction_loss: 83.6598 - val_KL_loss: 20.7176 - lr: 9.4143e-04\n",
      "Epoch 407/1000\n",
      "412416/412416 [==============================] - 15s 35us/sample - loss: 161.3750 - reconstruction_loss: 88.5795 - KL_loss: 21.1372 - val_loss: 104.2904 - val_reconstruction_loss: 83.5512 - val_KL_loss: 20.7392 - lr: 9.4143e-04\n",
      "Epoch 408/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 161.1789 - reconstruction_loss: 88.4781 - KL_loss: 21.1562 - val_loss: 104.2396 - val_reconstruction_loss: 83.4057 - val_KL_loss: 20.8340 - lr: 9.4143e-04\n",
      "Epoch 409/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 161.2057 - reconstruction_loss: 88.3659 - KL_loss: 21.2513 - val_loss: 104.3597 - val_reconstruction_loss: 83.4343 - val_KL_loss: 20.9254 - lr: 9.4143e-04\n",
      "Epoch 410/1000\n",
      "412416/412416 [==============================] - 13s 32us/sample - loss: 161.2447 - reconstruction_loss: 88.3603 - KL_loss: 21.3423 - val_loss: 104.2347 - val_reconstruction_loss: 83.2905 - val_KL_loss: 20.9442 - lr: 9.4143e-04\n",
      "Epoch 411/1000\n",
      "412416/412416 [==============================] - 14s 33us/sample - loss: 161.0799 - reconstruction_loss: 88.2369 - KL_loss: 21.3608 - val_loss: 104.5883 - val_reconstruction_loss: 83.7002 - val_KL_loss: 20.8881 - lr: 9.4143e-04\n",
      "Epoch 412/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 161.4409 - reconstruction_loss: 88.6510 - KL_loss: 21.3061 - val_loss: 104.5672 - val_reconstruction_loss: 83.7464 - val_KL_loss: 20.8208 - lr: 9.4143e-04\n",
      "Epoch 413/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 161.3977 - reconstruction_loss: 88.6871 - KL_loss: 21.2403 - val_loss: 104.2384 - val_reconstruction_loss: 83.4569 - val_KL_loss: 20.7814 - lr: 9.4143e-04\n",
      "Epoch 414/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 160.9604 - reconstruction_loss: 88.4091 - KL_loss: 21.2016 - val_loss: 104.3332 - val_reconstruction_loss: 83.4783 - val_KL_loss: 20.8549 - lr: 9.4143e-04\n",
      "Epoch 415/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 161.0868 - reconstruction_loss: 88.4066 - KL_loss: 21.2728 - val_loss: 104.2721 - val_reconstruction_loss: 83.3182 - val_KL_loss: 20.9538 - lr: 9.4143e-04\n",
      "Epoch 416/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 160.9423 - reconstruction_loss: 88.2658 - KL_loss: 21.3686 - val_loss: 104.2486 - val_reconstruction_loss: 83.3288 - val_KL_loss: 20.9197 - lr: 9.4143e-04\n",
      "Epoch 417/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 160.8250 - reconstruction_loss: 88.2178 - KL_loss: 21.3371 - val_loss: 104.3842 - val_reconstruction_loss: 83.5753 - val_KL_loss: 20.8088 - lr: 9.4143e-04\n",
      "Epoch 418/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 161.0540 - reconstruction_loss: 88.5205 - KL_loss: 21.2293 - val_loss: 104.3121 - val_reconstruction_loss: 83.5118 - val_KL_loss: 20.8003 - lr: 9.4143e-04\n",
      "Epoch 419/1000\n",
      "412416/412416 [==============================] - 17s 41us/sample - loss: 160.8242 - reconstruction_loss: 88.4215 - KL_loss: 21.2202 - val_loss: 104.3603 - val_reconstruction_loss: 83.4500 - val_KL_loss: 20.9104 - lr: 9.4143e-04\n",
      "Epoch 420/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 160.9490 - reconstruction_loss: 88.3976 - KL_loss: 21.3229 - val_loss: 104.3367 - val_reconstruction_loss: 83.3642 - val_KL_loss: 20.9725 - lr: 9.4143e-04\n",
      "Epoch 421/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 160.8298 - reconstruction_loss: 88.3103 - KL_loss: 21.3825 - val_loss: 104.2770 - val_reconstruction_loss: 83.3773 - val_KL_loss: 20.8997 - lr: 8.4729e-04\n",
      "Epoch 422/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 160.6517 - reconstruction_loss: 88.2860 - KL_loss: 21.3140 - val_loss: 104.2590 - val_reconstruction_loss: 83.4535 - val_KL_loss: 20.8055 - lr: 8.4729e-04\n",
      "Epoch 423/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 160.7368 - reconstruction_loss: 88.3827 - KL_loss: 21.2214 - val_loss: 104.2700 - val_reconstruction_loss: 83.4369 - val_KL_loss: 20.8331 - lr: 8.4729e-04\n",
      "Epoch 424/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 160.6289 - reconstruction_loss: 88.3579 - KL_loss: 21.2483 - val_loss: 104.2152 - val_reconstruction_loss: 83.2826 - val_KL_loss: 20.9326 - lr: 8.4729e-04\n",
      "Epoch 425/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 160.5482 - reconstruction_loss: 88.1932 - KL_loss: 21.3458 - val_loss: 104.3559 - val_reconstruction_loss: 83.3613 - val_KL_loss: 20.9946 - lr: 8.4729e-04\n",
      "Epoch 426/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 160.6709 - reconstruction_loss: 88.2899 - KL_loss: 21.4090 - val_loss: 104.3253 - val_reconstruction_loss: 83.3431 - val_KL_loss: 20.9822 - lr: 8.4729e-04\n",
      "Epoch 427/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 160.6251 - reconstruction_loss: 88.2466 - KL_loss: 21.3983 - val_loss: 104.2469 - val_reconstruction_loss: 83.3166 - val_KL_loss: 20.9303 - lr: 8.4729e-04\n",
      "Epoch 428/1000\n",
      "412416/412416 [==============================] - 14s 34us/sample - loss: 160.4911 - reconstruction_loss: 88.2396 - KL_loss: 21.3448 - val_loss: 104.2152 - val_reconstruction_loss: 83.3329 - val_KL_loss: 20.8823 - lr: 8.4729e-04\n",
      "Epoch 429/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 160.4736 - reconstruction_loss: 88.3007 - KL_loss: 21.2950 - val_loss: 104.2557 - val_reconstruction_loss: 83.3787 - val_KL_loss: 20.8770 - lr: 8.4729e-04\n",
      "Epoch 430/1000\n",
      "412416/412416 [==============================] - 13s 32us/sample - loss: 160.3896 - reconstruction_loss: 88.3000 - KL_loss: 21.2929 - val_loss: 104.2312 - val_reconstruction_loss: 83.3091 - val_KL_loss: 20.9222 - lr: 8.4729e-04\n",
      "Epoch 431/1000\n",
      "412416/412416 [==============================] - 15s 37us/sample - loss: 160.2855 - reconstruction_loss: 88.2224 - KL_loss: 21.3365 - val_loss: 104.1811 - val_reconstruction_loss: 83.1761 - val_KL_loss: 21.0049 - lr: 8.4729e-04\n",
      "Epoch 432/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 160.3890 - reconstruction_loss: 88.1304 - KL_loss: 21.4176 - val_loss: 104.1961 - val_reconstruction_loss: 83.1960 - val_KL_loss: 21.0001 - lr: 8.4729e-04\n",
      "Epoch 433/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 160.1893 - reconstruction_loss: 88.1211 - KL_loss: 21.4112 - val_loss: 104.2672 - val_reconstruction_loss: 83.3250 - val_KL_loss: 20.9422 - lr: 8.4729e-04\n",
      "Epoch 434/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 160.2657 - reconstruction_loss: 88.2292 - KL_loss: 21.3576 - val_loss: 104.1883 - val_reconstruction_loss: 83.2818 - val_KL_loss: 20.9065 - lr: 8.4729e-04\n",
      "Epoch 435/1000\n",
      "412416/412416 [==============================] - 17s 41us/sample - loss: 160.1839 - reconstruction_loss: 88.2277 - KL_loss: 21.3199 - val_loss: 104.2483 - val_reconstruction_loss: 83.2793 - val_KL_loss: 20.9690 - lr: 8.4729e-04\n",
      "Epoch 436/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 160.1541 - reconstruction_loss: 88.1883 - KL_loss: 21.3794 - val_loss: 104.1466 - val_reconstruction_loss: 83.1623 - val_KL_loss: 20.9842 - lr: 8.4729e-04\n",
      "Epoch 437/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 160.0864 - reconstruction_loss: 88.1077 - KL_loss: 21.3952 - val_loss: 104.3552 - val_reconstruction_loss: 83.4317 - val_KL_loss: 20.9235 - lr: 8.4729e-04\n",
      "Epoch 438/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 160.2741 - reconstruction_loss: 88.3482 - KL_loss: 21.3363 - val_loss: 104.3366 - val_reconstruction_loss: 83.4172 - val_KL_loss: 20.9194 - lr: 8.4729e-04\n",
      "Epoch 439/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 160.2601 - reconstruction_loss: 88.3136 - KL_loss: 21.3342 - val_loss: 104.2074 - val_reconstruction_loss: 83.2039 - val_KL_loss: 21.0035 - lr: 8.4729e-04\n",
      "Epoch 440/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 159.9920 - reconstruction_loss: 88.1189 - KL_loss: 21.4155 - val_loss: 104.3869 - val_reconstruction_loss: 83.2947 - val_KL_loss: 21.0922 - lr: 8.4729e-04\n",
      "Epoch 441/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 160.2471 - reconstruction_loss: 88.2042 - KL_loss: 21.4987 - val_loss: 104.1562 - val_reconstruction_loss: 83.0530 - val_KL_loss: 21.1031 - lr: 8.4729e-04\n",
      "Epoch 442/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 159.9772 - reconstruction_loss: 88.0024 - KL_loss: 21.5108 - val_loss: 104.5969 - val_reconstruction_loss: 83.5808 - val_KL_loss: 21.0161 - lr: 8.4729e-04\n",
      "Epoch 443/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 160.3978 - reconstruction_loss: 88.4907 - KL_loss: 21.4309 - val_loss: 104.6335 - val_reconstruction_loss: 83.6646 - val_KL_loss: 20.9689 - lr: 8.4729e-04\n",
      "Epoch 444/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 160.4695 - reconstruction_loss: 88.5984 - KL_loss: 21.3841 - val_loss: 104.3675 - val_reconstruction_loss: 83.3823 - val_KL_loss: 20.9851 - lr: 8.4729e-04\n",
      "Epoch 445/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 160.1409 - reconstruction_loss: 88.2765 - KL_loss: 21.3949 - val_loss: 104.3157 - val_reconstruction_loss: 83.2613 - val_KL_loss: 21.0545 - lr: 8.4729e-04\n",
      "Epoch 446/1000\n",
      "412416/412416 [==============================] - 13s 31us/sample - loss: 160.0492 - reconstruction_loss: 88.1723 - KL_loss: 21.4619 - val_loss: 104.4944 - val_reconstruction_loss: 83.3956 - val_KL_loss: 21.0989 - lr: 8.4729e-04\n",
      "Epoch 447/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 160.2089 - reconstruction_loss: 88.2982 - KL_loss: 21.5101 - val_loss: 104.3180 - val_reconstruction_loss: 83.2509 - val_KL_loss: 21.0671 - lr: 7.6256e-04\n",
      "Epoch 448/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 159.9042 - reconstruction_loss: 88.1533 - KL_loss: 21.4794 - val_loss: 104.2562 - val_reconstruction_loss: 83.2609 - val_KL_loss: 20.9953 - lr: 7.6256e-04\n",
      "Epoch 449/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 159.8108 - reconstruction_loss: 88.1734 - KL_loss: 21.4053 - val_loss: 104.2962 - val_reconstruction_loss: 83.3110 - val_KL_loss: 20.9852 - lr: 7.6256e-04\n",
      "Epoch 450/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 159.8600 - reconstruction_loss: 88.2281 - KL_loss: 21.3956 - val_loss: 104.2224 - val_reconstruction_loss: 83.2147 - val_KL_loss: 21.0076 - lr: 7.6256e-04\n",
      "Epoch 451/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 159.7769 - reconstruction_loss: 88.1345 - KL_loss: 21.4201 - val_loss: 104.1700 - val_reconstruction_loss: 83.1453 - val_KL_loss: 21.0247 - lr: 7.6256e-04\n",
      "Epoch 452/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 159.6628 - reconstruction_loss: 88.0827 - KL_loss: 21.4351 - val_loss: 104.2989 - val_reconstruction_loss: 83.2492 - val_KL_loss: 21.0497 - lr: 7.6256e-04\n",
      "Epoch 453/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 159.7769 - reconstruction_loss: 88.1690 - KL_loss: 21.4586 - val_loss: 104.2035 - val_reconstruction_loss: 83.1211 - val_KL_loss: 21.0825 - lr: 7.6256e-04\n",
      "Epoch 454/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 159.6252 - reconstruction_loss: 87.9978 - KL_loss: 21.4942 - val_loss: 104.4855 - val_reconstruction_loss: 83.3593 - val_KL_loss: 21.1261 - lr: 7.6256e-04\n",
      "Epoch 455/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 159.9022 - reconstruction_loss: 88.2684 - KL_loss: 21.5412 - val_loss: 104.4191 - val_reconstruction_loss: 83.3065 - val_KL_loss: 21.1126 - lr: 7.6256e-04\n",
      "Epoch 456/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 159.9087 - reconstruction_loss: 88.2246 - KL_loss: 21.5257 - val_loss: 104.2379 - val_reconstruction_loss: 83.1858 - val_KL_loss: 21.0521 - lr: 7.6256e-04\n",
      "Epoch 457/1000\n",
      "412416/412416 [==============================] - 11s 27us/sample - loss: 159.6318 - reconstruction_loss: 88.1017 - KL_loss: 21.4594 - val_loss: 104.2079 - val_reconstruction_loss: 83.2329 - val_KL_loss: 20.9750 - lr: 7.6256e-04\n",
      "Epoch 458/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 159.5727 - reconstruction_loss: 88.1824 - KL_loss: 21.3831 - val_loss: 104.1892 - val_reconstruction_loss: 83.2271 - val_KL_loss: 20.9620 - lr: 7.6256e-04\n",
      "Epoch 459/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 159.6023 - reconstruction_loss: 88.1621 - KL_loss: 21.3758 - val_loss: 104.1540 - val_reconstruction_loss: 83.1316 - val_KL_loss: 21.0224 - lr: 7.6256e-04\n",
      "Epoch 460/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 159.4462 - reconstruction_loss: 88.0593 - KL_loss: 21.4361 - val_loss: 104.3113 - val_reconstruction_loss: 83.1790 - val_KL_loss: 21.1323 - lr: 7.6256e-04\n",
      "Epoch 461/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 159.6604 - reconstruction_loss: 88.0825 - KL_loss: 21.5396 - val_loss: 104.2688 - val_reconstruction_loss: 83.0582 - val_KL_loss: 21.2106 - lr: 6.8630e-04\n",
      "Epoch 462/1000\n",
      "412416/412416 [==============================] - 12s 29us/sample - loss: 159.5517 - reconstruction_loss: 87.9754 - KL_loss: 21.6174 - val_loss: 104.2240 - val_reconstruction_loss: 83.0331 - val_KL_loss: 21.1910 - lr: 6.8630e-04\n",
      "Epoch 463/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 159.5755 - reconstruction_loss: 87.9663 - KL_loss: 21.6024 - val_loss: 104.2126 - val_reconstruction_loss: 83.1125 - val_KL_loss: 21.1001 - lr: 6.8630e-04\n",
      "Epoch 464/1000\n",
      "412416/412416 [==============================] - 12s 28us/sample - loss: 159.4384 - reconstruction_loss: 88.0095 - KL_loss: 21.5128 - val_loss: 104.2153 - val_reconstruction_loss: 83.2032 - val_KL_loss: 21.0121 - lr: 6.8630e-04\n",
      "Epoch 465/1000\n",
      "412416/412416 [==============================] - 11s 28us/sample - loss: 159.4616 - reconstruction_loss: 88.1112 - KL_loss: 21.4217 - val_loss: 104.1831 - val_reconstruction_loss: 83.1677 - val_KL_loss: 21.0155 - lr: 6.8630e-04\n",
      "Epoch 466/1000\n",
      "412416/412416 [==============================] - 15s 36us/sample - loss: 159.3557 - reconstruction_loss: 88.0647 - KL_loss: 21.4246 - val_loss: 104.2249 - val_reconstruction_loss: 83.1336 - val_KL_loss: 21.0914 - lr: 6.8630e-04\n",
      "\n",
      "training finished in 466 epochs (early stop), transform data to adjust the platform effect...\n",
      "\n",
      "WARNING: when transforming data, after reversed Min-Max Scaling, apply exp transformation then multiple the factor and round to integer\n",
      "\n",
      "re-run DE on CVAE transformed scRNA-seq data!\n",
      "filtering genes present in <10 cells: 5 genes removed\n",
      "\n",
      "Differential analysis across cell-types on scRNA-seq data...\n",
      "0%...8%...WARNING: only 11 genes passing filtering (<20) for Endo vs Smc\n",
      "17%...25%...33%...WARNING: only 14 genes passing filtering (<20) for PVALB vs SST\n",
      "WARNING: only 17 genes passing filtering (<20) for PVALB vs VIP\n",
      "42%...WARNING: only 11 genes passing filtering (<20) for SST vs VIP\n",
      "WARNING: only 19 genes passing filtering (<20) for SST vs eL5\n",
      "50%...58%...67%...WARNING: only 15 genes passing filtering (<20) for eL2/3 vs eL4\n",
      "WARNING: only 15 genes passing filtering (<20) for eL2/3 vs eL5\n",
      "75%...WARNING: only 17 genes passing filtering (<20) for eL4 vs eL2/3\n",
      "WARNING: only 8 genes passing filtering (<20) for eL4 vs eL5\n",
      "83%...92%...WARNING: only 13 genes passing filtering (<20) for eL6 vs eL5\n",
      "finally selected 568 cell-type marker genes\n",
      "\n",
      "\n",
      "save variables related to CVAE to files!\n",
      "\n",
      "platform effect adjustment by CVAE finished. Elapsed time: 138.47 minutes.\n",
      "\n",
      "\n",
      "use the marker genes derived from CVAE transformed scRNA-seq for downstream regression!\n",
      "\n",
      "gene filtering before modeling...\n",
      "22 genes with nUMIs<5 in all spatial spots and need to be excluded\n",
      "finally use 546 genes for modeling\n",
      "\n",
      "spot filtering before modeling...\n",
      "all spots passed filtering\n",
      "\n",
      "\n",
      "######### Start GLRM modeling... #########\n",
      "\n",
      "GLRM settings:\n",
      "use SciPy minimize method:  L-BFGS-B\n",
      "global optimization turned off, local minimum will be used in GLRM\n",
      "use hybrid version of GLRM\n",
      "Numba detected total 64 available CPU cores. Use 64 CPU cores\n",
      "use 2001 points to calculate the heavy-tail density\n",
      "use weight threshold for Adaptive Lasso:  0.001\n",
      "total 292 unique nUMIs, min: 0.0, max: 1180.0\n",
      "\n",
      "Build graph: \n",
      " Graph with 581 nodes and 1029 edges\n",
      "\n",
      "estimation of gene-specific platform effect gamma_g is skipped as already using CVAE to adjust platform effect\n",
      "\n",
      "\n",
      "Start GLRM fitting...\n",
      "\n",
      "first estimate MLE theta and corresponding e^alpha and sigma^2...\n",
      "\n",
      "GLRM model initialization...\n",
      "calculate MLE theta and sigma^2 iteratively...\n",
      "  iter | time_opt | time_sig | sigma2\n",
      "     0 |   19.625 |    3.255 |  0.419\n",
      "     1 |   14.418 |    2.879 |  0.267\n",
      "     2 |   12.720 |    2.868 |  0.225\n",
      "     3 |   10.890 |    2.429 |  0.213\n",
      "     4 |    9.291 |    2.453 |  0.209\n",
      "     5 |    7.392 |    2.039 |  0.208\n",
      "MLE theta and sigma^2 calculation finished. Elapsed time: 1.50 minutes.\n",
      "MLE theta estimation finished. Elapsed time: 1.50 minutes.\n",
      "\n",
      "calculate weights of Adaptive Lasso...\n",
      "\n",
      "Stage 1: variable selection using Adaptive Lasso starts with the MLE theta and e^alpha, using already estimated sigma^2 and gamma_g...\n",
      "specified hyper-parameter for Adaptive Lasso is: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "hyper-parameter for Adaptive Lasso: use cross-validation to find the optimal value from 8 candidates...\n",
      "\n",
      "Start cross-validation for hyper-parameter lambda_r...\n",
      "directly estimate theta by Adaptive Lasso loss function as NO Graph Laplacian constrain!\n",
      "0%...11%...22%...33%...44%...56%...67%...early stop\n",
      "find optimal lambda_r 0.720 with average negative log-likelihood 50324.4194 by 5 fold cross-validation. Elapsed time: 7.17 minutes.\n",
      "\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |     11.428 |     11.429 |      0.130 |      0.130 |       1.00 |       1.00 |    4.852 |    0.000 |    0.004 |   0.190953 |   0.095477\n",
      "     1 |     11.413 |      0.047 |      0.129 |      0.141 |       1.00 |       2.00 |    7.130 |    0.000 |    0.003 |   0.190704 |   0.095352\n",
      "     2 |      9.198 |      7.125 |      0.127 |      0.159 |       2.00 |       2.00 |    7.776 |    0.000 |    0.003 |   0.153489 |   0.076744\n",
      "     3 |      7.004 |      9.024 |      0.127 |      0.171 |       2.00 |       4.00 |    5.949 |    0.000 |    0.003 |   0.103802 |   0.051901\n",
      "     4 |      5.657 |     11.097 |      0.129 |      0.189 |       4.00 |       4.00 |    7.595 |    0.000 |    0.003 |   0.086634 |   0.043317\n",
      "     5 |      4.596 |     10.838 |      0.129 |      0.201 |       4.00 |       4.00 |    8.017 |    0.000 |    0.003 |   0.071094 |   0.035547\n",
      "     6 |      4.049 |      8.371 |      0.126 |      0.210 |       4.00 |       8.00 |    7.534 |    0.000 |    0.003 |   0.061935 |   0.030968\n",
      "     7 |      3.398 |     10.156 |      0.128 |      0.227 |       8.00 |       8.00 |    8.038 |    0.000 |    0.003 |   0.051316 |   0.025658\n",
      "     8 |      2.843 |     11.184 |      0.129 |      0.243 |       8.00 |       8.00 |    7.174 |    0.000 |    0.003 |   0.042247 |   0.021124\n",
      "     9 |      2.556 |      8.991 |      0.127 |      0.257 |       8.00 |      16.00 |    6.657 |    0.000 |    0.003 |   0.038246 |   0.019123\n",
      "    10 |      2.234 |     11.234 |      0.129 |      0.283 |      16.00 |      16.00 |    7.784 |    0.000 |    0.003 |   0.033401 |   0.016700\n",
      "    11 |      1.926 |     12.938 |      0.131 |      0.306 |      16.00 |      16.00 |    7.317 |    0.000 |    0.003 |   0.028456 |   0.014228\n",
      "    12 |      1.735 |     11.197 |      0.129 |      0.325 |      16.00 |      32.00 |    7.132 |    0.000 |    0.003 |   0.025702 |   0.012851\n",
      "    13 |      1.483 |     16.020 |      0.134 |      0.359 |      32.00 |      32.00 |    8.519 |    0.000 |    0.003 |   0.022078 |   0.011039\n",
      "    14 |      1.256 |     18.425 |      0.137 |      0.387 |      32.00 |      32.00 |    7.326 |    0.000 |    0.003 |   0.018470 |   0.009235\n",
      "    15 |      1.138 |     15.158 |      0.133 |      0.410 |      32.00 |      64.00 |    6.937 |    0.000 |    0.003 |   0.016709 |   0.008355\n",
      "    16 |      0.990 |     19.701 |      0.138 |      0.454 |      64.00 |      64.00 |    7.849 |    0.000 |    0.003 |   0.014536 |   0.007268\n",
      "    17 |      0.829 |     23.751 |      0.142 |      0.491 |      64.00 |      64.00 |    7.135 |    0.000 |    0.003 |   0.012050 |   0.006025\n",
      "    18 |      0.733 |     21.087 |      0.139 |      0.520 |      64.00 |     128.00 |    6.853 |    0.000 |    0.003 |   0.010578 |   0.005289\n",
      "    19 |      0.639 |     26.966 |      0.145 |      0.575 |     128.00 |     128.00 |    7.536 |    0.000 |    0.003 |   0.009186 |   0.004593\n",
      "    20 |      0.537 |     30.626 |      0.149 |      0.620 |     128.00 |     128.00 |    6.981 |    0.000 |    0.003 |   0.007647 |   0.003824\n",
      "    21 |      0.467 |     27.820 |      0.146 |      0.657 |     128.00 |     256.00 |    7.009 |    0.000 |    0.003 |   0.006663 |   0.003332\n",
      "    22 |      0.404 |     34.735 |      0.153 |      0.723 |     256.00 |     256.00 |    7.244 |    0.000 |    0.003 |   0.005705 |   0.002852\n",
      "    23 |      0.338 |     39.387 |      0.157 |      0.779 |     256.00 |     256.00 |    6.684 |    0.000 |    0.003 |   0.004687 |   0.002344\n",
      "    24 |      0.294 |     36.275 |      0.154 |      0.823 |     256.00 |     512.00 |    6.403 |    0.000 |    0.003 |   0.004041 |   0.002021\n",
      "    25 |      0.250 |     45.367 |      0.163 |      0.903 |     512.00 |     512.00 |    7.103 |    0.000 |    0.003 |   0.003429 |   0.001715\n",
      "    26 |      0.215 |     48.415 |      0.166 |      0.971 |     512.00 |     512.00 |    6.168 |    0.000 |    0.003 |   0.002865 |   0.001432\n",
      "    27 |      0.193 |     40.891 |      0.159 |      1.029 |     512.00 |    1024.00 |    5.761 |    0.000 |    0.003 |   0.002557 |   0.001278\n",
      "    28 |      0.164 |     55.094 |      0.173 |      1.136 |    1024.00 |    1024.00 |    8.374 |    0.000 |    0.003 |   0.002155 |   0.001078\n",
      "    29 |      0.135 |     64.890 |      0.183 |      1.225 |    1024.00 |    1024.00 |    5.764 |    0.000 |    0.003 |   0.001735 |   0.000868\n",
      "    30 |      0.120 |     55.562 |      0.174 |      1.295 |    1024.00 |    2048.00 |    6.602 |    0.000 |    0.003 |   0.001526 |   0.000763\n",
      "    31 |      0.105 |     63.562 |      0.182 |      1.428 |    2048.00 |    2048.00 |    5.661 |    0.000 |    0.003 |   0.001321 |   0.000661\n",
      "    32 |      0.093 |     69.027 |      0.187 |      1.552 |    2048.00 |    2048.00 |    5.225 |    0.000 |    0.003 |   0.001122 |   0.000561\n",
      "    33 |      0.084 |     60.567 |      0.179 |      1.662 |    2048.00 |    4096.00 |    5.016 |    0.000 |    0.003 |   0.001001 |   0.000501\n",
      "    34 |      0.071 |     92.839 |      0.211 |      1.861 |    4096.00 |    4096.00 |    5.294 |    0.000 |    0.003 |   0.000840 |   0.000420\n",
      "    35 |      0.059 |    106.416 |      0.225 |      2.024 |    4096.00 |    4096.00 |    4.797 |    0.000 |    0.003 |   0.000678 |   0.000339\n",
      "    36 |      0.052 |     91.187 |      0.209 |      2.153 |    4096.00 |    8192.00 |    4.598 |    0.000 |    0.003 |   0.000594 |   0.000297\n",
      "    37 |      0.044 |    118.070 |      0.236 |      2.378 |    8192.00 |    8192.00 |    4.875 |    0.000 |    0.003 |   0.000489 |   0.000245\n",
      "    38 |      0.036 |    136.029 |      0.254 |      2.558 |    8192.00 |    8192.00 |    4.387 |    0.000 |    0.003 |   0.000391 |   0.000195\n",
      "    39 |      0.032 |    113.918 |      0.232 |      2.704 |    8192.00 |   16384.00 |    4.206 |    0.000 |    0.003 |   0.000341 |   0.000170\n",
      "    40 |      0.026 |    151.010 |      0.269 |      2.963 |   16384.00 |   16384.00 |    4.299 |    0.000 |    0.003 |   0.000282 |   0.000141\n",
      "    41 |      0.023 |    150.699 |      0.269 |      3.190 |   16384.00 |   16384.00 |    4.022 |    0.000 |    0.003 |   0.000230 |   0.000115\n",
      "    42 |      0.020 |    120.391 |      0.238 |      3.390 |   16384.00 |   32768.00 |    4.250 |    0.000 |    0.003 |   0.000208 |   0.000104\n",
      "    43 |      0.017 |    158.665 |      0.277 |      3.768 |   32768.00 |   32768.00 |    4.070 |    0.000 |    0.003 |   0.000173 |   0.000087\n",
      "    44 |      0.015 |    187.966 |      0.306 |      4.095 |   32768.00 |   32768.00 |    3.790 |    0.000 |    0.003 |   0.000139 |   0.000070\n",
      "    45 |      0.013 |    158.919 |      0.277 |      4.369 |   32768.00 |   65536.00 |    3.775 |    0.000 |    0.003 |   0.000118 |   0.000059\n",
      "    46 |      0.011 |    206.561 |      0.325 |      4.843 |   65536.00 |   65536.00 |    3.733 |    0.000 |    0.003 |   0.000096 |   0.000048\n",
      "    47 |      0.009 |    233.669 |      0.352 |      5.227 |   65536.00 |   65536.00 |    3.694 |    0.000 |    0.003 |   0.000075 |   0.000037\n",
      "    48 |      0.007 |    210.475 |      0.329 |      5.529 |   65536.00 |          / |    3.457 |    0.000 |    0.003 |   0.000062 |   0.000031\n",
      "early stop!\n",
      "Terminated (optimal) in 49 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 4.99 minutes.\n",
      "\n",
      "Stage 1 variable selection finished. Elapsed time: 12.16 minutes.\n",
      "\n",
      "Stage 2: final theta estimation with Graph Laplacian Constrain using already estimated sigma^2 and gamma_g\n",
      "specified hyper-parameter for Graph Laplacian Constrain is: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "hyper-parameter for Graph Laplacian Constrain: use cross-validation to find the optimal value from 8 candidates...\n",
      "\n",
      "Start cross-validation for hyper-parameter lambda_g...\n",
      "still use ADMM even NO Graph Laplacian constrain (lambda_g=0)\n",
      "0%...11%...22%...33%...44%...56%...67%...78%...89%...100%\n",
      "find optimal lambda_g 13.895 with average negative log-likelihood 50655.3764 by 5 fold cross-validation. Elapsed time: 76.47 minutes.\n",
      "\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |     14.027 |     11.882 |      0.132 |      0.132 |       1.00 |       1.00 |   12.073 |    0.000 |    0.007 |   0.135521 |   0.139477\n",
      "     1 |     11.754 |      8.449 |      0.130 |      0.142 |       1.00 |       1.00 |    5.202 |    0.000 |    0.007 |   0.158147 |   0.123074\n",
      "     2 |     10.214 |      9.732 |      0.128 |      0.151 |       1.00 |       1.00 |    5.386 |    0.000 |    0.007 |   0.122125 |   0.109745\n",
      "     3 |      9.301 |     10.934 |      0.129 |      0.160 |       1.00 |       2.00 |    5.194 |    0.000 |    0.007 |   0.103238 |   0.100497\n",
      "     4 |      8.981 |     17.855 |      0.136 |      0.177 |       2.00 |       2.00 |    6.392 |    0.000 |    0.007 |   0.096875 |   0.097279\n",
      "     5 |      8.273 |     19.362 |      0.137 |      0.193 |       2.00 |       2.00 |    6.072 |    0.000 |    0.007 |   0.095269 |   0.089186\n",
      "     6 |      7.535 |     21.210 |      0.139 |      0.208 |       2.00 |       4.00 |    6.025 |    0.000 |    0.007 |   0.081994 |   0.081862\n",
      "     7 |      7.251 |     33.360 |      0.151 |      0.236 |       4.00 |       4.00 |    6.770 |    0.000 |    0.007 |   0.078063 |   0.077960\n",
      "     8 |      6.506 |     35.297 |      0.153 |      0.262 |       4.00 |       4.00 |    6.751 |    0.000 |    0.007 |   0.075935 |   0.069583\n",
      "     9 |      5.682 |     38.186 |      0.156 |      0.284 |       4.00 |       8.00 |    6.396 |    0.000 |    0.007 |   0.061630 |   0.061508\n",
      "    10 |      5.262 |     57.715 |      0.176 |      0.325 |       8.00 |       8.00 |    7.312 |    0.000 |    0.007 |   0.057112 |   0.056193\n",
      "    11 |      4.462 |     60.757 |      0.179 |      0.359 |       8.00 |       8.00 |    7.104 |    0.000 |    0.007 |   0.052679 |   0.047224\n",
      "    12 |      3.626 |     64.811 |      0.183 |      0.387 |       8.00 |      16.00 |    6.563 |    0.000 |    0.007 |   0.038687 |   0.038965\n",
      "    13 |      3.181 |     92.272 |      0.210 |      0.435 |      16.00 |      16.00 |    7.165 |    0.000 |    0.007 |   0.034486 |   0.033525\n",
      "    14 |      2.466 |     96.076 |      0.214 |      0.472 |      16.00 |      16.00 |    6.780 |    0.000 |    0.007 |   0.029674 |   0.025672\n",
      "    15 |      1.768 |    100.453 |      0.219 |      0.498 |      16.00 |      32.00 |    6.283 |    0.000 |    0.008 |   0.018263 |   0.018844\n",
      "    16 |      1.462 |    132.130 |      0.250 |      0.541 |      32.00 |      32.00 |    7.005 |    0.000 |    0.007 |   0.015251 |   0.015172\n",
      "    17 |      1.002 |    135.437 |      0.254 |      0.568 |      32.00 |      32.00 |    6.361 |    0.000 |    0.007 |   0.012619 |   0.010146\n",
      "    18 |      0.573 |    138.731 |      0.257 |      0.584 |      32.00 |      64.00 |    5.399 |    0.000 |    0.007 |   0.005503 |   0.006002\n",
      "    19 |      0.475 |    167.545 |      0.286 |      0.608 |      64.00 |      64.00 |    6.320 |    0.000 |    0.006 |   0.003934 |   0.004719\n",
      "    20 |      0.294 |    169.292 |      0.287 |      0.621 |      64.00 |      64.00 |    5.572 |    0.000 |    0.006 |   0.004020 |   0.002731\n",
      "    21 |      0.120 |    171.025 |      0.289 |      0.625 |      64.00 |     128.00 |    4.018 |    0.000 |    0.006 |   0.001180 |   0.001119\n",
      "    22 |      0.133 |    192.529 |      0.311 |      0.633 |     128.00 |     128.00 |    4.009 |    0.000 |    0.005 |   0.000455 |   0.001111\n",
      "    23 |      0.081 |    193.088 |      0.311 |      0.637 |     128.00 |     128.00 |    4.779 |    0.000 |    0.005 |   0.001181 |   0.000651\n",
      "    24 |      0.026 |    193.772 |      0.312 |      0.638 |     128.00 |     256.00 |    3.154 |    0.000 |    0.005 |   0.000353 |   0.000224\n",
      "    25 |      0.040 |    207.740 |      0.326 |      0.641 |     256.00 |     256.00 |    2.879 |    0.000 |    0.004 |   0.000120 |   0.000292\n",
      "    26 |      0.023 |    207.838 |      0.326 |      0.644 |     256.00 |     256.00 |    4.026 |    0.000 |    0.004 |   0.000341 |   0.000178\n",
      "    27 |      0.008 |    208.025 |      0.326 |      0.645 |     256.00 |     512.00 |    2.764 |    0.000 |    0.004 |   0.000103 |   0.000059\n",
      "    28 |      0.012 |    216.147 |      0.334 |      0.647 |     512.00 |     512.00 |    2.505 |    0.000 |    0.004 |   0.000030 |   0.000082\n",
      "    29 |      0.007 |    216.154 |      0.334 |      0.649 |     512.00 |     512.00 |    3.235 |    0.000 |    0.004 |   0.000097 |   0.000049\n",
      "    30 |      0.002 |    216.193 |      0.334 |      0.649 |     512.00 |          / |    2.625 |    0.000 |    0.004 |   0.000031 |   0.000016\n",
      "early stop!\n",
      "Terminated (optimal) in 31 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 2.88 minutes.\n",
      "\n",
      "\n",
      "stage 2 finished. Elapsed time: 79.35 minutes.\n",
      "\n",
      "GLRM fitting finished. Elapsed time: 93.02 minutes.\n",
      "\n",
      "\n",
      "Post-processing estimated cell-type proportion theta...\n",
      "hard thresholding small theta values with threshold 0\n",
      "\n",
      "\n",
      "cell type deconvolution finished. Estimate results saved in /home/exouser/Spatial/celltype_proportions.csv. Elapsed time: 3.86 hours.\n",
      "\n",
      "\n",
      "######### No imputation #########\n",
      "\n",
      "\n",
      "whole pipeline finished. Total elapsed time: 3.86 hours.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='runDeconvolution -q sim_seq_based_spatial_spot_nUMI.csv                           -r scRNA_data_full.csv                           -c ref_scRNA_cell_celltype.csv                           -a sim_spatial_spot_adjacency_matrix.csv                           --n_marker_per_cmp 20                           -n 64                           --cvae_init_lr 0.003                           --num_hidden_layer 1                           --use_batch_norm false                           --cvae_train_epoch 1000                           --diagnosis true\\n', returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = '''runDeconvolution -q sim_seq_based_spatial_spot_nUMI.csv \\\n",
    "                          -r scRNA_data_full.csv \\\n",
    "                          -c ref_scRNA_cell_celltype.csv \\\n",
    "                          -a sim_spatial_spot_adjacency_matrix.csv \\\n",
    "                          --n_marker_per_cmp 20 \\\n",
    "                          -n 64 \\\n",
    "                          --cvae_init_lr 0.003 \\\n",
    "                          --num_hidden_layer 1 \\\n",
    "                          --use_batch_norm false \\\n",
    "                          --cvae_train_epoch 1000 \\\n",
    "                          --diagnosis true\n",
    "'''\n",
    "\n",
    "subprocess.run(cmd, check=True, text=True, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
