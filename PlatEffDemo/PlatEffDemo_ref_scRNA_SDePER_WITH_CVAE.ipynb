{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f03238",
   "metadata": {},
   "source": [
    "# Run *SDePER* on simulated data: Platform effect demonstration + scRNA-seq data as reference + WITH CVAE\n",
    "\n",
    "In this Notebook we run SDePER on STARmap-based simulated data. To best demonstrate the platform effect, we treat all 2,002 STARmap single cells as fake spatial spots and performed cell type deconvolution.\n",
    "\n",
    "**Platform effect demonstration** means we actually use single cells for cell type deconvolution to best demonstrate the platform effect. The reference data for deconvolution includes all single cells with the **matched 12 cell types**.\n",
    "\n",
    "**scRNA-seq data as reference** means the reference data is scRNA-seq data ([GSE115746](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115746)) from the same tissue with simulated spatial data, therefore **platform effect exists**.\n",
    "\n",
    "**WITH CVAE** means we use CVAE to remove platform effect.\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "So here we use the **3 input files** as shown below:\n",
    "\n",
    "1. raw nUMI counts of all 2,002 STARmap single cells (cells × genes): [STARmap_cell_nUMI.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Run_SDePER_on_simulation_data/Scenario_1/ref_spatial/STARmap_cell_nUMI.csv)\n",
    "2. raw nUMI counts of reference scRNA-seq data (cells × genes): `scRNA_data_full.csv`. Since the file size of csv file of raw nUMI matrix of all 23,178 cells and 45,768 genes is up to 2.3 GB, we do not provide this file in our repository. It's just a **matrix transpose** of [GSE115746_cells_exon_counts.csv.gz](https://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE115746&format=file&file=GSE115746%5Fcells%5Fexon%5Fcounts%2Ecsv%2Egz) in [GSE115746](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE115746) to satisty the file format requirement that rows as cells and columns as genes.\n",
    "3. cell type annotations for cells of **the matched 12 cell types** in reference scRNA-seq data (cells × 1): [ref_scRNA_cell_celltype.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Run_SDePER_on_simulation_data/Scenario_1/ref_scRNA_seq/ref_scRNA_cell_celltype.csv)\n",
    "\n",
    "NOTE: No adjacency matrix was used as input!\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "SDePER settings are:\n",
    "\n",
    "* number of used CPU cores `n_core`: 64\n",
    "* not use pseudo-spots in CVAE training by set `n_pseudo_spot`: 0\n",
    "* number of included highly variable genes `n_hv_gene`: 1000\n",
    "* for diagnostic purposes set `diagnosis`: true\n",
    "\n",
    "ALL other options are left as default.\n",
    "\n",
    "==================================================================================================================\n",
    "\n",
    "the `bash` command to start cell type deconvolution is\n",
    "\n",
    "`runDeconvolution -q STARmap_cell_nUMI.csv -r scRNA_data_full.csv -c ref_scRNA_cell_celltype.csv -n 64 --n_pseudo_spot 0 --diagnosis true`\n",
    "\n",
    "Note this Notebook uses **SDePER v1.5.0**. Cell type deconvolution result is renamed as [PlatEffDemo_ref_scRNA_SDePER_WITH_CVAE_celltype_proportions.csv](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Run_SDePER_on_simulation_data/PlatEffDemo/PlatEffDemo_ref_scRNA_SDePER_WITH_CVAE_celltype_proportions.csv). Folder of diagnostic plots is compressed and renamed as [PlatEffDemo_ref_scRNA_SDePER_WITH_CVAE_diagnosis.tar](https://github.com/az7jh2/SDePER_Analysis/blob/main/Simulation/Run_SDePER_on_simulation_data/PlatEffDemo/PlatEffDemo_ref_scRNA_SDePER_WITH_CVAE_diagnosis.tar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f43766d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SDePER (Spatial Deconvolution method with Platform Effect Removal) v1.5.0\n",
      "\n",
      "\n",
      "running options:\n",
      "spatial_file: /home/exouser/Spatial/STARmap_cell_nUMI.csv\n",
      "ref_file: /home/exouser/Spatial/scRNA_data_full.csv\n",
      "ref_celltype_file: /home/exouser/Spatial/ref_scRNA_cell_celltype.csv\n",
      "marker_file: None\n",
      "loc_file: None\n",
      "A_file: None\n",
      "n_cores: 64\n",
      "threshold: 0\n",
      "use_cvae: True\n",
      "use_imputation: False\n",
      "diagnosis: True\n",
      "verbose: True\n",
      "use_fdr: True\n",
      "p_val_cutoff: 0.05\n",
      "fc_cutoff: 1.2\n",
      "pct1_cutoff: 0.3\n",
      "pct2_cutoff: 0.1\n",
      "sortby_fc: True\n",
      "n_marker_per_cmp: 20\n",
      "filter_cell: True\n",
      "filter_gene: True\n",
      "n_hv_gene: 1000\n",
      "n_pseudo_spot: 0\n",
      "pseudo_spot_min_cell: 2\n",
      "pseudo_spot_max_cell: 8\n",
      "seq_depth_scaler: 10000\n",
      "cvae_input_scaler: 10\n",
      "cvae_init_lr: 0.01\n",
      "num_hidden_layer: 1\n",
      "use_batch_norm: True\n",
      "cvae_train_epoch: 500\n",
      "use_spatial_pseudo: False\n",
      "redo_de: True\n",
      "seed: 383\n",
      "lambda_r: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "lambda_g: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "diameter: 200\n",
      "impute_diameter: [160, 114, 80]\n",
      "\n",
      "\n",
      "######### Preprocessing... #########\n",
      "\n",
      "first build CVAE...\n",
      "\n",
      "read spatial data from file /home/exouser/Spatial/STARmap_cell_nUMI.csv\n",
      "total 2002 spots; 1020 genes\n",
      "\n",
      "filtering genes present in <3 spots: No genes removed\n",
      "\n",
      "filtering 0 mitochondrial genes\n",
      "\n",
      "finally remain 2002 spots; 1020 genes for downstream analysis\n",
      "\n",
      "read scRNA-seq data from file /home/exouser/Spatial/scRNA_data_full.csv\n",
      "total 23178 cells; 45768 genes\n",
      "read scRNA-seq cell-type annotation from file /home/exouser/Spatial/ref_scRNA_cell_celltype.csv\n",
      "total 12 cell-types\n",
      "subset cells with cell-type annotation, finally keep 11835 cells; 45768 genes\n",
      "\n",
      "filtering cells with <200 genes: No cells removed\n",
      "\n",
      "filtering genes present in <10 cells: 12063 genes removed\n",
      "\n",
      "filtering 0 mitochondrial genes\n",
      "\n",
      "finally remain 11835 cells; 33705 genes for downstream analysis\n",
      "\n",
      "total 991 overlapped genes\n",
      "\n",
      "use all 991 genes for downstream analysis as there are less genes available than specified number 1000\n",
      "\n",
      "start CVAE building...\n",
      "\n",
      "generate 0 pseudo-spots containing 2 to 8 cells from scRNA-seq cells...\n",
      "\n",
      "#cells of cell types in reference scRNA-seq data:\n",
      "eL6: 2773\n",
      "VIP: 1690\n",
      "eL5: 1679\n",
      "SST: 1567\n",
      "eL4: 1350\n",
      "PVALB: 1211\n",
      "eL2/3: 968\n",
      "Astro: 361\n",
      "Endo: 76\n",
      "Oligo: 61\n",
      "Smc: 55\n",
      "Micro: 44\n",
      "HIGHLIGHT: augment single cells to 4159 cells per cell type\n",
      "0%...8%...17%...25%...33%...42%...50%...58%...67%...75%...83%...92%...100%\n",
      "generate 0 pseudo-spots containing 2 to 6 spots from spatial spots...\n",
      "\n",
      "HIGHLIGHT: first apply log transformation on sequencing depth normalized gene expressions, followed by Min-Max scaling\n",
      "\n",
      "                         |  training | validation\n",
      "spatial spots            |      2002 |         0\n",
      "spatial pseudo-spots     |         0 |         0\n",
      "scRNA-seq cells          |     39926 |      9982\n",
      "scRNA-seq pseudo-spots   |         0 |         0\n",
      "\n",
      "scaling inputs to range 0 to 10\n",
      "\n",
      "CVAE structure:\n",
      "Encoder: 992 - 188 - 36\n",
      "Decoder: 37 - 188 - 991\n",
      "\n",
      "\n",
      "Start training...\n",
      "\n",
      "Train on 41928 samples, validate on 9982 samples\n",
      "Epoch 1/500\n",
      "41928/41928 [==============================] - 3s 60us/sample - loss: 124.2312 - reconstruction_loss: 1023.2520 - KL_loss: 29.3491 - val_loss: 40897392.0000 - val_reconstruction_loss: 17140726.0000 - val_KL_loss: 23756666.0000 - lr: 0.0100\n",
      "Epoch 2/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 91.4901 - reconstruction_loss: 688.4142 - KL_loss: 27.4965 - val_loss: 1745536.7500 - val_reconstruction_loss: 926815.1250 - val_KL_loss: 818721.6250 - lr: 0.0100\n",
      "Epoch 3/500\n",
      "41928/41928 [==============================] - 2s 47us/sample - loss: 84.0174 - reconstruction_loss: 577.5422 - KL_loss: 26.1565 - val_loss: 436779.2812 - val_reconstruction_loss: 321239.5312 - val_KL_loss: 115539.7734 - lr: 0.0100\n",
      "Epoch 4/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 80.5200 - reconstruction_loss: 529.8885 - KL_loss: 25.5262 - val_loss: 203412.7969 - val_reconstruction_loss: 163363.9688 - val_KL_loss: 40048.8438 - lr: 0.0100\n",
      "Epoch 5/500\n",
      "41928/41928 [==============================] - 2s 48us/sample - loss: 78.4099 - reconstruction_loss: 501.2361 - KL_loss: 25.4882 - val_loss: 77592.5547 - val_reconstruction_loss: 68665.2031 - val_KL_loss: 8927.3584 - lr: 0.0100\n",
      "Epoch 6/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 76.9904 - reconstruction_loss: 480.0141 - KL_loss: 25.7313 - val_loss: 46641.0742 - val_reconstruction_loss: 43959.5938 - val_KL_loss: 2681.4780 - lr: 0.0100\n",
      "Epoch 7/500\n",
      "41928/41928 [==============================] - 2s 48us/sample - loss: 75.7755 - reconstruction_loss: 462.4549 - KL_loss: 26.1174 - val_loss: 36659.8945 - val_reconstruction_loss: 35363.4180 - val_KL_loss: 1296.4727 - lr: 0.0100\n",
      "Epoch 8/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 74.7740 - reconstruction_loss: 447.3827 - KL_loss: 26.5857 - val_loss: 30439.5977 - val_reconstruction_loss: 29589.0742 - val_KL_loss: 850.5236 - lr: 0.0100\n",
      "Epoch 9/500\n",
      "41928/41928 [==============================] - 2s 48us/sample - loss: 73.8888 - reconstruction_loss: 433.7787 - KL_loss: 27.0853 - val_loss: 24924.0254 - val_reconstruction_loss: 24292.3828 - val_KL_loss: 631.6433 - lr: 0.0100\n",
      "Epoch 10/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 73.0609 - reconstruction_loss: 421.2148 - KL_loss: 27.5694 - val_loss: 19868.3047 - val_reconstruction_loss: 19368.5215 - val_KL_loss: 499.7839 - lr: 0.0100\n",
      "Epoch 11/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 72.3800 - reconstruction_loss: 410.5293 - KL_loss: 28.0143 - val_loss: 15881.8799 - val_reconstruction_loss: 15470.7148 - val_KL_loss: 411.1645 - lr: 0.0100\n",
      "Epoch 12/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 71.7980 - reconstruction_loss: 401.2557 - KL_loss: 28.4072 - val_loss: 12804.5117 - val_reconstruction_loss: 12458.7305 - val_KL_loss: 345.7825 - lr: 0.0100\n",
      "Epoch 13/500\n",
      "41928/41928 [==============================] - 2s 59us/sample - loss: 71.2457 - reconstruction_loss: 393.3006 - KL_loss: 28.7323 - val_loss: 10378.2939 - val_reconstruction_loss: 10081.3848 - val_KL_loss: 296.9095 - lr: 0.0100\n",
      "Epoch 14/500\n",
      "41928/41928 [==============================] - 2s 48us/sample - loss: 70.7641 - reconstruction_loss: 386.0047 - KL_loss: 28.9780 - val_loss: 8285.5557 - val_reconstruction_loss: 8029.6567 - val_KL_loss: 255.8992 - lr: 0.0100\n",
      "Epoch 15/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 70.3212 - reconstruction_loss: 379.3780 - KL_loss: 29.1349 - val_loss: 6712.9746 - val_reconstruction_loss: 6490.4346 - val_KL_loss: 222.5398 - lr: 0.0100\n",
      "Epoch 16/500\n",
      "41928/41928 [==============================] - 2s 51us/sample - loss: 69.8984 - reconstruction_loss: 373.5026 - KL_loss: 29.2043 - val_loss: 5357.4692 - val_reconstruction_loss: 5160.7979 - val_KL_loss: 196.6707 - lr: 0.0100\n",
      "Epoch 17/500\n",
      "41928/41928 [==============================] - 2s 48us/sample - loss: 69.5221 - reconstruction_loss: 368.0037 - KL_loss: 29.1891 - val_loss: 4385.2944 - val_reconstruction_loss: 4211.2124 - val_KL_loss: 174.0820 - lr: 0.0100\n",
      "Epoch 18/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 69.1869 - reconstruction_loss: 363.5222 - KL_loss: 29.0979 - val_loss: 3584.4248 - val_reconstruction_loss: 3427.9495 - val_KL_loss: 156.4752 - lr: 0.0100\n",
      "Epoch 19/500\n",
      "41928/41928 [==============================] - 2s 51us/sample - loss: 68.8635 - reconstruction_loss: 359.2110 - KL_loss: 28.9347 - val_loss: 3013.8108 - val_reconstruction_loss: 2872.5212 - val_KL_loss: 141.2896 - lr: 0.0100\n",
      "Epoch 20/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 68.5613 - reconstruction_loss: 355.3245 - KL_loss: 28.7182 - val_loss: 2488.3794 - val_reconstruction_loss: 2360.1704 - val_KL_loss: 128.2093 - lr: 0.0100\n",
      "Epoch 21/500\n",
      "41928/41928 [==============================] - 2s 51us/sample - loss: 68.2677 - reconstruction_loss: 351.5099 - KL_loss: 28.4531 - val_loss: 2104.5725 - val_reconstruction_loss: 1987.7664 - val_KL_loss: 116.8064 - lr: 0.0100\n",
      "Epoch 22/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 68.0014 - reconstruction_loss: 348.1110 - KL_loss: 28.1525 - val_loss: 1740.7850 - val_reconstruction_loss: 1635.2134 - val_KL_loss: 105.5717 - lr: 0.0100\n",
      "Epoch 23/500\n",
      "41928/41928 [==============================] - 2s 48us/sample - loss: 67.7441 - reconstruction_loss: 344.9514 - KL_loss: 27.8218 - val_loss: 1474.1478 - val_reconstruction_loss: 1378.1472 - val_KL_loss: 96.0007 - lr: 0.0100\n",
      "Epoch 24/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 67.5237 - reconstruction_loss: 342.1222 - KL_loss: 27.4717 - val_loss: 1245.1467 - val_reconstruction_loss: 1156.8923 - val_KL_loss: 88.2546 - lr: 0.0100\n",
      "Epoch 25/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 67.3026 - reconstruction_loss: 339.2146 - KL_loss: 27.1067 - val_loss: 1069.6592 - val_reconstruction_loss: 989.0954 - val_KL_loss: 80.5638 - lr: 0.0100\n",
      "Epoch 26/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 67.0676 - reconstruction_loss: 336.4142 - KL_loss: 26.7403 - val_loss: 898.0116 - val_reconstruction_loss: 824.6902 - val_KL_loss: 73.3214 - lr: 0.0100\n",
      "Epoch 27/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 66.8569 - reconstruction_loss: 333.8597 - KL_loss: 26.3742 - val_loss: 771.7255 - val_reconstruction_loss: 704.3407 - val_KL_loss: 67.3848 - lr: 0.0100\n",
      "Epoch 28/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 66.6468 - reconstruction_loss: 331.3361 - KL_loss: 26.0146 - val_loss: 684.2901 - val_reconstruction_loss: 621.9225 - val_KL_loss: 62.3676 - lr: 0.0100\n",
      "Epoch 29/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 66.4721 - reconstruction_loss: 329.0663 - KL_loss: 25.6728 - val_loss: 595.2198 - val_reconstruction_loss: 538.1821 - val_KL_loss: 57.0377 - lr: 0.0100\n",
      "Epoch 30/500\n",
      "41928/41928 [==============================] - 2s 56us/sample - loss: 66.2749 - reconstruction_loss: 326.6542 - KL_loss: 25.3545 - val_loss: 535.5231 - val_reconstruction_loss: 482.2309 - val_KL_loss: 53.2922 - lr: 0.0100\n",
      "Epoch 31/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 66.1176 - reconstruction_loss: 324.3606 - KL_loss: 25.0551 - val_loss: 486.5654 - val_reconstruction_loss: 436.5779 - val_KL_loss: 49.9874 - lr: 0.0100\n",
      "Epoch 32/500\n",
      "41928/41928 [==============================] - 2s 54us/sample - loss: 65.9433 - reconstruction_loss: 322.1567 - KL_loss: 24.7925 - val_loss: 437.4482 - val_reconstruction_loss: 391.2533 - val_KL_loss: 46.1949 - lr: 0.0100\n",
      "Epoch 33/500\n",
      "41928/41928 [==============================] - 2s 54us/sample - loss: 65.7567 - reconstruction_loss: 320.0184 - KL_loss: 24.5558 - val_loss: 402.1677 - val_reconstruction_loss: 358.6797 - val_KL_loss: 43.4880 - lr: 0.0100\n",
      "Epoch 34/500\n",
      "41928/41928 [==============================] - 2s 52us/sample - loss: 65.5949 - reconstruction_loss: 317.8001 - KL_loss: 24.3325 - val_loss: 376.2615 - val_reconstruction_loss: 335.5316 - val_KL_loss: 40.7299 - lr: 0.0100\n",
      "Epoch 35/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 65.3955 - reconstruction_loss: 315.6753 - KL_loss: 24.1460 - val_loss: 353.9299 - val_reconstruction_loss: 315.3950 - val_KL_loss: 38.5349 - lr: 0.0100\n",
      "Epoch 36/500\n",
      "41928/41928 [==============================] - 2s 54us/sample - loss: 65.2291 - reconstruction_loss: 313.5544 - KL_loss: 23.9792 - val_loss: 337.6633 - val_reconstruction_loss: 300.9929 - val_KL_loss: 36.6704 - lr: 0.0100\n",
      "Epoch 37/500\n",
      "41928/41928 [==============================] - 2s 55us/sample - loss: 65.0305 - reconstruction_loss: 311.2812 - KL_loss: 23.8433 - val_loss: 324.1453 - val_reconstruction_loss: 289.4104 - val_KL_loss: 34.7349 - lr: 0.0100\n",
      "Epoch 38/500\n",
      "41928/41928 [==============================] - 2s 51us/sample - loss: 64.8716 - reconstruction_loss: 309.2281 - KL_loss: 23.7427 - val_loss: 314.4058 - val_reconstruction_loss: 281.1980 - val_KL_loss: 33.2078 - lr: 0.0100\n",
      "Epoch 39/500\n",
      "41928/41928 [==============================] - 2s 52us/sample - loss: 64.7079 - reconstruction_loss: 307.1158 - KL_loss: 23.6692 - val_loss: 302.2594 - val_reconstruction_loss: 270.5545 - val_KL_loss: 31.7049 - lr: 0.0100\n",
      "Epoch 40/500\n",
      "41928/41928 [==============================] - 2s 52us/sample - loss: 64.5754 - reconstruction_loss: 305.4669 - KL_loss: 23.6159 - val_loss: 295.8150 - val_reconstruction_loss: 265.3784 - val_KL_loss: 30.4365 - lr: 0.0100\n",
      "Epoch 41/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 64.3967 - reconstruction_loss: 303.3383 - KL_loss: 23.5798 - val_loss: 290.1786 - val_reconstruction_loss: 260.5767 - val_KL_loss: 29.6019 - lr: 0.0100\n",
      "Epoch 42/500\n",
      "41928/41928 [==============================] - 2s 54us/sample - loss: 64.2153 - reconstruction_loss: 301.2097 - KL_loss: 23.5712 - val_loss: 286.4504 - val_reconstruction_loss: 257.6903 - val_KL_loss: 28.7602 - lr: 0.0100\n",
      "Epoch 43/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 64.0740 - reconstruction_loss: 299.5013 - KL_loss: 23.5783 - val_loss: 281.5594 - val_reconstruction_loss: 253.4733 - val_KL_loss: 28.0861 - lr: 0.0100\n",
      "Epoch 44/500\n",
      "41928/41928 [==============================] - 2s 54us/sample - loss: 63.9245 - reconstruction_loss: 297.7519 - KL_loss: 23.5961 - val_loss: 277.5886 - val_reconstruction_loss: 250.2736 - val_KL_loss: 27.3150 - lr: 0.0100\n",
      "Epoch 45/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 63.7492 - reconstruction_loss: 295.6529 - KL_loss: 23.6220 - val_loss: 275.0441 - val_reconstruction_loss: 248.1837 - val_KL_loss: 26.8604 - lr: 0.0100\n",
      "Epoch 46/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 63.6066 - reconstruction_loss: 293.7647 - KL_loss: 23.6657 - val_loss: 273.4781 - val_reconstruction_loss: 247.2001 - val_KL_loss: 26.2780 - lr: 0.0100\n",
      "Epoch 47/500\n",
      "41928/41928 [==============================] - 2s 55us/sample - loss: 63.4411 - reconstruction_loss: 291.9832 - KL_loss: 23.7070 - val_loss: 270.9760 - val_reconstruction_loss: 244.9708 - val_KL_loss: 26.0052 - lr: 0.0100\n",
      "Epoch 48/500\n",
      "41928/41928 [==============================] - 2s 54us/sample - loss: 63.2959 - reconstruction_loss: 290.2062 - KL_loss: 23.7399 - val_loss: 269.1734 - val_reconstruction_loss: 243.0915 - val_KL_loss: 26.0819 - lr: 0.0100\n",
      "Epoch 49/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 63.1349 - reconstruction_loss: 288.4539 - KL_loss: 23.7943 - val_loss: 268.8037 - val_reconstruction_loss: 243.1981 - val_KL_loss: 25.6056 - lr: 0.0100\n",
      "Epoch 50/500\n",
      "41928/41928 [==============================] - 2s 52us/sample - loss: 62.9676 - reconstruction_loss: 286.6589 - KL_loss: 23.8567 - val_loss: 268.2292 - val_reconstruction_loss: 242.8905 - val_KL_loss: 25.3387 - lr: 0.0100\n",
      "Epoch 51/500\n",
      "41928/41928 [==============================] - 2s 52us/sample - loss: 62.8159 - reconstruction_loss: 284.9782 - KL_loss: 23.9225 - val_loss: 266.7801 - val_reconstruction_loss: 241.6410 - val_KL_loss: 25.1390 - lr: 0.0100\n",
      "Epoch 52/500\n",
      "41928/41928 [==============================] - 2s 54us/sample - loss: 62.6471 - reconstruction_loss: 283.0756 - KL_loss: 23.9979 - val_loss: 267.5983 - val_reconstruction_loss: 242.7713 - val_KL_loss: 24.8270 - lr: 0.0100\n",
      "Epoch 53/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 62.4904 - reconstruction_loss: 281.4464 - KL_loss: 24.0839 - val_loss: 264.0883 - val_reconstruction_loss: 239.5333 - val_KL_loss: 24.5550 - lr: 0.0100\n",
      "Epoch 54/500\n",
      "41928/41928 [==============================] - 2s 54us/sample - loss: 62.3327 - reconstruction_loss: 279.7837 - KL_loss: 24.1731 - val_loss: 263.5742 - val_reconstruction_loss: 239.1379 - val_KL_loss: 24.4363 - lr: 0.0100\n",
      "Epoch 55/500\n",
      "41928/41928 [==============================] - 2s 52us/sample - loss: 62.2038 - reconstruction_loss: 278.4088 - KL_loss: 24.2626 - val_loss: 262.4815 - val_reconstruction_loss: 238.3019 - val_KL_loss: 24.1796 - lr: 0.0100\n",
      "Epoch 56/500\n",
      "41928/41928 [==============================] - 2s 54us/sample - loss: 62.0654 - reconstruction_loss: 276.7652 - KL_loss: 24.3515 - val_loss: 260.9147 - val_reconstruction_loss: 236.5352 - val_KL_loss: 24.3794 - lr: 0.0100\n",
      "Epoch 57/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 61.9361 - reconstruction_loss: 275.6165 - KL_loss: 24.4408 - val_loss: 259.7219 - val_reconstruction_loss: 235.5019 - val_KL_loss: 24.2200 - lr: 0.0100\n",
      "Epoch 58/500\n",
      "41928/41928 [==============================] - 2s 54us/sample - loss: 61.7933 - reconstruction_loss: 274.1114 - KL_loss: 24.5289 - val_loss: 257.4425 - val_reconstruction_loss: 233.2961 - val_KL_loss: 24.1464 - lr: 0.0100\n",
      "Epoch 59/500\n",
      "41928/41928 [==============================] - 2s 56us/sample - loss: 61.6375 - reconstruction_loss: 272.5852 - KL_loss: 24.6145 - val_loss: 256.7156 - val_reconstruction_loss: 232.4701 - val_KL_loss: 24.2455 - lr: 0.0100\n",
      "Epoch 60/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 61.5056 - reconstruction_loss: 271.2854 - KL_loss: 24.7048 - val_loss: 254.6865 - val_reconstruction_loss: 230.5380 - val_KL_loss: 24.1484 - lr: 0.0100\n",
      "Epoch 61/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 61.3410 - reconstruction_loss: 269.8292 - KL_loss: 24.7959 - val_loss: 251.7932 - val_reconstruction_loss: 227.4860 - val_KL_loss: 24.3072 - lr: 0.0100\n",
      "Epoch 62/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 61.2042 - reconstruction_loss: 268.6896 - KL_loss: 24.8798 - val_loss: 250.8153 - val_reconstruction_loss: 226.4071 - val_KL_loss: 24.4081 - lr: 0.0100\n",
      "Epoch 63/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 61.0556 - reconstruction_loss: 267.4557 - KL_loss: 24.9732 - val_loss: 251.7319 - val_reconstruction_loss: 227.5916 - val_KL_loss: 24.1403 - lr: 0.0100\n",
      "Epoch 64/500\n",
      "41928/41928 [==============================] - 2s 52us/sample - loss: 60.9010 - reconstruction_loss: 266.1645 - KL_loss: 25.0696 - val_loss: 249.1247 - val_reconstruction_loss: 224.5712 - val_KL_loss: 24.5535 - lr: 0.0100\n",
      "Epoch 65/500\n",
      "41928/41928 [==============================] - 2s 52us/sample - loss: 60.7934 - reconstruction_loss: 265.0319 - KL_loss: 25.1587 - val_loss: 251.5691 - val_reconstruction_loss: 227.3178 - val_KL_loss: 24.2513 - lr: 0.0100\n",
      "Epoch 66/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 60.6394 - reconstruction_loss: 264.0291 - KL_loss: 25.2571 - val_loss: 251.5405 - val_reconstruction_loss: 227.0734 - val_KL_loss: 24.4671 - lr: 0.0100\n",
      "Epoch 67/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 60.4744 - reconstruction_loss: 262.7859 - KL_loss: 25.3523 - val_loss: 249.2975 - val_reconstruction_loss: 224.6919 - val_KL_loss: 24.6055 - lr: 0.0100\n",
      "Epoch 68/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 60.3711 - reconstruction_loss: 261.8038 - KL_loss: 25.4612 - val_loss: 249.4172 - val_reconstruction_loss: 224.5764 - val_KL_loss: 24.8408 - lr: 0.0100\n",
      "Epoch 69/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 60.2486 - reconstruction_loss: 260.6269 - KL_loss: 25.5734 - val_loss: 250.0385 - val_reconstruction_loss: 225.2615 - val_KL_loss: 24.7770 - lr: 0.0100\n",
      "Epoch 70/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 60.0739 - reconstruction_loss: 259.5955 - KL_loss: 25.6618 - val_loss: 250.5160 - val_reconstruction_loss: 225.3305 - val_KL_loss: 25.1856 - lr: 0.0100\n",
      "Epoch 71/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 59.9514 - reconstruction_loss: 258.6785 - KL_loss: 25.7464 - val_loss: 250.7379 - val_reconstruction_loss: 225.5772 - val_KL_loss: 25.1607 - lr: 0.0100\n",
      "Epoch 72/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 59.7708 - reconstruction_loss: 257.5647 - KL_loss: 25.8209 - val_loss: 248.8069 - val_reconstruction_loss: 223.2766 - val_KL_loss: 25.5303 - lr: 0.0100\n",
      "Epoch 73/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 59.6402 - reconstruction_loss: 256.5213 - KL_loss: 25.9087 - val_loss: 248.8276 - val_reconstruction_loss: 223.2508 - val_KL_loss: 25.5767 - lr: 0.0100\n",
      "Epoch 74/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 59.5424 - reconstruction_loss: 255.8354 - KL_loss: 26.0065 - val_loss: 248.4086 - val_reconstruction_loss: 222.7966 - val_KL_loss: 25.6120 - lr: 0.0100\n",
      "Epoch 75/500\n",
      "41928/41928 [==============================] - 2s 53us/sample - loss: 59.4156 - reconstruction_loss: 254.9048 - KL_loss: 26.1180 - val_loss: 247.9807 - val_reconstruction_loss: 222.3683 - val_KL_loss: 25.6124 - lr: 0.0100\n",
      "Epoch 76/500\n",
      "41928/41928 [==============================] - 2s 52us/sample - loss: 59.2652 - reconstruction_loss: 253.7494 - KL_loss: 26.2352 - val_loss: 248.2559 - val_reconstruction_loss: 222.6120 - val_KL_loss: 25.6439 - lr: 0.0100\n",
      "Epoch 77/500\n",
      "41928/41928 [==============================] - 2s 51us/sample - loss: 59.1637 - reconstruction_loss: 253.0731 - KL_loss: 26.3499 - val_loss: 248.3707 - val_reconstruction_loss: 222.7200 - val_KL_loss: 25.6507 - lr: 0.0100\n",
      "Epoch 78/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 59.0337 - reconstruction_loss: 252.0428 - KL_loss: 26.4661 - val_loss: 246.9426 - val_reconstruction_loss: 221.3085 - val_KL_loss: 25.6341 - lr: 0.0100\n",
      "Epoch 79/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 58.8981 - reconstruction_loss: 251.1521 - KL_loss: 26.5849 - val_loss: 245.2805 - val_reconstruction_loss: 219.2557 - val_KL_loss: 26.0248 - lr: 0.0100\n",
      "Epoch 80/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 58.7696 - reconstruction_loss: 250.2331 - KL_loss: 26.7136 - val_loss: 245.7493 - val_reconstruction_loss: 219.6850 - val_KL_loss: 26.0643 - lr: 0.0100\n",
      "Epoch 81/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 58.6633 - reconstruction_loss: 249.4062 - KL_loss: 26.8402 - val_loss: 243.8058 - val_reconstruction_loss: 217.6097 - val_KL_loss: 26.1961 - lr: 0.0100\n",
      "Epoch 82/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 58.5451 - reconstruction_loss: 248.6154 - KL_loss: 26.9738 - val_loss: 242.6579 - val_reconstruction_loss: 216.4238 - val_KL_loss: 26.2341 - lr: 0.0100\n",
      "Epoch 83/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 58.4370 - reconstruction_loss: 247.7572 - KL_loss: 27.0916 - val_loss: 240.5439 - val_reconstruction_loss: 214.1181 - val_KL_loss: 26.4258 - lr: 0.0100\n",
      "Epoch 84/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 58.3230 - reconstruction_loss: 246.8440 - KL_loss: 27.2413 - val_loss: 241.1516 - val_reconstruction_loss: 214.8436 - val_KL_loss: 26.3080 - lr: 0.0100\n",
      "Epoch 85/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 58.2083 - reconstruction_loss: 246.0855 - KL_loss: 27.3738 - val_loss: 239.5956 - val_reconstruction_loss: 212.9658 - val_KL_loss: 26.6297 - lr: 0.0100\n",
      "Epoch 86/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 58.0937 - reconstruction_loss: 245.4634 - KL_loss: 27.5068 - val_loss: 240.8293 - val_reconstruction_loss: 214.2200 - val_KL_loss: 26.6093 - lr: 0.0100\n",
      "Epoch 87/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 57.9686 - reconstruction_loss: 244.6237 - KL_loss: 27.6385 - val_loss: 239.1677 - val_reconstruction_loss: 212.4622 - val_KL_loss: 26.7055 - lr: 0.0100\n",
      "Epoch 88/500\n",
      "41928/41928 [==============================] - 2s 51us/sample - loss: 57.8167 - reconstruction_loss: 243.8146 - KL_loss: 27.7544 - val_loss: 239.3806 - val_reconstruction_loss: 212.5513 - val_KL_loss: 26.8293 - lr: 0.0100\n",
      "Epoch 89/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 57.7182 - reconstruction_loss: 243.6048 - KL_loss: 27.8636 - val_loss: 241.2568 - val_reconstruction_loss: 214.3512 - val_KL_loss: 26.9056 - lr: 0.0100\n",
      "Epoch 90/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 57.5971 - reconstruction_loss: 242.9798 - KL_loss: 27.9678 - val_loss: 239.8900 - val_reconstruction_loss: 212.8169 - val_KL_loss: 27.0731 - lr: 0.0100\n",
      "Epoch 91/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 57.4910 - reconstruction_loss: 242.2545 - KL_loss: 28.0893 - val_loss: 240.0589 - val_reconstruction_loss: 212.9002 - val_KL_loss: 27.1587 - lr: 0.0100\n",
      "Epoch 92/500\n",
      "41928/41928 [==============================] - 2s 48us/sample - loss: 57.3780 - reconstruction_loss: 241.7472 - KL_loss: 28.1964 - val_loss: 240.2898 - val_reconstruction_loss: 212.9328 - val_KL_loss: 27.3571 - lr: 0.0100\n",
      "Epoch 93/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 57.2230 - reconstruction_loss: 240.9631 - KL_loss: 28.3214 - val_loss: 244.5687 - val_reconstruction_loss: 217.4250 - val_KL_loss: 27.1437 - lr: 0.0100\n",
      "Epoch 94/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 57.1012 - reconstruction_loss: 240.4008 - KL_loss: 28.4545 - val_loss: 242.3689 - val_reconstruction_loss: 214.8620 - val_KL_loss: 27.5069 - lr: 0.0100\n",
      "Epoch 95/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 56.9797 - reconstruction_loss: 239.9172 - KL_loss: 28.5702 - val_loss: 241.8411 - val_reconstruction_loss: 214.0848 - val_KL_loss: 27.7564 - lr: 0.0100\n",
      "Epoch 96/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 56.9099 - reconstruction_loss: 239.5507 - KL_loss: 28.6804 - val_loss: 241.3944 - val_reconstruction_loss: 213.3062 - val_KL_loss: 28.0882 - lr: 0.0100\n",
      "Epoch 97/500\n",
      "41928/41928 [==============================] - 2s 57us/sample - loss: 56.7820 - reconstruction_loss: 238.9623 - KL_loss: 28.7811 - val_loss: 239.1574 - val_reconstruction_loss: 210.8530 - val_KL_loss: 28.3045 - lr: 0.0100\n",
      "Epoch 98/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 56.6439 - reconstruction_loss: 238.4587 - KL_loss: 28.9009 - val_loss: 240.1351 - val_reconstruction_loss: 211.8738 - val_KL_loss: 28.2613 - lr: 0.0100\n",
      "Epoch 99/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 56.5446 - reconstruction_loss: 237.8146 - KL_loss: 29.0312 - val_loss: 241.2221 - val_reconstruction_loss: 212.7291 - val_KL_loss: 28.4930 - lr: 0.0100\n",
      "Epoch 100/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 56.4422 - reconstruction_loss: 237.5712 - KL_loss: 29.1554 - val_loss: 242.4189 - val_reconstruction_loss: 214.0298 - val_KL_loss: 28.3891 - lr: 0.0100\n",
      "Epoch 101/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 56.3343 - reconstruction_loss: 236.9093 - KL_loss: 29.2825 - val_loss: 241.4983 - val_reconstruction_loss: 212.9342 - val_KL_loss: 28.5641 - lr: 0.0100\n",
      "Epoch 102/500\n",
      "41928/41928 [==============================] - 3s 66us/sample - loss: 56.2181 - reconstruction_loss: 236.4497 - KL_loss: 29.4074 - val_loss: 242.5908 - val_reconstruction_loss: 213.9847 - val_KL_loss: 28.6061 - lr: 0.0100\n",
      "Epoch 103/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 56.0836 - reconstruction_loss: 235.9366 - KL_loss: 29.5431 - val_loss: 242.0345 - val_reconstruction_loss: 213.2722 - val_KL_loss: 28.7622 - lr: 0.0100\n",
      "Epoch 104/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 55.9858 - reconstruction_loss: 235.6539 - KL_loss: 29.6532 - val_loss: 242.2518 - val_reconstruction_loss: 213.0522 - val_KL_loss: 29.1996 - lr: 0.0100\n",
      "Epoch 105/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 55.8715 - reconstruction_loss: 235.3324 - KL_loss: 29.7885 - val_loss: 243.5578 - val_reconstruction_loss: 214.3153 - val_KL_loss: 29.2425 - lr: 0.0100\n",
      "Epoch 106/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 55.7446 - reconstruction_loss: 234.8516 - KL_loss: 29.9155 - val_loss: 242.8618 - val_reconstruction_loss: 213.4895 - val_KL_loss: 29.3724 - lr: 0.0100\n",
      "Epoch 107/500\n",
      "41928/41928 [==============================] - 2s 51us/sample - loss: 55.5986 - reconstruction_loss: 234.3336 - KL_loss: 30.0404 - val_loss: 242.4307 - val_reconstruction_loss: 212.9053 - val_KL_loss: 29.5253 - lr: 0.0100\n",
      "Epoch 108/500\n",
      "41928/41928 [==============================] - 2s 51us/sample - loss: 55.4883 - reconstruction_loss: 233.8971 - KL_loss: 30.1540 - val_loss: 246.1967 - val_reconstruction_loss: 216.6695 - val_KL_loss: 29.5272 - lr: 0.0100\n",
      "Epoch 109/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 55.3825 - reconstruction_loss: 233.5004 - KL_loss: 30.2548 - val_loss: 243.4553 - val_reconstruction_loss: 213.7649 - val_KL_loss: 29.6903 - lr: 0.0100\n",
      "Epoch 110/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 55.2270 - reconstruction_loss: 232.9501 - KL_loss: 30.3665 - val_loss: 245.2780 - val_reconstruction_loss: 215.6695 - val_KL_loss: 29.6085 - lr: 0.0100\n",
      "Epoch 111/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 55.1344 - reconstruction_loss: 232.8342 - KL_loss: 30.4645 - val_loss: 244.3148 - val_reconstruction_loss: 214.4201 - val_KL_loss: 29.8947 - lr: 0.0100\n",
      "Epoch 112/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 55.0467 - reconstruction_loss: 232.6649 - KL_loss: 30.5501 - val_loss: 244.0338 - val_reconstruction_loss: 213.8964 - val_KL_loss: 30.1374 - lr: 0.0100\n",
      "Epoch 113/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 54.9527 - reconstruction_loss: 232.1378 - KL_loss: 30.6685 - val_loss: 244.7146 - val_reconstruction_loss: 214.6853 - val_KL_loss: 30.0293 - lr: 0.0100\n",
      "Epoch 114/500\n",
      "41928/41928 [==============================] - 2s 48us/sample - loss: 54.8019 - reconstruction_loss: 231.5508 - KL_loss: 30.8093 - val_loss: 247.6969 - val_reconstruction_loss: 217.5139 - val_KL_loss: 30.1829 - lr: 0.0100\n",
      "Epoch 115/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 54.7111 - reconstruction_loss: 231.1853 - KL_loss: 30.9304 - val_loss: 250.0383 - val_reconstruction_loss: 219.9998 - val_KL_loss: 30.0385 - lr: 0.0100\n",
      "Epoch 116/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 54.6290 - reconstruction_loss: 230.9306 - KL_loss: 31.0430 - val_loss: 250.1918 - val_reconstruction_loss: 219.9067 - val_KL_loss: 30.2851 - lr: 0.0100\n",
      "Epoch 117/500\n",
      "41928/41928 [==============================] - 2s 52us/sample - loss: 54.4980 - reconstruction_loss: 230.8262 - KL_loss: 31.1614 - val_loss: 249.3053 - val_reconstruction_loss: 218.6886 - val_KL_loss: 30.6168 - lr: 0.0100\n",
      "Epoch 118/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 54.3992 - reconstruction_loss: 230.2515 - KL_loss: 31.2760 - val_loss: 248.8086 - val_reconstruction_loss: 218.4532 - val_KL_loss: 30.3554 - lr: 0.0090\n",
      "Epoch 119/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 54.2699 - reconstruction_loss: 229.8017 - KL_loss: 31.3905 - val_loss: 247.0545 - val_reconstruction_loss: 216.5464 - val_KL_loss: 30.5081 - lr: 0.0090\n",
      "Epoch 120/500\n",
      "41928/41928 [==============================] - 2s 52us/sample - loss: 54.1947 - reconstruction_loss: 229.3012 - KL_loss: 31.4986 - val_loss: 247.1393 - val_reconstruction_loss: 216.5294 - val_KL_loss: 30.6100 - lr: 0.0090\n",
      "Epoch 121/500\n",
      "41928/41928 [==============================] - 2s 51us/sample - loss: 54.0992 - reconstruction_loss: 229.2916 - KL_loss: 31.6110 - val_loss: 246.3790 - val_reconstruction_loss: 215.6041 - val_KL_loss: 30.7749 - lr: 0.0090\n",
      "Epoch 122/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 54.0123 - reconstruction_loss: 229.0161 - KL_loss: 31.7019 - val_loss: 247.3601 - val_reconstruction_loss: 216.6037 - val_KL_loss: 30.7563 - lr: 0.0090\n",
      "Epoch 123/500\n",
      "41928/41928 [==============================] - 2s 51us/sample - loss: 53.9272 - reconstruction_loss: 228.7515 - KL_loss: 31.8061 - val_loss: 248.0026 - val_reconstruction_loss: 217.2321 - val_KL_loss: 30.7705 - lr: 0.0090\n",
      "Epoch 124/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 53.8288 - reconstruction_loss: 228.3582 - KL_loss: 31.9161 - val_loss: 244.5293 - val_reconstruction_loss: 213.5745 - val_KL_loss: 30.9548 - lr: 0.0090\n",
      "Epoch 125/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 53.7362 - reconstruction_loss: 228.0573 - KL_loss: 32.0302 - val_loss: 244.6447 - val_reconstruction_loss: 213.6171 - val_KL_loss: 31.0276 - lr: 0.0090\n",
      "Epoch 126/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 53.6240 - reconstruction_loss: 227.8318 - KL_loss: 32.1453 - val_loss: 243.0734 - val_reconstruction_loss: 211.8678 - val_KL_loss: 31.2056 - lr: 0.0090\n",
      "Epoch 127/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 53.5408 - reconstruction_loss: 227.3309 - KL_loss: 32.2488 - val_loss: 245.0535 - val_reconstruction_loss: 213.8383 - val_KL_loss: 31.2152 - lr: 0.0090\n",
      "Epoch 128/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 53.4327 - reconstruction_loss: 226.9519 - KL_loss: 32.3530 - val_loss: 245.3834 - val_reconstruction_loss: 214.0960 - val_KL_loss: 31.2874 - lr: 0.0090\n",
      "Epoch 129/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 53.3539 - reconstruction_loss: 226.8177 - KL_loss: 32.4574 - val_loss: 247.0128 - val_reconstruction_loss: 215.6194 - val_KL_loss: 31.3934 - lr: 0.0090\n",
      "Epoch 130/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 53.2961 - reconstruction_loss: 226.5511 - KL_loss: 32.5724 - val_loss: 248.9079 - val_reconstruction_loss: 217.3792 - val_KL_loss: 31.5286 - lr: 0.0090\n",
      "Epoch 131/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 53.1761 - reconstruction_loss: 226.2763 - KL_loss: 32.6855 - val_loss: 247.8856 - val_reconstruction_loss: 216.1173 - val_KL_loss: 31.7683 - lr: 0.0090\n",
      "Epoch 132/500\n",
      "41928/41928 [==============================] - 2s 50us/sample - loss: 53.1092 - reconstruction_loss: 226.1882 - KL_loss: 32.7890 - val_loss: 247.7925 - val_reconstruction_loss: 215.9319 - val_KL_loss: 31.8605 - lr: 0.0090\n",
      "Epoch 133/500\n",
      "41928/41928 [==============================] - 2s 51us/sample - loss: 53.0103 - reconstruction_loss: 225.9032 - KL_loss: 32.8795 - val_loss: 246.6621 - val_reconstruction_loss: 214.7732 - val_KL_loss: 31.8889 - lr: 0.0090\n",
      "Epoch 134/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 52.8982 - reconstruction_loss: 225.5626 - KL_loss: 32.9934 - val_loss: 248.7327 - val_reconstruction_loss: 216.7515 - val_KL_loss: 31.9812 - lr: 0.0090\n",
      "Epoch 135/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 52.8030 - reconstruction_loss: 225.3658 - KL_loss: 33.1151 - val_loss: 252.8596 - val_reconstruction_loss: 220.9667 - val_KL_loss: 31.8929 - lr: 0.0090\n",
      "Epoch 136/500\n",
      "41928/41928 [==============================] - 2s 49us/sample - loss: 52.7355 - reconstruction_loss: 225.3190 - KL_loss: 33.2247 - val_loss: 252.8725 - val_reconstruction_loss: 220.6735 - val_KL_loss: 32.1990 - lr: 0.0090\n",
      "Epoch 137/500\n",
      "41928/41928 [==============================] - 2s 51us/sample - loss: 52.6817 - reconstruction_loss: 225.2603 - KL_loss: 33.3218 - val_loss: 254.2600 - val_reconstruction_loss: 221.9750 - val_KL_loss: 32.2851 - lr: 0.0090\n",
      "\n",
      "training finished in 137 epochs (early stop), transform data to adjust the platform effect...\n",
      "\n",
      "HIGHLIGHT: when transforming data, after reversed Min-Max Scaling, apply exp transformation then multiple the factor and round to integer\n",
      "\n",
      "re-run DE on CVAE transformed scRNA-seq data!\n",
      "filtering genes present in <10 cells: 2 genes removed\n",
      "\n",
      "Differential analysis across cell-types on scRNA-seq data...\n",
      "0%...8%...17%...25%...33%...\n",
      "WARNING: only 17 genes passing filtering (<20) for PVALB vs SST\n",
      "42%...50%...58%...67%...\n",
      "WARNING: only 11 genes passing filtering (<20) for eL2/3 vs eL5\n",
      "\n",
      "WARNING: only 14 genes passing filtering (<20) for eL2/3 vs eL6\n",
      "75%...\n",
      "WARNING: only 1 genes passing filtering (<20) for eL4 vs eL5\n",
      "\n",
      "WARNING: only 15 genes passing filtering (<20) for eL4 vs eL6\n",
      "83%...92%...\n",
      "WARNING: only 11 genes passing filtering (<20) for eL6 vs eL5\n",
      "100%\n",
      "finally selected 471 cell-type marker genes\n",
      "\n",
      "HIGHLIGHT: got initial guess of cell type proportions based on original CVAE latent embedding\n",
      "embedding dimension: 36\n",
      "\n",
      "save variables related to CVAE to files!\n",
      "\n",
      "platform effect adjustment by CVAE finished. Elapsed time: 13.06 minutes.\n",
      "\n",
      "\n",
      "use the marker genes derived from CVAE transformed scRNA-seq for downstream regression!\n",
      "\n",
      "gene filtering before modeling...\n",
      "51 genes with nUMIs<5 in all spatial spots and need to be excluded\n",
      "finally use 420 genes for modeling\n",
      "\n",
      "spot filtering before modeling...\n",
      "all spots passed filtering\n",
      "\n",
      "\n",
      "######### Start GLRM modeling... #########\n",
      "\n",
      "GLRM settings:\n",
      "use SciPy minimize method:  L-BFGS-B\n",
      "global optimization turned off, local minimum will be used in GLRM\n",
      "use hybrid version of GLRM\n",
      "Numba detected total 64 available CPU cores. Use 64 CPU cores\n",
      "use 2001 points to calculate the heavy-tail density\n",
      "use weight threshold for Adaptive Lasso:  0.001\n",
      "total 334 unique nUMIs, min: 0.0, max: 710.0\n",
      "\n",
      "Build graph: \n",
      " Graph with 2002 nodes and 0 edges\n",
      "\n",
      "estimation of gene-specific platform effect gamma_g is skipped as already using CVAE to adjust platform effect\n",
      "\n",
      "\n",
      "Start GLRM fitting...\n",
      "\n",
      "first estimate MLE theta and corresponding e^alpha and sigma^2...\n",
      "\n",
      "GLRM model initialization...\n",
      "HIGHLIGHT: use initial guess derived from CVAE rather than uniform distribution for theta initialization\n",
      "calculate MLE theta and sigma^2 iteratively...\n",
      "  iter | time_opt | time_sig | sigma2\n",
      "     0 |   66.645 |   24.079 |  2.386\n",
      "     1 |   40.256 |   16.202 |  2.479\n",
      "     2 |   24.749 |   12.805 |  2.485\n",
      "     3 |   14.482 |    9.546 |  2.485\n",
      "MLE theta and sigma^2 calculation finished. Elapsed time: 3.48 minutes.\n",
      "MLE theta estimation finished. Elapsed time: 3.48 minutes.\n",
      "\n",
      "calculate weights of Adaptive Lasso...\n",
      "\n",
      "Stage 1: variable selection using Adaptive Lasso starts with the MLE theta and e^alpha, using already estimated sigma^2 and gamma_g...\n",
      "specified hyper-parameter for Adaptive Lasso is: [0.1, 0.268, 0.72, 1.931, 5.179, 13.895, 37.276, 100.0]\n",
      "hyper-parameter for Adaptive Lasso: use cross-validation to find the optimal value from 8 candidates...\n",
      "\n",
      "Start cross-validation for hyper-parameter lambda_r...\n",
      "directly estimate theta by Adaptive Lasso loss function as NO Graph Laplacian constrain!\n",
      "0%...11%...22%...33%...44%...56%...67%...early stop\n",
      "find optimal lambda_r 0.720 with average negative log-likelihood 232727.7576 by 5 fold cross-validation. Elapsed time: 33.89 minutes.\n",
      "\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |     21.881 |     21.880 |      0.241 |      0.241 |       1.00 |       1.00 |    5.519 |    0.000 |    0.004 |   0.198408 |   0.099204\n",
      "     1 |     21.830 |      0.250 |      0.241 |      0.263 |       1.00 |       2.00 |   28.586 |    0.000 |    0.004 |   0.197926 |   0.098963\n",
      "     2 |     14.027 |     18.862 |      0.238 |      0.290 |       2.00 |       2.00 |   29.644 |    0.001 |    0.004 |   0.124594 |   0.062297\n",
      "     3 |      8.628 |     19.974 |      0.239 |      0.301 |       2.00 |       2.00 |   26.261 |    0.000 |    0.004 |   0.045339 |   0.022670\n",
      "     4 |      8.689 |     11.910 |      0.231 |      0.305 |       2.00 |       2.00 |   25.180 |    0.000 |    0.004 |   0.071689 |   0.035845\n",
      "     5 |      8.068 |      5.837 |      0.227 |      0.308 |       2.00 |       2.00 |   26.546 |    0.000 |    0.004 |   0.068369 |   0.034184\n",
      "     6 |      6.335 |      6.537 |      0.226 |      0.313 |       2.00 |       4.00 |   29.100 |    0.001 |    0.005 |   0.047579 |   0.023790\n",
      "     7 |      5.071 |      9.601 |      0.229 |      0.326 |       4.00 |       4.00 |   29.770 |    0.000 |    0.004 |   0.030269 |   0.015135\n",
      "     8 |      4.403 |      8.529 |      0.228 |      0.339 |       4.00 |       4.00 |   24.715 |    0.000 |    0.004 |   0.026605 |   0.013302\n",
      "     9 |      4.074 |      6.058 |      0.225 |      0.351 |       4.00 |       8.00 |   24.396 |    0.001 |    0.005 |   0.026725 |   0.013362\n",
      "    10 |      3.527 |      8.521 |      0.228 |      0.372 |       8.00 |       8.00 |   28.971 |    0.001 |    0.004 |   0.021662 |   0.010831\n",
      "    11 |      3.045 |      9.976 |      0.229 |      0.390 |       8.00 |       8.00 |   27.076 |    0.001 |    0.004 |   0.016203 |   0.008101\n",
      "    12 |      2.767 |      8.633 |      0.228 |      0.405 |       8.00 |      16.00 |   25.496 |    0.001 |    0.004 |   0.015497 |   0.007748\n",
      "    13 |      2.434 |     10.843 |      0.230 |      0.433 |      16.00 |      16.00 |   28.712 |    0.001 |    0.004 |   0.013637 |   0.006819\n",
      "    14 |      2.117 |     12.520 |      0.232 |      0.458 |      16.00 |      16.00 |   26.897 |    0.001 |    0.004 |   0.011397 |   0.005698\n",
      "    15 |      1.907 |     11.573 |      0.231 |      0.479 |      16.00 |      32.00 |   26.105 |    0.001 |    0.004 |   0.009974 |   0.004987\n",
      "    16 |      1.686 |     14.538 |      0.234 |      0.518 |      32.00 |      32.00 |   27.358 |    0.000 |    0.004 |   0.008930 |   0.004465\n",
      "    17 |      1.481 |     16.668 |      0.236 |      0.554 |      32.00 |      32.00 |   25.698 |    0.000 |    0.004 |   0.007804 |   0.003902\n",
      "    18 |      1.333 |     15.501 |      0.235 |      0.584 |      32.00 |      64.00 |   26.553 |    0.001 |    0.004 |   0.006948 |   0.003474\n",
      "    19 |      1.158 |     21.185 |      0.240 |      0.639 |      64.00 |      64.00 |   27.250 |    0.001 |    0.004 |   0.006003 |   0.003001\n",
      "    20 |      1.001 |     24.464 |      0.244 |      0.685 |      64.00 |      64.00 |   26.035 |    0.000 |    0.004 |   0.005183 |   0.002591\n",
      "    21 |      0.904 |     21.757 |      0.241 |      0.724 |      64.00 |     128.00 |   24.393 |    0.001 |    0.004 |   0.004681 |   0.002340\n",
      "    22 |      0.794 |     27.427 |      0.247 |      0.798 |     128.00 |     128.00 |   25.682 |    0.001 |    0.005 |   0.004104 |   0.002052\n",
      "    23 |      0.690 |     31.872 |      0.251 |      0.862 |     128.00 |     128.00 |   24.425 |    0.001 |    0.004 |   0.003539 |   0.001769\n",
      "    24 |      0.613 |     29.622 |      0.249 |      0.917 |     128.00 |     256.00 |   23.801 |    0.001 |    0.004 |   0.003140 |   0.001570\n",
      "    25 |      0.530 |     39.152 |      0.258 |      1.015 |     256.00 |     256.00 |   23.424 |    0.000 |    0.004 |   0.002714 |   0.001357\n",
      "    26 |      0.464 |     42.937 |      0.262 |      1.100 |     256.00 |     256.00 |   22.276 |    0.001 |    0.004 |   0.002355 |   0.001178\n",
      "    27 |      0.413 |     38.925 |      0.258 |      1.172 |     256.00 |     512.00 |   24.136 |    0.001 |    0.004 |   0.002092 |   0.001046\n",
      "    28 |      0.356 |     53.198 |      0.272 |      1.301 |     512.00 |     512.00 |   24.220 |    0.001 |    0.004 |   0.001789 |   0.000894\n",
      "    29 |      0.304 |     60.308 |      0.280 |      1.411 |     512.00 |     512.00 |   20.898 |    0.001 |    0.004 |   0.001514 |   0.000757\n",
      "    30 |      0.270 |     53.814 |      0.273 |      1.502 |     512.00 |    1024.00 |   19.400 |    0.000 |    0.004 |   0.001338 |   0.000669\n",
      "    31 |      0.233 |     67.088 |      0.286 |      1.667 |    1024.00 |    1024.00 |   20.147 |    0.000 |    0.004 |   0.001144 |   0.000572\n",
      "    32 |      0.199 |     76.490 |      0.296 |      1.809 |    1024.00 |    1024.00 |   19.060 |    0.001 |    0.004 |   0.000967 |   0.000483\n",
      "    33 |      0.178 |     67.621 |      0.287 |      1.929 |    1024.00 |    2048.00 |   18.665 |    0.001 |    0.004 |   0.000859 |   0.000429\n",
      "    34 |      0.153 |     89.135 |      0.308 |      2.147 |    2048.00 |    2048.00 |   19.750 |    0.001 |    0.004 |   0.000732 |   0.000366\n",
      "    35 |      0.130 |    100.076 |      0.319 |      2.334 |    2048.00 |    2048.00 |   19.488 |    0.001 |    0.005 |   0.000614 |   0.000307\n",
      "    36 |      0.114 |     92.514 |      0.312 |      2.486 |    2048.00 |    4096.00 |   17.780 |    0.001 |    0.004 |   0.000532 |   0.000266\n",
      "    37 |      0.095 |    123.357 |      0.343 |      2.749 |    4096.00 |    4096.00 |   17.513 |    0.000 |    0.004 |   0.000438 |   0.000219\n",
      "    38 |      0.080 |    136.275 |      0.355 |      2.967 |    4096.00 |    4096.00 |   16.400 |    0.000 |    0.004 |   0.000361 |   0.000180\n",
      "    39 |      0.071 |    116.542 |      0.336 |      3.146 |    4096.00 |    8192.00 |   16.874 |    0.001 |    0.004 |   0.000316 |   0.000158\n",
      "    40 |      0.060 |    146.448 |      0.366 |      3.472 |    8192.00 |    8192.00 |   16.050 |    0.001 |    0.004 |   0.000262 |   0.000131\n",
      "    41 |      0.051 |    160.539 |      0.380 |      3.752 |    8192.00 |    8192.00 |   14.424 |    0.000 |    0.004 |   0.000214 |   0.000107\n",
      "    42 |      0.044 |    150.003 |      0.369 |      3.981 |    8192.00 |   16384.00 |   13.523 |    0.000 |    0.004 |   0.000182 |   0.000091\n",
      "    43 |      0.036 |    190.326 |      0.410 |      4.375 |   16384.00 |   16384.00 |   14.880 |    0.001 |    0.005 |   0.000146 |   0.000073\n",
      "    44 |      0.030 |    215.114 |      0.434 |      4.691 |   16384.00 |   16384.00 |   12.525 |    0.001 |    0.004 |   0.000116 |   0.000058\n",
      "    45 |      0.026 |    186.267 |      0.405 |      4.939 |   16384.00 |   32768.00 |   11.962 |    0.000 |    0.004 |   0.000097 |   0.000049\n",
      "    46 |      0.021 |    217.023 |      0.436 |      5.379 |   32768.00 |   32768.00 |   11.132 |    0.000 |    0.004 |   0.000078 |   0.000039\n",
      "    47 |      0.018 |    231.004 |      0.450 |      5.750 |   32768.00 |          / |   11.608 |    0.001 |    0.004 |   0.000062 |   0.000031\n",
      "early stop!\n",
      "Terminated (optimal) in 48 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 17.54 minutes.\n",
      "\n",
      "Stage 1 variable selection finished. Elapsed time: 51.43 minutes.\n",
      "\n",
      "Stage 2: final theta estimation with Graph Laplacian Constrain using already estimated sigma^2 and gamma_g\n",
      "HIGHLIGHT: reuse estimated theta and e^alpha in stage 1 as initial value\n",
      "No Adjacency Matrix of spots specified! Ignore Graph Laplacian Constrain in stage 2\n",
      "\n",
      "start ADMM iteration...\n",
      "  iter |  res_pri_n | res_dual_n |    eps_pri |   eps_dual |        rho |    new_rho | time_opt | time_reg | time_lap | tilde_RMSE |   hat_RMSE\n",
      "     0 |      1.323 |      1.323 |      0.221 |      0.221 |       1.00 |       1.00 |   35.206 |    0.001 |    0.004 |   0.006197 |   0.003099\n",
      "     1 |      0.006 |      1.323 |      0.221 |      0.221 |       1.00 |          / |    4.145 |    0.001 |    0.004 |   0.000002 |   0.000001\n",
      "early stop!\n",
      "Terminated (optimal) in 2 iterations.\n",
      "One optimization by ADMM finished. Elapsed time: 0.66 minutes.\n",
      "\n",
      "\n",
      "stage 2 finished. Elapsed time: 0.66 minutes.\n",
      "\n",
      "GLRM fitting finished. Elapsed time: 55.57 minutes.\n",
      "\n",
      "\n",
      "Post-processing estimated cell-type proportion theta...\n",
      "hard thresholding small theta values with threshold 0\n",
      "\n",
      "\n",
      "cell type deconvolution finished. Estimate results saved in /home/exouser/Spatial/celltype_proportions.csv. Elapsed time: 1.14 hours.\n",
      "\n",
      "\n",
      "######### No imputation #########\n",
      "\n",
      "\n",
      "whole pipeline finished. Total elapsed time: 1.14 hours.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='runDeconvolution -q STARmap_cell_nUMI.csv                           -r scRNA_data_full.csv                           -c ref_scRNA_cell_celltype.csv                           -n 64                           --n_pseudo_spot 0                           --n_hv_gene 1000                           --diagnosis true\\n', returncode=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "cmd = '''runDeconvolution -q STARmap_cell_nUMI.csv \\\n",
    "                          -r scRNA_data_full.csv \\\n",
    "                          -c ref_scRNA_cell_celltype.csv \\\n",
    "                          -n 64 \\\n",
    "                          --n_pseudo_spot 0 \\\n",
    "                          --n_hv_gene 1000 \\\n",
    "                          --diagnosis true\n",
    "'''\n",
    "\n",
    "subprocess.run(cmd, check=True, text=True, shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
